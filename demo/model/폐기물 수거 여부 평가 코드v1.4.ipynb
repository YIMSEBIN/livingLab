{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Large Waste Items:\n",
    "- Number of detections: 65\n",
    "- Average confidence: 0.427\n",
    "- Maximum confidence: 0.912\n",
    "- Minimum confidence: 0.252\n",
    "\n",
    "PP bag:\n",
    "- Number of detections: 44\n",
    "- Average confidence: 0.557\n",
    "- Maximum confidence: 0.943\n",
    "- Minimum confidence: 0.270\n",
    "\n",
    "General Waste:\n",
    "- Number of detections: 163\n",
    "- Average confidence: 0.426\n",
    "- Maximum confidence: 0.741\n",
    "- Minimum confidence: 0.250\n",
    "\n",
    "CleanNet:\n",
    "- Number of detections: 119\n",
    "- Average confidence: 0.745\n",
    "- Maximum confidence: 0.959\n",
    "- Minimum confidence: 0.265\n",
    "\n",
    "이걸 기준으로 평균보다 아래는 Red바운딩 박스 이상은 Green 바운딩 박스를 표출한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "iou적용 및 위도경도 csv로 내려받을 수 있도록 수정하였다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: 필요한 라이브러리 임포트\n",
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from time import time\n",
    "import pandas as pd\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: IoU 관련 함수 정의\n",
    "def calculate_iou(box1, box2):\n",
    "    x1_1, y1_1, x2_1, y2_1 = box1\n",
    "    x1_2, y1_2, x2_2, y2_2 = box2\n",
    "    \n",
    "    x1_i = max(x1_1, x1_2)\n",
    "    y1_i = max(y1_1, y1_2)\n",
    "    x2_i = min(x2_1, x2_2)\n",
    "    y2_i = min(y2_1, y2_2)\n",
    "    \n",
    "    if x2_i < x1_i or y2_i < y1_i:\n",
    "        return 0.0\n",
    "    \n",
    "    intersection = (x2_i - x1_i) * (y2_i - y1_i)\n",
    "    area1 = (x2_1 - x1_1) * (y2_1 - y1_1)\n",
    "    area2 = (x2_2 - x1_2) * (y2_2 - y1_2)\n",
    "    union = area1 + area2 - intersection\n",
    "    \n",
    "    return intersection / union if union > 0 else 0\n",
    "\n",
    "def non_max_suppression(boxes, scores, classes, iou_threshold=0.5):\n",
    "    if len(boxes) == 0:\n",
    "        return [], [], []\n",
    "        \n",
    "    selected_indices = []\n",
    "    indices = np.argsort(scores)[::-1]\n",
    "    \n",
    "    while len(indices) > 0:\n",
    "        current_idx = indices[0]\n",
    "        selected_indices.append(current_idx)\n",
    "        \n",
    "        if len(indices) == 1:\n",
    "            break\n",
    "            \n",
    "        ious = [calculate_iou(boxes[current_idx], boxes[idx]) for idx in indices[1:]]\n",
    "        indices = indices[1:][np.array(ious) < iou_threshold]\n",
    "    \n",
    "    return [boxes[i] for i in selected_indices], [scores[i] for i in selected_indices], [classes[i] for i in selected_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: 카테고리 매핑 정의\n",
    "category_mapping = {\n",
    "    # 대형폐기물 (Large Waste Items)\n",
    "    'arcade machine': 'Large Waste Items',\n",
    "    'Audio': 'Large Waste Items',\n",
    "    'Computer': 'Large Waste Items',\n",
    "    'fax machine': 'Large Waste Items',\n",
    "    'Main unit': 'Large Waste Items',\n",
    "    'Monitor': 'Large Waste Items',\n",
    "    'Printer': 'Large Waste Items',\n",
    "    'sewing machine': 'Large Waste Items',\n",
    "    'Speaker': 'Large Waste Items',\n",
    "    'typewriter': 'Large Waste Items',\n",
    "    'Vacuum cleaner': 'Large Waste Items',\n",
    "    'Video player': 'Large Waste Items',\n",
    "    'Bathtub': 'Large Waste Items',\n",
    "    'Sink': 'Large Waste Items',\n",
    "    'Kitchen sink': 'Large Waste Items',\n",
    "    'Toilet bowl': 'Large Waste Items',\n",
    "    'Bed': 'Large Waste Items',\n",
    "    'Bookcase': 'Large Waste Items',\n",
    "    'Bookstand': 'Large Waste Items',\n",
    "    'Cabinet': 'Large Waste Items',\n",
    "    'chair': 'Large Waste Items',\n",
    "    'Cupboard': 'Large Waste Items',\n",
    "    'Desk': 'Large Waste Items',\n",
    "    'Dining table': 'Large Waste Items',\n",
    "    'Display cabinet': 'Large Waste Items',\n",
    "    'Display stand': 'Large Waste Items',\n",
    "    'Drawer unit': 'Large Waste Items',\n",
    "    'Shoe rack': 'Large Waste Items',\n",
    "    'Small cabinet': 'Large Waste Items',\n",
    "    'Sofa': 'Large Waste Items',\n",
    "    'Table': 'Large Waste Items',\n",
    "    'TV stand': 'Large Waste Items',\n",
    "    'Vanity table': 'Large Waste Items',\n",
    "    'Wardrobe': 'Large Waste Items',\n",
    "    'Air conditioner': 'Large Waste Items',\n",
    "    'Air purifier': 'Large Waste Items',\n",
    "    'dish dryer': 'Large Waste Items',\n",
    "    'Electric rice cooker': 'Large Waste Items',\n",
    "    'Fan': 'Large Waste Items',\n",
    "    'Gas oven range': 'Large Waste Items',\n",
    "    'Heater': 'Large Waste Items',\n",
    "    'Humidifier': 'Large Waste Items',\n",
    "    'Microwave': 'Large Waste Items',\n",
    "    'refrigerator': 'Large Waste Items',\n",
    "    'Spin dryer': 'Large Waste Items',\n",
    "    'TV': 'Large Waste Items',\n",
    "    'Washing machine': 'Large Waste Items',\n",
    "    'Aquarium': 'Large Waste Items',\n",
    "    'Bamboo mat': 'Large Waste Items',\n",
    "    'Bedding items': 'Large Waste Items',\n",
    "    'bicycle': 'Large Waste Items',\n",
    "    'Carpet': 'Large Waste Items',\n",
    "    'Clothes drying rack': 'Large Waste Items',\n",
    "    'Coat rack': 'Large Waste Items',\n",
    "    'Door panel': 'Large Waste Items',\n",
    "    'Earthenware jar': 'Large Waste Items',\n",
    "    'Floor covering': 'Large Waste Items',\n",
    "    'Frame': 'Large Waste Items',\n",
    "    'lumber': 'Large Waste Items',\n",
    "    'Mannequin': 'Large Waste Items',\n",
    "    'Mat': 'Large Waste Items',\n",
    "    'Piano': 'Large Waste Items',\n",
    "    'Rice storage container': 'Large Waste Items',\n",
    "    'Signboard': 'Large Waste Items',\n",
    "    'Stroller': 'Large Waste Items',\n",
    "    'Wall clock': 'Large Waste Items',\n",
    "    'Water tank': 'Large Waste Items',\n",
    "    'audio cabinet': 'Large Waste Items',\n",
    "    'suitcase': 'Large Waste Items',\n",
    "    \n",
    "    # 기타 카테고리\n",
    "    'PP bag': 'PP bag',\n",
    "    'General waste bag': 'General Waste',\n",
    "    'waste pile': 'General Waste',\n",
    "    'CleanNet': 'CleanNet',\n",
    "    'General Waste': 'General Waste'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: 바운딩 박스 색상 및 시간 계산 함수\n",
    "def get_bbox_color(class_name, confidence):\n",
    "    mapped_class = category_mapping.get(class_name, class_name)\n",
    "    \n",
    "    if mapped_class == 'Large Waste Items':\n",
    "        return (0, 255, 0) if confidence > 0.4 else (0, 0, 255)\n",
    "    elif mapped_class == 'PP bag':\n",
    "        return (0, 255, 0) if confidence > 0.5 else (0, 0, 255)\n",
    "    else:\n",
    "        return (255, 255, 255)\n",
    "\n",
    "def calculate_average_times(process_times):\n",
    "    avg_times = {\n",
    "        'preprocess': sum(t['preprocess'] for t in process_times) / len(process_times),\n",
    "        'inference': sum(t['inference'] for t in process_times) / len(process_times),\n",
    "        'postprocess': sum(t['postprocess'] for t in process_times) / len(process_times)\n",
    "    }\n",
    "    \n",
    "    return avg_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: 파일명 파싱 함수\n",
    "def parse_location_from_filename(filename):\n",
    "    parts = filename.split('-')\n",
    "    if len(parts) >= 5:\n",
    "        lat = float(f\"{parts[1]}.{parts[2]}\")\n",
    "        lon = float(f\"{parts[3]}.{parts[4].split('_')[0]}\")\n",
    "        return lat, lon\n",
    "    return None, None\n",
    "\n",
    "def parse_time_from_filename(filename):\n",
    "    time_str = filename[:12]\n",
    "    try:\n",
    "        return datetime.strptime(time_str, '%Y%m%d%H%M').strftime('%Y-%m-%d %H:%M:%S')\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: 객체 감지 함수\n",
    "def detect_objects(model_path, image_dir, iou_threshold=0.5):\n",
    "    model = YOLO(model_path)\n",
    "    output_dir = './runs/detect/predict/'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    image_paths = []\n",
    "    for ext in ['*.jpg', '*.jpeg', '*.png']:\n",
    "        image_paths.extend(list(Path(image_dir).glob(ext)))\n",
    "    \n",
    "    print(f\"Found {len(image_paths)} images in the directory.\")\n",
    "    \n",
    "    results = []\n",
    "    process_times = []\n",
    "    \n",
    "    for idx, image_path in enumerate(image_paths, 1):\n",
    "        print(f\"\\nProcessing image {idx}/{len(image_paths)}: {image_path.name}\")\n",
    "        \n",
    "        image = cv2.imread(str(image_path))\n",
    "        if image is None:\n",
    "            print(f\"Error: Could not load image {image_path}\")\n",
    "            continue\n",
    "            \n",
    "        result = model.predict(source=str(image_path), save=False, device='cpu')[0]\n",
    "        \n",
    "        process_times.append({\n",
    "            'preprocess': result.speed['preprocess'],\n",
    "            'inference': result.speed['inference'],\n",
    "            'postprocess': result.speed['postprocess']\n",
    "        })\n",
    "        \n",
    "        boxes = result.boxes.xyxy.cpu().numpy()\n",
    "        scores = result.boxes.conf.cpu().numpy()\n",
    "        class_ids = result.boxes.cls.cpu().numpy()\n",
    "        class_names = [result.names[int(cls_id)] for cls_id in class_ids]\n",
    "        \n",
    "        filtered_boxes, filtered_scores, filtered_classes = non_max_suppression(\n",
    "            boxes, scores, class_names, iou_threshold\n",
    "        )\n",
    "        \n",
    "        for box, confidence, class_name in zip(filtered_boxes, filtered_scores, filtered_classes):\n",
    "            x1, y1, x2, y2 = box.astype(int)\n",
    "            try:\n",
    "                color = get_bbox_color(class_name, confidence)\n",
    "                cv2.rectangle(image, (x1, y1), (x2, y2), color, 2)\n",
    "                label = f'{class_name} {confidence:.2f}'\n",
    "                cv2.putText(image, label, (x1, y1-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing detection for class {class_name}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        output_path = os.path.join(output_dir, f'{Path(image_path).stem}_detected{Path(image_path).suffix}')\n",
    "        cv2.imwrite(output_path, image)\n",
    "        \n",
    "        results.append({\n",
    "            'image_name': image_path.name,\n",
    "            'detections': [\n",
    "                {'class': cls, 'confidence': conf, 'box': box.tolist()}\n",
    "                for cls, conf, box in zip(filtered_classes, filtered_scores, filtered_boxes)\n",
    "            ]\n",
    "        })\n",
    "    \n",
    "    avg_times = calculate_average_times(process_times)\n",
    "    return results, avg_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results(results, avg_times, output_path):\n",
    "    # CSV 데이터를 저장할 리스트\n",
    "    csv_data = []\n",
    "    \n",
    "    for result in results:\n",
    "        image_name = result['image_name']\n",
    "        lat, lon = parse_location_from_filename(image_name)\n",
    "        time = parse_time_from_filename(image_name)\n",
    "        \n",
    "        # 각 이미지의 각 객체별 정보 저장\n",
    "        detections_by_type = {}\n",
    "        for detection in result['detections']:\n",
    "            obj_type = category_mapping.get(detection['class'], detection['class'])\n",
    "            if obj_type not in detections_by_type:\n",
    "                detections_by_type[obj_type] = {\n",
    "                    'count': 0,\n",
    "                    'total_score': 0,\n",
    "                }\n",
    "            detections_by_type[obj_type]['count'] += 1\n",
    "            detections_by_type[obj_type]['total_score'] += detection['confidence']\n",
    "        \n",
    "        # 이미지 파일명 생성 (detected 접미사 추가)\n",
    "        detected_image_name = f\"{os.path.splitext(image_name)[0]}_detected.jpg\"\n",
    "        \n",
    "        # 각 객체 타입별로 CSV 행 생성\n",
    "        for obj_type, stats in detections_by_type.items():\n",
    "            avg_score = stats['total_score'] / stats['count']\n",
    "            csv_data.append({\n",
    "                'Latitude': lat,\n",
    "                'Longitude': lon,\n",
    "                'Type': obj_type,\n",
    "                'Count': stats['count'],\n",
    "                'Time': time,\n",
    "                'Image': detected_image_name,  # 수정된 부분: 파일명만 저장\n",
    "                'Score': round(avg_score, 3)\n",
    "            })\n",
    "    \n",
    "    # DataFrame 생성 및 CSV 저장\n",
    "    df = pd.DataFrame(csv_data)\n",
    "    csv_output_path = output_path + '.csv'\n",
    "    df.to_csv(csv_output_path, index=False, encoding='utf-8-sig')\n",
    "    \n",
    "    # 요약 통계 반환\n",
    "    summary = {\n",
    "        \"total_images\": len(results),\n",
    "        \"positive_count\": sum(1 for result in results if any(\n",
    "            category_mapping.get(det['class'], det['class']) == 'Large Waste Items'\n",
    "            for det in result['detections']\n",
    "        )),\n",
    "        \"negative_count\": sum(1 for result in results if not any(\n",
    "            category_mapping.get(det['class'], det['class']) == 'Large Waste Items'\n",
    "            for det in result['detections']\n",
    "        ))\n",
    "    }\n",
    "    \n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting object detection with IoU filtering...\n",
      "Found 76 images in the directory.\n",
      "\n",
      "Processing image 1/76: 202411061313-36-307488-127-352668_jpg.rf.2debbe814df92503a0079be859650cc0.jpg\n",
      "\n",
      "image 1/1 c:\\Users\\yimsebin\\livingLab\\demo\\model\\test\\images\\202411061313-36-307488-127-352668_jpg.rf.2debbe814df92503a0079be859650cc0.jpg: 640x640 1 General Waste, 2 CleanNets, 59.4ms\n",
      "Speed: 4.0ms preprocess, 59.4ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "Processing image 2/76: 202411061319-36-307974-127-355197_jpg.rf.40e66b47ea454f90ee055c740410cc1c.jpg\n",
      "\n",
      "image 1/1 c:\\Users\\yimsebin\\livingLab\\demo\\model\\test\\images\\202411061319-36-307974-127-355197_jpg.rf.40e66b47ea454f90ee055c740410cc1c.jpg: 640x640 3 General Wastes, 2 CleanNets, 27.2ms\n",
      "Speed: 2.1ms preprocess, 27.2ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "Processing image 3/76: 202411061322-36-307550-127-354681_jpg.rf.e09f59bc4cc8915a9da9c47c91f306f0.jpg\n",
      "\n",
      "image 1/1 c:\\Users\\yimsebin\\livingLab\\demo\\model\\test\\images\\202411061322-36-307550-127-354681_jpg.rf.e09f59bc4cc8915a9da9c47c91f306f0.jpg: 640x640 2 General Wastes, 2 CleanNets, 22.3ms\n",
      "Speed: 2.0ms preprocess, 22.3ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "Processing image 4/76: 202411061322-36-320218-127-343319_jpg.rf.86814c1d60da676d5d9596a2a3da0efb.jpg\n",
      "\n",
      "image 1/1 c:\\Users\\yimsebin\\livingLab\\demo\\model\\test\\images\\202411061322-36-320218-127-343319_jpg.rf.86814c1d60da676d5d9596a2a3da0efb.jpg: 640x640 1 Large Waste Items, 4 General Wastes, 1 CleanNet, 39.9ms\n",
      "Speed: 3.1ms preprocess, 39.9ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "Processing image 5/76: 202411061323-36-319426-127-343581-2_jpg.rf.5c85eeb865231f3fb9ac4fd355319416.jpg\n",
      "\n",
      "image 1/1 c:\\Users\\yimsebin\\livingLab\\demo\\model\\test\\images\\202411061323-36-319426-127-343581-2_jpg.rf.5c85eeb865231f3fb9ac4fd355319416.jpg: 640x640 1 Large Waste Items, 1 General Waste, 1 CleanNet, 25.6ms\n",
      "Speed: 1.0ms preprocess, 25.6ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "Processing image 6/76: 202411061325-36-308399-127-354248_jpg.rf.fe8e74b6e0015e468fad270eb87ca499.jpg\n",
      "\n",
      "image 1/1 c:\\Users\\yimsebin\\livingLab\\demo\\model\\test\\images\\202411061325-36-308399-127-354248_jpg.rf.fe8e74b6e0015e468fad270eb87ca499.jpg: 640x640 3 General Wastes, 2 CleanNets, 24.5ms\n",
      "Speed: 1.0ms preprocess, 24.5ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "Processing image 7/76: 202411061326-36-318897-127-343185_jpg.rf.d6707abf64c488ae79ddec26206a2f89.jpg\n",
      "\n",
      "image 1/1 c:\\Users\\yimsebin\\livingLab\\demo\\model\\test\\images\\202411061326-36-318897-127-343185_jpg.rf.d6707abf64c488ae79ddec26206a2f89.jpg: 640x640 2 Large Waste Itemss, 2 General Wastes, 1 CleanNet, 24.3ms\n",
      "Speed: 1.0ms preprocess, 24.3ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "Processing image 8/76: 202411061328-36-318047-127-344154_jpg.rf.7aefec6f116650e0e611553b9811a6d8.jpg\n",
      "\n",
      "image 1/1 c:\\Users\\yimsebin\\livingLab\\demo\\model\\test\\images\\202411061328-36-318047-127-344154_jpg.rf.7aefec6f116650e0e611553b9811a6d8.jpg: 640x640 2 General Wastes, 2 CleanNets, 23.3ms\n",
      "Speed: 1.1ms preprocess, 23.3ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "Processing image 9/76: 202411061328-36-318467-127-342773_jpg.rf.8e7d885664cc4a093d1ef0afcc79b2e8.jpg\n",
      "\n",
      "image 1/1 c:\\Users\\yimsebin\\livingLab\\demo\\model\\test\\images\\202411061328-36-318467-127-342773_jpg.rf.8e7d885664cc4a093d1ef0afcc79b2e8.jpg: 640x640 4 General Wastes, 2 CleanNets, 23.4ms\n",
      "Speed: 1.0ms preprocess, 23.4ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "Processing image 10/76: 202411061329-36-309133-127-353018_jpg.rf.66723f2b0a1f776c240789b2605d7c6e.jpg\n",
      "\n",
      "image 1/1 c:\\Users\\yimsebin\\livingLab\\demo\\model\\test\\images\\202411061329-36-309133-127-353018_jpg.rf.66723f2b0a1f776c240789b2605d7c6e.jpg: 640x640 2 General Wastes, 2 CleanNets, 24.3ms\n",
      "Speed: 2.0ms preprocess, 24.3ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "Processing image 11/76: 202411061330-36-318792-127-344275_jpg.rf.3f74fb24878d9a28014f79a8ad00e4c4.jpg\n",
      "\n",
      "image 1/1 c:\\Users\\yimsebin\\livingLab\\demo\\model\\test\\images\\202411061330-36-318792-127-344275_jpg.rf.3f74fb24878d9a28014f79a8ad00e4c4.jpg: 640x640 1 General Waste, 25.2ms\n",
      "Speed: 1.0ms preprocess, 25.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "Processing image 12/76: 202411061337-36-309239-127-353916_jpg.rf.4c7a323affb98f997405ec20ca6d216e.jpg\n",
      "\n",
      "image 1/1 c:\\Users\\yimsebin\\livingLab\\demo\\model\\test\\images\\202411061337-36-309239-127-353916_jpg.rf.4c7a323affb98f997405ec20ca6d216e.jpg: 640x640 1 Large Waste Items, 4 General Wastes, 2 CleanNets, 24.7ms\n",
      "Speed: 2.0ms preprocess, 24.7ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "Processing image 13/76: 202411061339-36-310790-127-352166_jpg.rf.78fd2988c35f72ab7c8c932764bb04c6.jpg\n",
      "\n",
      "image 1/1 c:\\Users\\yimsebin\\livingLab\\demo\\model\\test\\images\\202411061339-36-310790-127-352166_jpg.rf.78fd2988c35f72ab7c8c932764bb04c6.jpg: 640x640 2 General Wastes, 2 CleanNets, 24.5ms\n",
      "Speed: 2.0ms preprocess, 24.5ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "Processing image 14/76: 202411061342-36-309176-127-351941_jpg.rf.05afb3c8e078242b96c4a730357296bb.jpg\n",
      "\n",
      "image 1/1 c:\\Users\\yimsebin\\livingLab\\demo\\model\\test\\images\\202411061342-36-309176-127-351941_jpg.rf.05afb3c8e078242b96c4a730357296bb.jpg: 640x640 3 General Wastes, 2 CleanNets, 24.8ms\n",
      "Speed: 2.0ms preprocess, 24.8ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "Processing image 15/76: 202411061342-36-315640-127-349174_jpg.rf.5444f5989d8b4ed01d08d05553d63571.jpg\n",
      "\n",
      "image 1/1 c:\\Users\\yimsebin\\livingLab\\demo\\model\\test\\images\\202411061342-36-315640-127-349174_jpg.rf.5444f5989d8b4ed01d08d05553d63571.jpg: 640x640 3 General Wastes, 2 CleanNets, 24.2ms\n",
      "Speed: 3.1ms preprocess, 24.2ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "Processing image 16/76: 202411061343-36-315732-127-349908_jpg.rf.f32a738af7e8cb0569568da90e660036.jpg\n",
      "\n",
      "image 1/1 c:\\Users\\yimsebin\\livingLab\\demo\\model\\test\\images\\202411061343-36-315732-127-349908_jpg.rf.f32a738af7e8cb0569568da90e660036.jpg: 640x640 4 General Wastes, 1 CleanNet, 27.2ms\n",
      "Speed: 3.1ms preprocess, 27.2ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "Processing image 17/76: 202411061345-36-314850-127-350158_jpg.rf.a5c076f0199e673c36da4ac790ee940e.jpg\n",
      "\n",
      "image 1/1 c:\\Users\\yimsebin\\livingLab\\demo\\model\\test\\images\\202411061345-36-314850-127-350158_jpg.rf.a5c076f0199e673c36da4ac790ee940e.jpg: 640x640 1 General Waste, 2 CleanNets, 26.7ms\n",
      "Speed: 2.0ms preprocess, 26.7ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "Processing image 18/76: 202411061346-36-308081-127-353130_jpg.rf.83770fab01c2a2e62838a2a2729231a7.jpg\n",
      "\n",
      "image 1/1 c:\\Users\\yimsebin\\livingLab\\demo\\model\\test\\images\\202411061346-36-308081-127-353130_jpg.rf.83770fab01c2a2e62838a2a2729231a7.jpg: 640x640 4 General Wastes, 2 CleanNets, 28.3ms\n",
      "Speed: 2.0ms preprocess, 28.3ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "Processing image 19/76: 202411061346-36-311745-127-353092_jpg.rf.3e408eccde358b63c05fd5a29f956aab.jpg\n",
      "\n",
      "image 1/1 c:\\Users\\yimsebin\\livingLab\\demo\\model\\test\\images\\202411061346-36-311745-127-353092_jpg.rf.3e408eccde358b63c05fd5a29f956aab.jpg: 640x640 3 General Wastes, 3 CleanNets, 28.7ms\n",
      "Speed: 2.0ms preprocess, 28.7ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "Processing image 20/76: 202411061346-36-314198-127-350451_jpg.rf.c77a12c283995fb4057bd0ca00b83161.jpg\n",
      "\n",
      "image 1/1 c:\\Users\\yimsebin\\livingLab\\demo\\model\\test\\images\\202411061346-36-314198-127-350451_jpg.rf.c77a12c283995fb4057bd0ca00b83161.jpg: 640x640 3 Large Waste Itemss, 1 General Waste, 1 CleanNet, 28.7ms\n",
      "Speed: 1.1ms preprocess, 28.7ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "Processing image 21/76: 202411061348-36-310510-127-353380_jpg.rf.1b445c1bbde0cedac1df9b0f9c220dd4.jpg\n",
      "\n",
      "image 1/1 c:\\Users\\yimsebin\\livingLab\\demo\\model\\test\\images\\202411061348-36-310510-127-353380_jpg.rf.1b445c1bbde0cedac1df9b0f9c220dd4.jpg: 640x640 3 General Wastes, 2 CleanNets, 24.2ms\n",
      "Speed: 2.0ms preprocess, 24.2ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "Processing image 22/76: 202411061348-36-313711-127-349988_jpg.rf.3581e8d79a3a050686554a20476cdb34.jpg\n",
      "\n",
      "image 1/1 c:\\Users\\yimsebin\\livingLab\\demo\\model\\test\\images\\202411061348-36-313711-127-349988_jpg.rf.3581e8d79a3a050686554a20476cdb34.jpg: 640x640 2 Large Waste Itemss, 2 General Wastes, 2 CleanNets, 26.3ms\n",
      "Speed: 3.0ms preprocess, 26.3ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "Processing image 23/76: 202411061349-36-311745-127-353092_jpg.rf.fb5f4cbdb3036192e55d681050fe72da.jpg\n",
      "\n",
      "image 1/1 c:\\Users\\yimsebin\\livingLab\\demo\\model\\test\\images\\202411061349-36-311745-127-353092_jpg.rf.fb5f4cbdb3036192e55d681050fe72da.jpg: 640x640 2 General Wastes, 2 CleanNets, 22.2ms\n",
      "Speed: 3.0ms preprocess, 22.2ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "Processing image 24/76: 202411061349-36-313040-127-350293_jpg.rf.52fab8e4c82156dcff635e98319420f9.jpg\n",
      "\n",
      "image 1/1 c:\\Users\\yimsebin\\livingLab\\demo\\model\\test\\images\\202411061349-36-313040-127-350293_jpg.rf.52fab8e4c82156dcff635e98319420f9.jpg: 640x640 3 General Wastes, 2 CleanNets, 23.7ms\n",
      "Speed: 3.0ms preprocess, 23.7ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "Processing image 25/76: 202411061350-36-314129-127-351297_jpg.rf.7b7238965a8cae0683e4d2bac4f5a06a.jpg\n",
      "\n",
      "image 1/1 c:\\Users\\yimsebin\\livingLab\\demo\\model\\test\\images\\202411061350-36-314129-127-351297_jpg.rf.7b7238965a8cae0683e4d2bac4f5a06a.jpg: 640x640 4 General Wastes, 3 CleanNets, 24.2ms\n",
      "Speed: 3.1ms preprocess, 24.2ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "Processing image 26/76: 202411061352-36-313215-127-351277-2_jpg.rf.f6aa801aed2a6e2e10462b781d4358bd.jpg\n",
      "\n",
      "image 1/1 c:\\Users\\yimsebin\\livingLab\\demo\\model\\test\\images\\202411061352-36-313215-127-351277-2_jpg.rf.f6aa801aed2a6e2e10462b781d4358bd.jpg: 640x640 1 Large Waste Items, 1 PP bag, 24.2ms\n",
      "Speed: 2.1ms preprocess, 24.2ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "Processing image 27/76: 202411061352-36-313215-127-351277-3_jpg.rf.46cfc13747bebd1a1755da3b8a3cd8dd.jpg\n",
      "\n",
      "image 1/1 c:\\Users\\yimsebin\\livingLab\\demo\\model\\test\\images\\202411061352-36-313215-127-351277-3_jpg.rf.46cfc13747bebd1a1755da3b8a3cd8dd.jpg: 640x640 1 Large Waste Items, 5 PP bags, 40.6ms\n",
      "Speed: 3.0ms preprocess, 40.6ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "Processing image 28/76: 202411061358-36-308465-127-354548_jpg.rf.965f2f2b47de25bdc9d795cb71576543.jpg\n",
      "\n",
      "image 1/1 c:\\Users\\yimsebin\\livingLab\\demo\\model\\test\\images\\202411061358-36-308465-127-354548_jpg.rf.965f2f2b47de25bdc9d795cb71576543.jpg: 640x640 4 General Wastes, 2 CleanNets, 23.6ms\n",
      "Speed: 2.0ms preprocess, 23.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "Processing image 29/76: 202411061359-36-310957-127-353130_jpg.rf.906b474010a7ba8fcea77eeef01433cd.jpg\n",
      "\n",
      "image 1/1 c:\\Users\\yimsebin\\livingLab\\demo\\model\\test\\images\\202411061359-36-310957-127-353130_jpg.rf.906b474010a7ba8fcea77eeef01433cd.jpg: 640x640 3 General Wastes, 2 CleanNets, 31.4ms\n",
      "Speed: 3.5ms preprocess, 31.4ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "Processing image 30/76: 202411061415-36-316636-127-347495_jpg.rf.79ef2fb18c5b765ead9711ec6516fcea.jpg\n",
      "\n",
      "image 1/1 c:\\Users\\yimsebin\\livingLab\\demo\\model\\test\\images\\202411061415-36-316636-127-347495_jpg.rf.79ef2fb18c5b765ead9711ec6516fcea.jpg: 640x640 3 Large Waste Itemss, 25.3ms\n",
      "Speed: 1.0ms preprocess, 25.3ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "Processing image 31/76: 202411061418-36-315697-127-346125_jpg.rf.e9aab1e2970ae7ad8efb2ab0e3dd7b81.jpg\n",
      "\n",
      "image 1/1 c:\\Users\\yimsebin\\livingLab\\demo\\model\\test\\images\\202411061418-36-315697-127-346125_jpg.rf.e9aab1e2970ae7ad8efb2ab0e3dd7b81.jpg: 640x640 5 Large Waste Itemss, 26.7ms\n",
      "Speed: 1.1ms preprocess, 26.7ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "Processing image 32/76: 202411061420-36-315055-127-346190_jpg.rf.f735fd248bc6690f4c75068de0897d1f.jpg\n",
      "\n",
      "image 1/1 c:\\Users\\yimsebin\\livingLab\\demo\\model\\test\\images\\202411061420-36-315055-127-346190_jpg.rf.f735fd248bc6690f4c75068de0897d1f.jpg: 640x640 4 Large Waste Itemss, 25.2ms\n",
      "Speed: 1.0ms preprocess, 25.2ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "Processing image 33/76: 202411061423-36-313839-127-348579_jpg.rf.4c6e29fd453c998c1710c6dd014daacf.jpg\n",
      "\n",
      "image 1/1 c:\\Users\\yimsebin\\livingLab\\demo\\model\\test\\images\\202411061423-36-313839-127-348579_jpg.rf.4c6e29fd453c998c1710c6dd014daacf.jpg: 640x640 2 Large Waste Itemss, 5 General Wastes, 24.0ms\n",
      "Speed: 1.0ms preprocess, 24.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "Processing image 34/76: 202411061423-36-314851-127-347123_jpg.rf.6b1796d5ce76d550ec8c684a8c23fafd.jpg\n",
      "\n",
      "image 1/1 c:\\Users\\yimsebin\\livingLab\\demo\\model\\test\\images\\202411061423-36-314851-127-347123_jpg.rf.6b1796d5ce76d550ec8c684a8c23fafd.jpg: 640x640 4 Large Waste Itemss, 4 PP bags, 1 General Waste, 24.3ms\n",
      "Speed: 1.0ms preprocess, 24.3ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "Processing image 35/76: 202411061423-36-315046-127-346080_jpg.rf.bdc3f041759c2e28c0f15e1397a3584d.jpg\n",
      "\n",
      "image 1/1 c:\\Users\\yimsebin\\livingLab\\demo\\model\\test\\images\\202411061423-36-315046-127-346080_jpg.rf.bdc3f041759c2e28c0f15e1397a3584d.jpg: 640x640 2 Large Waste Itemss, 23.3ms\n",
      "Speed: 2.0ms preprocess, 23.3ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "Processing image 36/76: 202411061431-36-309908-127-348185_jpg.rf.23fe7b1ddec31535d885e6cc9288d2da.jpg\n",
      "\n",
      "image 1/1 c:\\Users\\yimsebin\\livingLab\\demo\\model\\test\\images\\202411061431-36-309908-127-348185_jpg.rf.23fe7b1ddec31535d885e6cc9288d2da.jpg: 640x640 6 Large Waste Itemss, 23.7ms\n",
      "Speed: 1.0ms preprocess, 23.7ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "Processing image 37/76: 202411061431-36-311574-127-349342_jpg.rf.5182513e42f5000cf9af2f91816f2114.jpg\n",
      "\n",
      "image 1/1 c:\\Users\\yimsebin\\livingLab\\demo\\model\\test\\images\\202411061431-36-311574-127-349342_jpg.rf.5182513e42f5000cf9af2f91816f2114.jpg: 640x640 2 Large Waste Itemss, 2 General Wastes, 22.6ms\n",
      "Speed: 1.0ms preprocess, 22.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "Processing image 38/76: 202411061433-36-309089-127-349308_jpg.rf.680978375d34a5ce6bd988b65213fe68.jpg\n",
      "\n",
      "image 1/1 c:\\Users\\yimsebin\\livingLab\\demo\\model\\test\\images\\202411061433-36-309089-127-349308_jpg.rf.680978375d34a5ce6bd988b65213fe68.jpg: 640x640 10 Large Waste Itemss, 1 General Waste, 24.1ms\n",
      "Speed: 1.0ms preprocess, 24.1ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "Processing image 39/76: 202411061434-36-309502-127-348344_jpg.rf.18196b3699941fd051eb67f4246945bb.jpg\n",
      "\n",
      "image 1/1 c:\\Users\\yimsebin\\livingLab\\demo\\model\\test\\images\\202411061434-36-309502-127-348344_jpg.rf.18196b3699941fd051eb67f4246945bb.jpg: 640x640 2 General Wastes, 24.0ms\n",
      "Speed: 1.0ms preprocess, 24.0ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "Processing image 40/76: 202411061438-36-309254-127-350078_jpg.rf.63980ab462c1a491c72131a321aa9bc4.jpg\n",
      "\n",
      "image 1/1 c:\\Users\\yimsebin\\livingLab\\demo\\model\\test\\images\\202411061438-36-309254-127-350078_jpg.rf.63980ab462c1a491c72131a321aa9bc4.jpg: 640x640 1 Large Waste Items, 1 CleanNet, 23.2ms\n",
      "Speed: 2.0ms preprocess, 23.2ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "Processing image 41/76: 202411081236-36-423913-127-407165-1_jpg.rf.0dfb1f31bd1bafebca3f8424e98cdc61.jpg\n",
      "\n",
      "image 1/1 c:\\Users\\yimsebin\\livingLab\\demo\\model\\test\\images\\202411081236-36-423913-127-407165-1_jpg.rf.0dfb1f31bd1bafebca3f8424e98cdc61.jpg: 640x640 3 Large Waste Itemss, 2 CleanNets, 23.5ms\n",
      "Speed: 1.0ms preprocess, 23.5ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "Processing image 42/76: 202411081236-36-423913-127-407165-2_jpg.rf.f6e2a098f84601afac7d2350a475d4cc.jpg\n",
      "\n",
      "image 1/1 c:\\Users\\yimsebin\\livingLab\\demo\\model\\test\\images\\202411081236-36-423913-127-407165-2_jpg.rf.f6e2a098f84601afac7d2350a475d4cc.jpg: 640x640 22 PP bags, 23.6ms\n",
      "Speed: 2.0ms preprocess, 23.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "Processing image 43/76: 202411111136-36-331616-127-335799_jpg.rf.896aa812873491b43e52dfbd3261d149.jpg\n",
      "\n",
      "image 1/1 c:\\Users\\yimsebin\\livingLab\\demo\\model\\test\\images\\202411111136-36-331616-127-335799_jpg.rf.896aa812873491b43e52dfbd3261d149.jpg: 640x640 1 PP bag, 2 General Wastes, 1 CleanNet, 23.3ms\n",
      "Speed: 1.0ms preprocess, 23.3ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "Processing image 44/76: 202411111137-36-331383-127-335375_jpg.rf.a50332a13cae1728d2c5dc6162c5b05d.jpg\n",
      "\n",
      "image 1/1 c:\\Users\\yimsebin\\livingLab\\demo\\model\\test\\images\\202411111137-36-331383-127-335375_jpg.rf.a50332a13cae1728d2c5dc6162c5b05d.jpg: 640x640 1 PP bag, 1 CleanNet, 27.6ms\n",
      "Speed: 2.0ms preprocess, 27.6ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "Processing image 45/76: 202411111139-36-331111-127-337848_jpg.rf.cbb982f1d1e160b11ae586add407ded9.jpg\n",
      "\n",
      "image 1/1 c:\\Users\\yimsebin\\livingLab\\demo\\model\\test\\images\\202411111139-36-331111-127-337848_jpg.rf.cbb982f1d1e160b11ae586add407ded9.jpg: 640x640 2 Large Waste Itemss, 2 General Wastes, 2 CleanNets, 25.3ms\n",
      "Speed: 1.5ms preprocess, 25.3ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "Processing image 46/76: 202411111148-36-330942-127-338586_jpg.rf.09088434bf25f50d833296c7059228a4.jpg\n",
      "\n",
      "image 1/1 c:\\Users\\yimsebin\\livingLab\\demo\\model\\test\\images\\202411111148-36-330942-127-338586_jpg.rf.09088434bf25f50d833296c7059228a4.jpg: 640x640 3 CleanNets, 22.2ms\n",
      "Speed: 2.0ms preprocess, 22.2ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "Processing image 47/76: 202411111150-36-331699-127-339169-1_jpg.rf.2dfbaba491bab23e08d12300bdec0a40.jpg\n",
      "\n",
      "image 1/1 c:\\Users\\yimsebin\\livingLab\\demo\\model\\test\\images\\202411111150-36-331699-127-339169-1_jpg.rf.2dfbaba491bab23e08d12300bdec0a40.jpg: 640x640 2 Large Waste Itemss, 3 General Wastes, 3 CleanNets, 22.0ms\n",
      "Speed: 2.0ms preprocess, 22.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "Processing image 48/76: 202411111150-36-331699-127-339169-2_jpg.rf.30de6ea0a36f7a5dff4c2d5633c75ff7.jpg\n",
      "\n",
      "image 1/1 c:\\Users\\yimsebin\\livingLab\\demo\\model\\test\\images\\202411111150-36-331699-127-339169-2_jpg.rf.30de6ea0a36f7a5dff4c2d5633c75ff7.jpg: 640x640 3 Large Waste Itemss, 22.4ms\n",
      "Speed: 2.0ms preprocess, 22.4ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "Processing image 49/76: 202411111153-36-36-331686-127-340556_jpg.rf.f610735c1c46cf5328fb96c544f43dd2.jpg\n",
      "\n",
      "image 1/1 c:\\Users\\yimsebin\\livingLab\\demo\\model\\test\\images\\202411111153-36-36-331686-127-340556_jpg.rf.f610735c1c46cf5328fb96c544f43dd2.jpg: 640x640 2 CleanNets, 23.2ms\n",
      "Speed: 1.0ms preprocess, 23.2ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "Processing image 50/76: 202411111156-36-331079-127-341792_jpg.rf.fe0c796d57c7083f244e29ed95c6585c.jpg\n",
      "\n",
      "image 1/1 c:\\Users\\yimsebin\\livingLab\\demo\\model\\test\\images\\202411111156-36-331079-127-341792_jpg.rf.fe0c796d57c7083f244e29ed95c6585c.jpg: 640x640 6 General Wastes, 2 CleanNets, 24.2ms\n",
      "Speed: 1.0ms preprocess, 24.2ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "Processing image 51/76: 202411111206-36-311745-127-353092_jpg.rf.175848f0b77ba404e4e6f11f44f8c590.jpg\n",
      "\n",
      "image 1/1 c:\\Users\\yimsebin\\livingLab\\demo\\model\\test\\images\\202411111206-36-311745-127-353092_jpg.rf.175848f0b77ba404e4e6f11f44f8c590.jpg: 640x640 1 General Waste, 2 CleanNets, 34.5ms\n",
      "Speed: 2.2ms preprocess, 34.5ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "Processing image 52/76: 202411111207-36-310510-127-353380_jpg.rf.816d4f9ca63a539abb4e7bd0bfafc0e0.jpg\n",
      "\n",
      "image 1/1 c:\\Users\\yimsebin\\livingLab\\demo\\model\\test\\images\\202411111207-36-310510-127-353380_jpg.rf.816d4f9ca63a539abb4e7bd0bfafc0e0.jpg: 640x640 3 General Wastes, 2 CleanNets, 23.8ms\n",
      "Speed: 1.1ms preprocess, 23.8ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "Processing image 53/76: 202411111210-36-310957-127-353130_jpg.rf.693876733d993c585b16039273a00299.jpg\n",
      "\n",
      "image 1/1 c:\\Users\\yimsebin\\livingLab\\demo\\model\\test\\images\\202411111210-36-310957-127-353130_jpg.rf.693876733d993c585b16039273a00299.jpg: 640x640 2 General Wastes, 2 CleanNets, 23.4ms\n",
      "Speed: 1.0ms preprocess, 23.4ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "Processing image 54/76: 202411111212-36-310790-127-352166_jpg.rf.d4bb484b862d3105c60cdbe0712b5651.jpg\n",
      "\n",
      "image 1/1 c:\\Users\\yimsebin\\livingLab\\demo\\model\\test\\images\\202411111212-36-310790-127-352166_jpg.rf.d4bb484b862d3105c60cdbe0712b5651.jpg: 640x640 7 General Wastes, 2 CleanNets, 23.3ms\n",
      "Speed: 1.0ms preprocess, 23.3ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "Processing image 55/76: 202411111213-36-309176-127-351941_jpg.rf.ddc9885bd847ea5b84dc27ccf6fd9fd9.jpg\n",
      "\n",
      "image 1/1 c:\\Users\\yimsebin\\livingLab\\demo\\model\\test\\images\\202411111213-36-309176-127-351941_jpg.rf.ddc9885bd847ea5b84dc27ccf6fd9fd9.jpg: 640x640 2 CleanNets, 23.2ms\n",
      "Speed: 1.0ms preprocess, 23.2ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "Processing image 56/76: 202411111215-36-309133-127-353018_jpg.rf.d74fb20ba076ca562ab4b9922841c5e4.jpg\n",
      "\n",
      "image 1/1 c:\\Users\\yimsebin\\livingLab\\demo\\model\\test\\images\\202411111215-36-309133-127-353018_jpg.rf.d74fb20ba076ca562ab4b9922841c5e4.jpg: 640x640 1 General Waste, 4 CleanNets, 22.2ms\n",
      "Speed: 2.0ms preprocess, 22.2ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "Processing image 57/76: 202411111216-36-309841-127-353640_jpg.rf.0cfe34d4e561604f27e4d6d51709d093.jpg\n",
      "\n",
      "image 1/1 c:\\Users\\yimsebin\\livingLab\\demo\\model\\test\\images\\202411111216-36-309841-127-353640_jpg.rf.0cfe34d4e561604f27e4d6d51709d093.jpg: 640x640 2 General Wastes, 3 CleanNets, 24.3ms\n",
      "Speed: 1.0ms preprocess, 24.3ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "Processing image 58/76: 202411111217-36-309239-127-353916_jpg.rf.45de2cd861119d382d4752616a1c7e31.jpg\n",
      "\n",
      "image 1/1 c:\\Users\\yimsebin\\livingLab\\demo\\model\\test\\images\\202411111217-36-309239-127-353916_jpg.rf.45de2cd861119d382d4752616a1c7e31.jpg: 640x640 4 General Wastes, 2 CleanNets, 22.2ms\n",
      "Speed: 1.0ms preprocess, 22.2ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "Processing image 59/76: 202411111220-36-308081-127-353130_jpg.rf.d482c4c6079ceee93fea151e3af3092a.jpg\n",
      "\n",
      "image 1/1 c:\\Users\\yimsebin\\livingLab\\demo\\model\\test\\images\\202411111220-36-308081-127-353130_jpg.rf.d482c4c6079ceee93fea151e3af3092a.jpg: 640x640 2 General Wastes, 2 CleanNets, 22.3ms\n",
      "Speed: 1.0ms preprocess, 22.3ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "Processing image 60/76: 202411111221-36-307340-127-353540_jpg.rf.44607ac9cf9e836b2131ef722b16478e.jpg\n",
      "\n",
      "image 1/1 c:\\Users\\yimsebin\\livingLab\\demo\\model\\test\\images\\202411111221-36-307340-127-353540_jpg.rf.44607ac9cf9e836b2131ef722b16478e.jpg: 640x640 (no detections), 22.9ms\n",
      "Speed: 2.0ms preprocess, 22.9ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "Processing image 61/76: 202411111223-36-307550-127-354681_jpg.rf.23480e44a8092c9c3311ca80c0ed1d96.jpg\n",
      "\n",
      "image 1/1 c:\\Users\\yimsebin\\livingLab\\demo\\model\\test\\images\\202411111223-36-307550-127-354681_jpg.rf.23480e44a8092c9c3311ca80c0ed1d96.jpg: 640x640 2 CleanNets, 25.1ms\n",
      "Speed: 2.1ms preprocess, 25.1ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "Processing image 62/76: 202411111224-36-307974-127-355197_jpg.rf.8824813e9e1263d3c32b197514edcf0b.jpg\n",
      "\n",
      "image 1/1 c:\\Users\\yimsebin\\livingLab\\demo\\model\\test\\images\\202411111224-36-307974-127-355197_jpg.rf.8824813e9e1263d3c32b197514edcf0b.jpg: 640x640 8 General Wastes, 3 CleanNets, 23.9ms\n",
      "Speed: 1.1ms preprocess, 23.9ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "Processing image 63/76: 202411111226-36-306652-127-353674_jpg.rf.bf1c0077ff4820a3e86121a92f4c3b79.jpg\n",
      "\n",
      "image 1/1 c:\\Users\\yimsebin\\livingLab\\demo\\model\\test\\images\\202411111226-36-306652-127-353674_jpg.rf.bf1c0077ff4820a3e86121a92f4c3b79.jpg: 640x640 4 General Wastes, 2 CleanNets, 25.3ms\n",
      "Speed: 1.0ms preprocess, 25.3ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "Processing image 64/76: 202411111226-36-306967-127-355016-1_jpg.rf.7b71d120bd3741d88af836c1e341b460.jpg\n",
      "\n",
      "image 1/1 c:\\Users\\yimsebin\\livingLab\\demo\\model\\test\\images\\202411111226-36-306967-127-355016-1_jpg.rf.7b71d120bd3741d88af836c1e341b460.jpg: 640x640 1 CleanNet, 25.8ms\n",
      "Speed: 1.0ms preprocess, 25.8ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "Processing image 65/76: 202411111228-36-307488-127-352668_jpg.rf.c021bf13ef90ee8f5a21b75c386ba96e.jpg\n",
      "\n",
      "image 1/1 c:\\Users\\yimsebin\\livingLab\\demo\\model\\test\\images\\202411111228-36-307488-127-352668_jpg.rf.c021bf13ef90ee8f5a21b75c386ba96e.jpg: 640x640 1 Large Waste Items, 2 General Wastes, 2 CleanNets, 23.8ms\n",
      "Speed: 1.0ms preprocess, 23.8ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "Processing image 66/76: 202411111229-36-308507-127-351318_jpg.rf.d6a1f843c6a43aa44bb88941b03a0142.jpg\n",
      "\n",
      "image 1/1 c:\\Users\\yimsebin\\livingLab\\demo\\model\\test\\images\\202411111229-36-308507-127-351318_jpg.rf.d6a1f843c6a43aa44bb88941b03a0142.jpg: 640x640 1 General Waste, 2 CleanNets, 25.2ms\n",
      "Speed: 1.0ms preprocess, 25.2ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "Processing image 67/76: 202411111231-36-307637-127-351064_jpg.rf.089c4d450857dd4dca741be2cf4f1cbb.jpg\n",
      "\n",
      "image 1/1 c:\\Users\\yimsebin\\livingLab\\demo\\model\\test\\images\\202411111231-36-307637-127-351064_jpg.rf.089c4d450857dd4dca741be2cf4f1cbb.jpg: 640x640 2 Large Waste Itemss, 1 PP bag, 1 General Waste, 2 CleanNets, 26.2ms\n",
      "Speed: 1.0ms preprocess, 26.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "Processing image 68/76: 202411111234-36-306079-127-351546_jpg.rf.c9c863d9067d40dd0035692f3d78d305.jpg\n",
      "\n",
      "image 1/1 c:\\Users\\yimsebin\\livingLab\\demo\\model\\test\\images\\202411111234-36-306079-127-351546_jpg.rf.c9c863d9067d40dd0035692f3d78d305.jpg: 640x640 8 General Wastes, 2 CleanNets, 24.7ms\n",
      "Speed: 1.0ms preprocess, 24.7ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "Processing image 69/76: 202411111237-36-306997-127-351164_jpg.rf.45266fd2de02a14c1c674892c8a0ae33.jpg\n",
      "\n",
      "image 1/1 c:\\Users\\yimsebin\\livingLab\\demo\\model\\test\\images\\202411111237-36-306997-127-351164_jpg.rf.45266fd2de02a14c1c674892c8a0ae33.jpg: 640x640 4 General Wastes, 2 CleanNets, 26.2ms\n",
      "Speed: 1.0ms preprocess, 26.2ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "Processing image 70/76: 202411111238-36-306863-127-350438_jpg.rf.6129b71701b75171bd7c11bce00321de.jpg\n",
      "\n",
      "image 1/1 c:\\Users\\yimsebin\\livingLab\\demo\\model\\test\\images\\202411111238-36-306863-127-350438_jpg.rf.6129b71701b75171bd7c11bce00321de.jpg: 640x640 4 General Wastes, 2 CleanNets, 28.9ms\n",
      "Speed: 1.0ms preprocess, 28.9ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "Processing image 71/76: 202411111240-36-305737-127-350237_jpg.rf.07a211fd644530b35bcda797c76089f0.jpg\n",
      "\n",
      "image 1/1 c:\\Users\\yimsebin\\livingLab\\demo\\model\\test\\images\\202411111240-36-305737-127-350237_jpg.rf.07a211fd644530b35bcda797c76089f0.jpg: 640x640 1 General Waste, 5 CleanNets, 24.3ms\n",
      "Speed: 1.0ms preprocess, 24.3ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "Processing image 72/76: 202411111241-36-305407-127-348838_jpg.rf.c9a72083f4584c5401821f536b962985.jpg\n",
      "\n",
      "image 1/1 c:\\Users\\yimsebin\\livingLab\\demo\\model\\test\\images\\202411111241-36-305407-127-348838_jpg.rf.c9a72083f4584c5401821f536b962985.jpg: 640x640 1 General Waste, 2 CleanNets, 25.2ms\n",
      "Speed: 1.0ms preprocess, 25.2ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "Processing image 73/76: 202411111246-36-324233-127-344985_jpg.rf.d4fac2031eba1010c932d2ba451f0b9e.jpg\n",
      "\n",
      "image 1/1 c:\\Users\\yimsebin\\livingLab\\demo\\model\\test\\images\\202411111246-36-324233-127-344985_jpg.rf.d4fac2031eba1010c932d2ba451f0b9e.jpg: 640x640 1 Large Waste Items, 5 General Wastes, 2 CleanNets, 22.7ms\n",
      "Speed: 1.0ms preprocess, 22.7ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "Processing image 74/76: 202411111251-36-322309-127-342752_jpg.rf.cee98eb6f397c8e712d2cc1e38b025ba.jpg\n",
      "\n",
      "image 1/1 c:\\Users\\yimsebin\\livingLab\\demo\\model\\test\\images\\202411111251-36-322309-127-342752_jpg.rf.cee98eb6f397c8e712d2cc1e38b025ba.jpg: 640x640 4 General Wastes, 2 CleanNets, 26.2ms\n",
      "Speed: 2.0ms preprocess, 26.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "Processing image 75/76: 202411111252-36-321934-127-343575_jpg.rf.f74b0372f7638eef7e00d1d9f0c16f70.jpg\n",
      "\n",
      "image 1/1 c:\\Users\\yimsebin\\livingLab\\demo\\model\\test\\images\\202411111252-36-321934-127-343575_jpg.rf.f74b0372f7638eef7e00d1d9f0c16f70.jpg: 640x640 1 General Waste, 1 CleanNet, 30.8ms\n",
      "Speed: 1.0ms preprocess, 30.8ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "Processing image 76/76: 23907_68481_2024_jpg.rf.b54912c39030808ac7c4302f5f2a9541.jpg\n",
      "\n",
      "image 1/1 c:\\Users\\yimsebin\\livingLab\\demo\\model\\test\\images\\23907_68481_2024_jpg.rf.b54912c39030808ac7c4302f5f2a9541.jpg: 640x640 2 PP bags, 2 General Wastes, 34.4ms\n",
      "Speed: 2.0ms preprocess, 34.4ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "Saving results to CSV...\n",
      "\n",
      "Processing Summary:\n",
      "Total images processed: 76\n",
      "Positive detections: 25\n",
      "Negative detections: 51\n",
      "\n",
      "Average Processing Times:\n",
      "Preprocess: 1.64ms\n",
      "Inference: 25.81ms\n",
      "Postprocess: 0.48ms\n",
      "Total Average: 27.93ms per image\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: 메인 실행 코드\n",
    "def main():\n",
    "    model_path = 'results_20241124_155336/waste_detection_model/weights/best.pt'\n",
    "    image_dir = 'test/images'\n",
    "    output_path = './test/detection_results'  # .csv will be added automatically\n",
    "    iou_threshold = 0.5\n",
    "    \n",
    "    print(\"Starting object detection with IoU filtering...\")\n",
    "    results, avg_times = detect_objects(model_path, image_dir, iou_threshold)\n",
    "    \n",
    "    print(\"\\nSaving results to CSV...\")\n",
    "    summary = save_results(results, avg_times, output_path)\n",
    "    \n",
    "    print(\"\\nProcessing Summary:\")\n",
    "    print(f\"Total images processed: {summary['total_images']}\")\n",
    "    print(f\"Positive detections: {summary['positive_count']}\")\n",
    "    print(f\"Negative detections: {summary['negative_count']}\")\n",
    "    print(f\"\\nAverage Processing Times:\")\n",
    "    print(f\"Preprocess: {avg_times['preprocess']:.2f}ms\")\n",
    "    print(f\"Inference: {avg_times['inference']:.2f}ms\")\n",
    "    print(f\"Postprocess: {avg_times['postprocess']:.2f}ms\")\n",
    "    print(f\"Total Average: {sum(avg_times.values()):.2f}ms per image\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "livinglab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
