{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training process...\n",
      "Found Train Annotations at: C:\\Users\\test\\Desktop\\waste detection.v4-add-pp-bag-8-1-1-.coco\\train\\train_annotations_yolo.json\n",
      "Found Valid Annotations at: C:\\Users\\test\\Desktop\\waste detection.v4-add-pp-bag-8-1-1-.coco\\valid\\valid_annotations_yolo.json\n",
      "Found Test Annotations at: C:\\Users\\test\\Desktop\\waste detection.v4-add-pp-bag-8-1-1-.coco\\test\\test_annotations_yolo.json\n",
      "Found Train Images at: C:\\Users\\test\\Desktop\\waste detection.v4-add-pp-bag-8-1-1-.coco\\train\\images\n",
      "Found Valid Images at: C:\\Users\\test\\Desktop\\waste detection.v4-add-pp-bag-8-1-1-.coco\\valid\\images\n",
      "Found Test Images at: C:\\Users\\test\\Desktop\\waste detection.v4-add-pp-bag-8-1-1-.coco\\test\\images\n",
      "\n",
      "Created YAML file at: C:\\Users\\test\\Desktop\\waste detection.v4-add-pp-bag-8-1-1-.coco\\data.yaml\n",
      "Number of classes: 4\n",
      "Classes in order:\n",
      "0: Large Waste Items\n",
      "1: PP bag\n",
      "2: General Waste\n",
      "3: CleanNet\n",
      "\n",
      "Converting annotations for train set...\n",
      "Successfully converted 2795 annotations for 636 images\n",
      "\n",
      "Converting annotations for valid set...\n",
      "Successfully converted 333 annotations for 79 images\n",
      "\n",
      "Converting annotations for test set...\n",
      "Successfully converted 428 annotations for 79 images\n",
      "\n",
      "Initializing model...\n",
      "\n",
      "Starting model training...\n",
      "Ultralytics 8.3.13  Python-3.12.7 torch-2.4.1+cpu CPU (12th Gen Intel Core(TM) i7-12700K)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0mtask=detect, mode=train, model=yolov8n.pt, data=C:\\Users\\test\\Desktop\\waste detection.v4-add-pp-bag-8-1-1-.coco\\data.yaml, epochs=100, time=None, patience=5, batch=8, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=4, project=C:\\Users\\test\\Desktop\\waste detection.v4-add-pp-bag-8-1-1-.coco\\results_20241124_155336, name=waste_detection_model, exist_ok=True, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=10.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=C:\\Users\\test\\Desktop\\waste detection.v4-add-pp-bag-8-1-1-.coco\\results_20241124_155336\\waste_detection_model\n",
      "Overriding model.yaml nc=80 with nc=4\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    431452  ultralytics.nn.modules.head.Detect           [4, [64, 128, 256]]           \n",
      "Model summary: 249 layers, 2,690,988 parameters, 2,690,972 gradients, 6.9 GFLOPs\n",
      "\n",
      "Transferred 313/391 items from pretrained weights\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning C:\\Users\\test\\Desktop\\waste detection.v4-add-pp-bag-8-1-1-.coco\\train\\labels... 636 images, 2 backgrounds, 0 corrupt: 100%|██████████| 638/638 [00:00<00:00, 1487.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: C:\\Users\\test\\Desktop\\waste detection.v4-add-pp-bag-8-1-1-.coco\\train\\labels.cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\test\\Desktop\\waste detection.v4-add-pp-bag-8-1-1-.coco\\valid\\labels... 79 images, 0 backgrounds, 0 corrupt: 100%|██████████| 79/79 [00:00<00:00, 668.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: C:\\Users\\test\\Desktop\\waste detection.v4-add-pp-bag-8-1-1-.coco\\valid\\labels.cache\n",
      "Plotting labels to C:\\Users\\test\\Desktop\\waste detection.v4-add-pp-bag-8-1-1-.coco\\results_20241124_155336\\waste_detection_model\\labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.00125, momentum=0.9) with parameter groups 63 weight(decay=0.0), 70 weight(decay=0.0005), 69 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mC:\\Users\\test\\Desktop\\waste detection.v4-add-pp-bag-8-1-1-.coco\\results_20241124_155336\\waste_detection_model\u001b[0m\n",
      "Starting training for 100 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      1/100         0G      1.614       3.13      1.589         57        640: 100%|██████████| 80/80 [01:09<00:00,  1.15it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:03<00:00,  1.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         79        333      0.892     0.0337      0.146     0.0629\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      2/100         0G       1.63      2.431      1.598         47        640: 100%|██████████| 80/80 [01:05<00:00,  1.23it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:03<00:00,  1.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         79        333      0.474      0.323      0.332      0.176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      3/100         0G       1.64      2.209      1.632         42        640: 100%|██████████| 80/80 [01:31<00:00,  1.15s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:03<00:00,  1.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         79        333      0.354      0.355      0.319      0.151\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "      4/100         0G      1.625       2.14      1.627         43        640: 100%|██████████| 80/80 [01:12<00:00,  1.10it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:04<00:00,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         79        333      0.401      0.385      0.351      0.187\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      5/100         0G      1.611       2.05      1.608         51        640: 100%|██████████| 80/80 [01:16<00:00,  1.04it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:04<00:00,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         79        333      0.532      0.371      0.403      0.183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      6/100         0G      1.593      1.999      1.596         46        640: 100%|██████████| 80/80 [01:17<00:00,  1.03it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:04<00:00,  1.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         79        333      0.473      0.433      0.421      0.222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      7/100         0G      1.576      1.916      1.567         33        640: 100%|██████████| 80/80 [01:10<00:00,  1.13it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:04<00:00,  1.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         79        333      0.504      0.486      0.434      0.219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      8/100         0G      1.572      1.868      1.598         33        640: 100%|██████████| 80/80 [01:23<00:00,  1.04s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:04<00:00,  1.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         79        333      0.391        0.4      0.418      0.201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      9/100         0G      1.554      1.843       1.59         44        640: 100%|██████████| 80/80 [01:10<00:00,  1.13it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:03<00:00,  1.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         79        333       0.49      0.527      0.492      0.245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     10/100         0G      1.518      1.766      1.548         39        640: 100%|██████████| 80/80 [01:08<00:00,  1.16it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:03<00:00,  1.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         79        333      0.526      0.545      0.504      0.249\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     11/100         0G      1.519      1.718      1.557         46        640: 100%|██████████| 80/80 [01:06<00:00,  1.20it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:03<00:00,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         79        333       0.51      0.515       0.52      0.258\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "     12/100         0G      1.526       1.68       1.57         44        640: 100%|██████████| 80/80 [01:05<00:00,  1.22it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:03<00:00,  1.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         79        333      0.519       0.54      0.489      0.241\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     13/100         0G      1.496       1.65      1.535         39        640: 100%|██████████| 80/80 [01:04<00:00,  1.23it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:03<00:00,  1.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         79        333      0.509      0.518      0.483      0.242\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "     14/100         0G      1.503      1.617      1.547         42        640: 100%|██████████| 80/80 [01:06<00:00,  1.20it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:03<00:00,  1.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         79        333      0.538      0.489       0.47      0.222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     15/100         0G      1.519       1.65      1.552         29        640: 100%|██████████| 80/80 [01:05<00:00,  1.23it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:03<00:00,  1.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         79        333      0.535      0.607      0.535      0.264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     16/100         0G      1.485      1.597      1.527         40        640: 100%|██████████| 80/80 [01:05<00:00,  1.22it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:03<00:00,  1.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         79        333      0.471      0.511      0.473      0.235\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "     17/100         0G      1.496      1.559      1.514         32        640: 100%|██████████| 80/80 [01:05<00:00,  1.22it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:03<00:00,  1.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         79        333      0.617      0.534      0.549      0.287\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     18/100         0G      1.476      1.537      1.506         69        640: 100%|██████████| 80/80 [01:04<00:00,  1.23it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:03<00:00,  1.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         79        333      0.625      0.527      0.561      0.281\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "     19/100         0G      1.471      1.513      1.511         50        640: 100%|██████████| 80/80 [01:05<00:00,  1.23it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:03<00:00,  1.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         79        333      0.611      0.586      0.576      0.294\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "     20/100         0G      1.443      1.454      1.492         49        640: 100%|██████████| 80/80 [01:04<00:00,  1.23it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:03<00:00,  1.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         79        333      0.525      0.566      0.554       0.28\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "     21/100         0G      1.448      1.485      1.509         34        640: 100%|██████████| 80/80 [01:05<00:00,  1.22it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:03<00:00,  1.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         79        333      0.503      0.541      0.503       0.25\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "     22/100         0G       1.43      1.467      1.495         31        640: 100%|██████████| 80/80 [01:08<00:00,  1.17it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:03<00:00,  1.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         79        333      0.505      0.617      0.541      0.265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     23/100         0G      1.448      1.451      1.514         43        640: 100%|██████████| 80/80 [01:05<00:00,  1.22it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:03<00:00,  1.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         79        333      0.597      0.548      0.549      0.249\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "     24/100         0G      1.433      1.412      1.498         50        640: 100%|██████████| 80/80 [01:04<00:00,  1.23it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:03<00:00,  1.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         79        333      0.587      0.581      0.501      0.257\n",
      "\u001b[34m\u001b[1mEarlyStopping: \u001b[0mTraining stopped early as no improvement observed in last 5 epochs. Best results observed at epoch 19, best model saved as best.pt.\n",
      "To update EarlyStopping(patience=5) pass a new patience value, i.e. `patience=300` or use `patience=0` to disable EarlyStopping.\n",
      "\n",
      "24 epochs completed in 0.489 hours.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizer stripped from C:\\Users\\test\\Desktop\\waste detection.v4-add-pp-bag-8-1-1-.coco\\results_20241124_155336\\waste_detection_model\\weights\\last.pt, 5.6MB\n",
      "Optimizer stripped from C:\\Users\\test\\Desktop\\waste detection.v4-add-pp-bag-8-1-1-.coco\\results_20241124_155336\\waste_detection_model\\weights\\best.pt, 5.6MB\n",
      "\n",
      "Validating C:\\Users\\test\\Desktop\\waste detection.v4-add-pp-bag-8-1-1-.coco\\results_20241124_155336\\waste_detection_model\\weights\\best.pt...\n",
      "Ultralytics 8.3.13  Python-3.12.7 torch-2.4.1+cpu CPU (12th Gen Intel Core(TM) i7-12700K)\n",
      "Model summary (fused): 186 layers, 2,685,148 parameters, 0 gradients, 6.8 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:02<00:00,  1.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         79        333      0.611      0.588      0.576      0.294\n",
      "     Large Waste Items         30         98      0.545      0.357      0.386       0.16\n",
      "                PP bag         11         57      0.686      0.536      0.574       0.23\n",
      "         General Waste         43         91       0.35      0.582      0.451      0.195\n",
      "              CleanNet         47         87      0.864      0.875      0.891      0.589\n",
      "Speed: 0.9ms preprocess, 27.9ms inference, 0.0ms loss, 0.6ms postprocess per image\n",
      "Results saved to \u001b[1mC:\\Users\\test\\Desktop\\waste detection.v4-add-pp-bag-8-1-1-.coco\\results_20241124_155336\\waste_detection_model\u001b[0m\n",
      "\n",
      "Evaluating model on test set...\n",
      "Ultralytics 8.3.13  Python-3.12.7 torch-2.4.1+cpu CPU (12th Gen Intel Core(TM) i7-12700K)\n",
      "Model summary (fused): 186 layers, 2,685,148 parameters, 0 gradients, 6.8 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\test\\Desktop\\waste detection.v4-add-pp-bag-8-1-1-.coco\\test\\labels... 79 images, 0 backgrounds, 0 corrupt: 100%|██████████| 79/79 [00:00<00:00, 1136.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: C:\\Users\\test\\Desktop\\waste detection.v4-add-pp-bag-8-1-1-.coco\\test\\labels.cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 10/10 [00:02<00:00,  3.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         79        428     0.0171     0.0233     0.0038   0.000755\n",
      "     Large Waste Items         25         91     0.0265      0.033    0.00561    0.00105\n",
      "                PP bag         11        102     0.0324     0.0294    0.00685    0.00123\n",
      "         General Waste         56        130    0.00928     0.0308    0.00246   0.000558\n",
      "              CleanNet         54        105          0          0   0.000296   0.000177\n",
      "Speed: 0.6ms preprocess, 25.8ms inference, 0.0ms loss, 0.7ms postprocess per image\n",
      "Results saved to \u001b[1mC:\\Users\\test\\Desktop\\waste detection.v4-add-pp-bag-8-1-1-.coco\\results_20241124_155336\\waste_detection_model\u001b[0m\n",
      "\n",
      "Running inference on test images...\n",
      "WARNING  'boxes' is deprecated and will be removed in in the future. Use 'show_boxes' instead.\n",
      "\n",
      "Error during training: '\u001b[31m\u001b[1mlabels\u001b[0m' is not a valid YOLO argument. Similar arguments are i.e. ['show_labels=True'].\n",
      "\n",
      "    Arguments received: ['yolo', '--f=c:\\\\Users\\\\test\\\\AppData\\\\Roaming\\\\jupyter\\\\runtime\\\\kernel-v36bdb5f63f9638af8525a96ab6f02b071430c51da.json']. Ultralytics 'yolo' commands use the following syntax:\n",
      "\n",
      "        yolo TASK MODE ARGS\n",
      "\n",
      "        Where   TASK (optional) is one of {'detect', 'segment', 'obb', 'pose', 'classify'}\n",
      "                MODE (required) is one of {'export', 'val', 'train', 'track', 'predict', 'benchmark'}\n",
      "                ARGS (optional) are any number of custom 'arg=value' pairs like 'imgsz=320' that override defaults.\n",
      "                    See all ARGS at https://docs.ultralytics.com/usage/cfg or with 'yolo cfg'\n",
      "\n",
      "    1. Train a detection model for 10 epochs with an initial learning_rate of 0.01\n",
      "        yolo train data=coco8.yaml model=yolo11n.pt epochs=10 lr0=0.01\n",
      "\n",
      "    2. Predict a YouTube video using a pretrained segmentation model at image size 320:\n",
      "        yolo predict model=yolo11n-seg.pt source='https://youtu.be/LNwODJXcvt4' imgsz=320\n",
      "\n",
      "    3. Val a pretrained detection model at batch-size 1 and image size 640:\n",
      "        yolo val model=yolo11n.pt data=coco8.yaml batch=1 imgsz=640\n",
      "\n",
      "    4. Export a YOLO11n classification model to ONNX format at image size 224 by 128 (no TASK required)\n",
      "        yolo export model=yolo11n-cls.pt format=onnx imgsz=224,128\n",
      "    \n",
      "    5. Streamlit real-time webcam inference GUI\n",
      "        yolo streamlit-predict\n",
      "        \n",
      "    6. Run special commands:\n",
      "        yolo help\n",
      "        yolo checks\n",
      "        yolo version\n",
      "        yolo settings\n",
      "        yolo copy-cfg\n",
      "        yolo cfg\n",
      "\n",
      "    Docs: https://docs.ultralytics.com\n",
      "    Community: https://community.ultralytics.com\n",
      "    GitHub: https://github.com/ultralytics/ultralytics\n",
      "    \n"
     ]
    },
    {
     "ename": "SyntaxError",
     "evalue": "'\u001b[31m\u001b[1mlabels\u001b[0m' is not a valid YOLO argument. Similar arguments are i.e. ['show_labels=True'].\n\n    Arguments received: ['yolo', '--f=c:\\\\Users\\\\test\\\\AppData\\\\Roaming\\\\jupyter\\\\runtime\\\\kernel-v36bdb5f63f9638af8525a96ab6f02b071430c51da.json']. Ultralytics 'yolo' commands use the following syntax:\n\n        yolo TASK MODE ARGS\n\n        Where   TASK (optional) is one of {'detect', 'segment', 'obb', 'pose', 'classify'}\n                MODE (required) is one of {'export', 'val', 'train', 'track', 'predict', 'benchmark'}\n                ARGS (optional) are any number of custom 'arg=value' pairs like 'imgsz=320' that override defaults.\n                    See all ARGS at https://docs.ultralytics.com/usage/cfg or with 'yolo cfg'\n\n    1. Train a detection model for 10 epochs with an initial learning_rate of 0.01\n        yolo train data=coco8.yaml model=yolo11n.pt epochs=10 lr0=0.01\n\n    2. Predict a YouTube video using a pretrained segmentation model at image size 320:\n        yolo predict model=yolo11n-seg.pt source='https://youtu.be/LNwODJXcvt4' imgsz=320\n\n    3. Val a pretrained detection model at batch-size 1 and image size 640:\n        yolo val model=yolo11n.pt data=coco8.yaml batch=1 imgsz=640\n\n    4. Export a YOLO11n classification model to ONNX format at image size 224 by 128 (no TASK required)\n        yolo export model=yolo11n-cls.pt format=onnx imgsz=224,128\n    \n    5. Streamlit real-time webcam inference GUI\n        yolo streamlit-predict\n        \n    6. Run special commands:\n        yolo help\n        yolo checks\n        yolo version\n        yolo settings\n        yolo copy-cfg\n        yolo cfg\n\n    Docs: https://docs.ultralytics.com\n    Community: https://community.ultralytics.com\n    GitHub: https://github.com/ultralytics/ultralytics\n     (<string>)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[1;36m(most recent call last)\u001b[0m:\n",
      "\u001b[0m  File \u001b[0;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\IPython\\core\\interactiveshell.py:3577\u001b[0m in \u001b[0;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\u001b[0m\n",
      "\u001b[0m  Cell \u001b[0;32mIn[2], line 382\u001b[0m\n    main()\u001b[0m\n",
      "\u001b[0m  Cell \u001b[0;32mIn[2], line 379\u001b[0m in \u001b[0;35mmain\u001b[0m\n    trainer.train(base_path)\u001b[0m\n",
      "\u001b[0m  Cell \u001b[0;32mIn[2], line 300\u001b[0m in \u001b[0;35mtrain\u001b[0m\n    predictions = model.predict(\u001b[0m\n",
      "\u001b[0m  File \u001b[0;32mc:\\Users\\test\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ultralytics\\engine\\model.py:546\u001b[0m in \u001b[0;35mpredict\u001b[0m\n    self.predictor = (predictor or self._smart_load(\"predictor\"))(overrides=args, _callbacks=self.callbacks)\u001b[0m\n",
      "\u001b[0m  File \u001b[0;32mc:\\Users\\test\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ultralytics\\engine\\predictor.py:88\u001b[0m in \u001b[0;35m__init__\u001b[0m\n    self.args = get_cfg(cfg, overrides)\u001b[0m\n",
      "\u001b[0m  File \u001b[0;32mc:\\Users\\test\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ultralytics\\cfg\\__init__.py:252\u001b[0m in \u001b[0;35mget_cfg\u001b[0m\n    check_dict_alignment(cfg, overrides)\u001b[0m\n",
      "\u001b[1;36m  File \u001b[1;32mc:\\Users\\test\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ultralytics\\cfg\\__init__.py:437\u001b[1;36m in \u001b[1;35mcheck_dict_alignment\u001b[1;36m\n\u001b[1;33m    raise SyntaxError(string + CLI_HELP_MSG) from e\u001b[1;36m\n",
      "\u001b[1;36m  File \u001b[1;32m<string>\u001b[1;36m\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m '\u001b[31m\u001b[1mlabels\u001b[0m' is not a valid YOLO argument. Similar arguments are i.e. ['show_labels=True'].\n\n    Arguments received: ['yolo', '--f=c:\\\\Users\\\\test\\\\AppData\\\\Roaming\\\\jupyter\\\\runtime\\\\kernel-v36bdb5f63f9638af8525a96ab6f02b071430c51da.json']. Ultralytics 'yolo' commands use the following syntax:\n\n        yolo TASK MODE ARGS\n\n        Where   TASK (optional) is one of {'detect', 'segment', 'obb', 'pose', 'classify'}\n                MODE (required) is one of {'export', 'val', 'train', 'track', 'predict', 'benchmark'}\n                ARGS (optional) are any number of custom 'arg=value' pairs like 'imgsz=320' that override defaults.\n                    See all ARGS at https://docs.ultralytics.com/usage/cfg or with 'yolo cfg'\n\n    1. Train a detection model for 10 epochs with an initial learning_rate of 0.01\n        yolo train data=coco8.yaml model=yolo11n.pt epochs=10 lr0=0.01\n\n    2. Predict a YouTube video using a pretrained segmentation model at image size 320:\n        yolo predict model=yolo11n-seg.pt source='https://youtu.be/LNwODJXcvt4' imgsz=320\n\n    3. Val a pretrained detection model at batch-size 1 and image size 640:\n        yolo val model=yolo11n.pt data=coco8.yaml batch=1 imgsz=640\n\n    4. Export a YOLO11n classification model to ONNX format at image size 224 by 128 (no TASK required)\n        yolo export model=yolo11n-cls.pt format=onnx imgsz=224,128\n    \n    5. Streamlit real-time webcam inference GUI\n        yolo streamlit-predict\n        \n    6. Run special commands:\n        yolo help\n        yolo checks\n        yolo version\n        yolo settings\n        yolo copy-cfg\n        yolo cfg\n\n    Docs: https://docs.ultralytics.com\n    Community: https://community.ultralytics.com\n    GitHub: https://github.com/ultralytics/ultralytics\n    \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from ultralytics import YOLO\n",
    "import yaml\n",
    "import json\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import shutil\n",
    "import cv2\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# 소분류-대분류 매핑 정의\n",
    "category_mapping = {\n",
    "    # 대형폐기물 (Large Waste Items)\n",
    "    'arcade machine': 'Large Waste Items', 'Audio': 'Large Waste Items', 'Computer': 'Large Waste Items',\n",
    "    'fax machine': 'Large Waste Items', 'Main unit': 'Large Waste Items', 'Monitor': 'Large Waste Items',\n",
    "    'Printer': 'Large Waste Items', 'sewing machine': 'Large Waste Items', 'Speaker': 'Large Waste Items',\n",
    "    'typewriter': 'Large Waste Items', 'Vacuum cleaner': 'Large Waste Items', 'Video player': 'Large Waste Items',\n",
    "    'Bathtub': 'Large Waste Items', 'Sink': 'Large Waste Items', 'Kitchen sink': 'Large Waste Items',\n",
    "    'Toilet bowl': 'Large Waste Items', 'Bed': 'Large Waste Items', 'Bookcase': 'Large Waste Items',\n",
    "    'Bookstand': 'Large Waste Items', 'Cabinet': 'Large Waste Items', 'chair': 'Large Waste Items',\n",
    "    'Cupboard': 'Large Waste Items', 'Desk': 'Large Waste Items', 'Dining table': 'Large Waste Items',\n",
    "    'Display cabinet': 'Large Waste Items', 'Display stand': 'Large Waste Items', 'Drawer unit': 'Large Waste Items',\n",
    "    'Shoe rack': 'Large Waste Items', 'Small cabinet': 'Large Waste Items', 'Sofa': 'Large Waste Items',\n",
    "    'Table': 'Large Waste Items', 'TV stand': 'Large Waste Items', 'Vanity table': 'Large Waste Items',\n",
    "    'Wardrobe': 'Large Waste Items', 'Air conditioner': 'Large Waste Items', 'Air purifier': 'Large Waste Items',\n",
    "    'dish dryer': 'Large Waste Items', 'Electric rice cooker': 'Large Waste Items', 'Fan': 'Large Waste Items',\n",
    "    'Gas oven range': 'Large Waste Items', 'Heater': 'Large Waste Items', 'Humidifier': 'Large Waste Items',\n",
    "    'Microwave': 'Large Waste Items', 'refrigerator': 'Large Waste Items', 'Spin dryer': 'Large Waste Items',\n",
    "    'TV': 'Large Waste Items', 'Washing machine': 'Large Waste Items', 'Aquarium': 'Large Waste Items',\n",
    "    'Bamboo mat': 'Large Waste Items', 'Bedding items': 'Large Waste Items', 'bicycle': 'Large Waste Items',\n",
    "    'Carpet': 'Large Waste Items', 'Clothes drying rack': 'Large Waste Items', 'Coat rack': 'Large Waste Items',\n",
    "    'Door panel': 'Large Waste Items', 'Earthenware jar': 'Large Waste Items', 'Floor covering': 'Large Waste Items',\n",
    "    'Frame': 'Large Waste Items', 'lumber': 'Large Waste Items', 'Mannequin': 'Large Waste Items',\n",
    "    'Mat': 'Large Waste Items', 'Piano': 'Large Waste Items', 'Rice storage container': 'Large Waste Items',\n",
    "    'Signboard': 'Large Waste Items', 'Stroller': 'Large Waste Items', 'Wall clock': 'Large Waste Items',\n",
    "    'Water tank': 'Large Waste Items', 'audio cabinet': 'Large Waste Items', 'suitcase': 'Large Waste Items',\n",
    "    \n",
    "    'PP bag': 'PP bag',\n",
    "    'General waste bag': 'General Waste',\n",
    "    'waste pile': 'General Waste',\n",
    "    'CleanNet': 'CleanNet'\n",
    "}\n",
    "\n",
    "class WasteDetectionTrainer:\n",
    "    def __init__(self):\n",
    "        # 대분류 클래스 및 색상 정의\n",
    "        self.major_classes = ['Large Waste Items', 'PP bag', 'General Waste', 'CleanNet']\n",
    "        self.class_to_idx = {name: idx for idx, name in enumerate(self.major_classes)}\n",
    "        \n",
    "        # 색상 정의 (대분류별로 다른 색상 사용)\n",
    "        self.category_colors = {\n",
    "            'Large Waste Items': (255, 0, 0),    # 빨강\n",
    "            'PP bag': (0, 255, 0),               # 초록\n",
    "            'General Waste': (0, 0, 255),        # 파랑\n",
    "            'CleanNet': (255, 255, 0)            # 노랑\n",
    "        }\n",
    "\n",
    "    def verify_files(self, base_path):\n",
    "        \"\"\"파일 경로 검증\"\"\"\n",
    "        required_files = {\n",
    "            'Train Annotations': os.path.join(base_path, \"train\", \"train_annotations_yolo.json\"),\n",
    "            'Valid Annotations': os.path.join(base_path, \"valid\", \"valid_annotations_yolo.json\"),\n",
    "            'Test Annotations': os.path.join(base_path, \"test\", \"test_annotations_yolo.json\"),\n",
    "            'Train Images': os.path.join(base_path, \"train\", \"images\"),\n",
    "            'Valid Images': os.path.join(base_path, \"valid\", \"images\"),\n",
    "            'Test Images': os.path.join(base_path, \"test\", \"images\")\n",
    "        }\n",
    "        \n",
    "        for name, path in required_files.items():\n",
    "            if not os.path.exists(path):\n",
    "                raise FileNotFoundError(f\"{name} not found at: {path}\")\n",
    "            print(f\"Found {name} at: {path}\")\n",
    "        \n",
    "        return required_files\n",
    "\n",
    "    def setup_directory_structure(self, base_path):\n",
    "        \"\"\"디렉토리 구조 설정\"\"\"\n",
    "        for split in ['train', 'valid', 'test']:\n",
    "            labels_dir = os.path.join(base_path, split, 'labels')\n",
    "            os.makedirs(labels_dir, exist_ok=True)\n",
    "\n",
    "    def convert_annotations_with_mapping(self, json_path, output_dir):\n",
    "        \"\"\"소분류를 대분류로 매핑하여 YOLO 형식으로 변환\"\"\"\n",
    "        try:\n",
    "            with open(json_path, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "            \n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "            converted_count = 0\n",
    "            processed_files = set()\n",
    "            errors = []\n",
    "            \n",
    "            for image_name, annotations in data.items():\n",
    "                txt_path = os.path.join(output_dir, Path(image_name).stem + '.txt')\n",
    "                \n",
    "                valid_annotations = []\n",
    "                for ann in annotations:\n",
    "                    try:\n",
    "                        class_name = ann['class_name']\n",
    "                        if class_name not in category_mapping:\n",
    "                            errors.append(f\"Unknown class: {class_name} in {image_name}\")\n",
    "                            continue\n",
    "                            \n",
    "                        major_category = category_mapping[class_name]\n",
    "                        if major_category not in self.class_to_idx:\n",
    "                            errors.append(f\"Unknown major category: {major_category} in {image_name}\")\n",
    "                            continue\n",
    "                            \n",
    "                        class_idx = self.class_to_idx[major_category]\n",
    "                        x_center, y_center, width, height = ann['bbox']\n",
    "                        \n",
    "                        if not all(0 <= coord <= 1 for coord in [x_center, y_center, width, height]):\n",
    "                            errors.append(f\"Invalid bbox coordinates in {image_name}: {ann['bbox']}\")\n",
    "                            continue\n",
    "                            \n",
    "                        valid_annotations.append((class_idx, x_center, y_center, width, height))\n",
    "                        \n",
    "                    except KeyError as e:\n",
    "                        errors.append(f\"Missing key {e} in annotation of {image_name}\")\n",
    "                    except Exception as e:\n",
    "                        errors.append(f\"Error processing annotation in {image_name}: {str(e)}\")\n",
    "                \n",
    "                if valid_annotations:\n",
    "                    with open(txt_path, 'w', encoding='utf-8') as f:\n",
    "                        for ann in valid_annotations:\n",
    "                            f.write(f\"{ann[0]} {ann[1]} {ann[2]} {ann[3]} {ann[4]}\\n\")\n",
    "                        converted_count += len(valid_annotations)\n",
    "                        processed_files.add(image_name)\n",
    "            \n",
    "            print(f\"Successfully converted {converted_count} annotations for {len(processed_files)} images\")\n",
    "            if errors:\n",
    "                print(\"\\nConversion errors occurred:\")\n",
    "                for error in errors[:10]:\n",
    "                    print(error)\n",
    "                if len(errors) > 10:\n",
    "                    print(f\"...and {len(errors) - 10} more errors\")\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {json_path}: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def create_data_yaml(self, base_path, yaml_path):\n",
    "        \"\"\"YAML 설정 파일 생성\"\"\"\n",
    "        data = {\n",
    "            'path': base_path,\n",
    "            'train': os.path.join('train', 'images'),\n",
    "            'val': os.path.join('valid', 'images'),\n",
    "            'test': os.path.join('test', 'images'),\n",
    "            'nc': len(self.major_classes),\n",
    "            'names': self.major_classes\n",
    "        }\n",
    "        \n",
    "        with open(yaml_path, 'w', encoding='utf-8') as f:\n",
    "            yaml.dump(data, f, default_flow_style=False, allow_unicode=True)\n",
    "        \n",
    "        print(f\"\\nCreated YAML file at: {yaml_path}\")\n",
    "        print(f\"Number of classes: {len(self.major_classes)}\")\n",
    "        print(\"Classes in order:\")\n",
    "        for idx, name in enumerate(self.major_classes):\n",
    "            print(f\"{idx}: {name}\")\n",
    "\n",
    "    def create_augmentation_config(self):\n",
    "        \"\"\"데이터 증강 설정\"\"\"\n",
    "        return {\n",
    "            'hsv_h': 0.015,  # HSV 색조 증강\n",
    "            'hsv_s': 0.7,    # HSV 채도 증강\n",
    "            'hsv_v': 0.4,    # HSV 명도 증강\n",
    "            'degrees': 10.0,  # 회전 각도\n",
    "            'translate': 0.1, # 이미지 이동\n",
    "            'scale': 0.5,    # 스케일 조정\n",
    "            'fliplr': 0.5,   # 좌우 반전\n",
    "            'mosaic': 1.0    # 모자이크 증강\n",
    "        }\n",
    "\n",
    "    def calculate_metrics(self, true_labels, pred_labels):\n",
    "        \"\"\"분류별 평가 지표 계산\"\"\"\n",
    "        confusion_mat = confusion_matrix(true_labels, pred_labels, \n",
    "                                       labels=range(len(self.major_classes)))\n",
    "        metrics = {}\n",
    "        \n",
    "        for i, class_name in enumerate(self.major_classes):\n",
    "            tp = confusion_mat[i, i]\n",
    "            fp = confusion_mat[:, i].sum() - tp\n",
    "            fn = confusion_mat[i, :].sum() - tp\n",
    "            \n",
    "            precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "            recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "            f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "            \n",
    "            metrics[class_name] = {\n",
    "                'precision': precision,\n",
    "                'recall': recall,\n",
    "                'f1': f1,\n",
    "                'TP': int(tp),\n",
    "                'FP': int(fp),\n",
    "                'FN': int(fn)\n",
    "            }\n",
    "        \n",
    "        return metrics, confusion_mat\n",
    "\n",
    "    def plot_confusion_matrix(self, confusion_mat, save_path):\n",
    "        \"\"\"혼동 행렬 시각화\"\"\"\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(confusion_mat, annot=True, fmt='d',\n",
    "                    xticklabels=self.major_classes,\n",
    "                    yticklabels=self.major_classes)\n",
    "        plt.title('Confusion Matrix')\n",
    "        plt.ylabel('True Label')\n",
    "        plt.xlabel('Predicted Label')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(save_path)\n",
    "        plt.close()\n",
    "\n",
    "    def draw_predictions(self, image_path, predictions, output_path):\n",
    "        \"\"\"예측 결과 시각화\"\"\"\n",
    "        image = cv2.imread(image_path)\n",
    "        for pred in predictions:\n",
    "            class_id = int(pred['class'])\n",
    "            class_name = self.major_classes[class_id]\n",
    "            color = self.category_colors[class_name]\n",
    "            \n",
    "            bbox = pred['bbox']\n",
    "            x1, y1, x2, y2 = [int(coord) for coord in bbox]\n",
    "            \n",
    "            # 박스 그리기\n",
    "            cv2.rectangle(image, (x1, y1), (x2, y2), color, 2)\n",
    "            \n",
    "            # 라벨 그리기\n",
    "            label = f\"{class_name} {pred['confidence']:.2f}\"\n",
    "            cv2.putText(image, label, (x1, y1-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "        \n",
    "        cv2.imwrite(output_path, image)\n",
    "\n",
    "    def train(self, base_path):\n",
    "        \"\"\"전체 학습 프로세스\"\"\"\n",
    "        print(\"Starting training process...\")\n",
    "        \n",
    "        # 파일 경로 검증\n",
    "        required_files = self.verify_files(base_path)\n",
    "        \n",
    "        # 결과 디렉토리 생성\n",
    "        results_dir = os.path.join(base_path, f\"results_{datetime.now().strftime('%Y%m%d_%H%M%S')}\")\n",
    "        os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "        # 디렉토리 구조 설정\n",
    "        self.setup_directory_structure(base_path)\n",
    "\n",
    "        # YAML 파일 생성\n",
    "        yaml_path = os.path.join(base_path, \"data.yaml\")\n",
    "        self.create_data_yaml(base_path, yaml_path)\n",
    "\n",
    "        # 소분류를 대분류로 변환하여 어노테이션 생성\n",
    "        for split, json_path in {\n",
    "            'train': required_files['Train Annotations'],\n",
    "            'valid': required_files['Valid Annotations'],\n",
    "            'test': required_files['Test Annotations']\n",
    "        }.items():\n",
    "            labels_dir = os.path.join(base_path, split, 'labels')\n",
    "            print(f\"\\nConverting annotations for {split} set...\")\n",
    "            self.convert_annotations_with_mapping(json_path, labels_dir)\n",
    "\n",
    "        # 모델 초기화 및 학습\n",
    "        print(\"\\nInitializing model...\")\n",
    "        model = YOLO('yolov8n.pt')\n",
    "        augmentation_config = self.create_augmentation_config()\n",
    "\n",
    "        try:\n",
    "            # 모델 학습\n",
    "            print(\"\\nStarting model training...\")\n",
    "            results = model.train(\n",
    "                data=yaml_path,\n",
    "                epochs=100,\n",
    "                imgsz=640,\n",
    "                batch=8,\n",
    "                name='waste_detection_model',\n",
    "                project=results_dir,\n",
    "                exist_ok=True,\n",
    "                workers=4,\n",
    "                patience=5,\n",
    "                **augmentation_config\n",
    "            )\n",
    "\n",
    "            # 테스트 셋 평가\n",
    "            print(\"\\nEvaluating model on test set...\")\n",
    "            test_results = model.val(\n",
    "                data=yaml_path,\n",
    "                split='test'\n",
    "            )\n",
    "\n",
    "            # 테스트 이미지에 대한 예측 수행\n",
    "            print(\"\\nRunning inference on test images...\")\n",
    "            test_images_dir = os.path.join(base_path, \"test\", \"images\")\n",
    "            visualization_dir = os.path.join(results_dir, \"visualizations\")\n",
    "            os.makedirs(visualization_dir, exist_ok=True)\n",
    "\n",
    "            predictions = model.predict(\n",
    "                source=test_images_dir,\n",
    "                save=True,\n",
    "                save_txt=True,\n",
    "                conf=0.25,\n",
    "                project=results_dir,\n",
    "                name='inference',\n",
    "                boxes=True,\n",
    "                labels=True\n",
    "            )\n",
    "\n",
    "            # 예측 결과 시각화\n",
    "            print(\"\\nVisualizing predictions...\")\n",
    "            for image_file in os.listdir(test_images_dir):\n",
    "                if image_file.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp')):\n",
    "                    image_path = os.path.join(test_images_dir, image_file)\n",
    "                    output_path = os.path.join(visualization_dir, f\"pred_{image_file}\")\n",
    "                    \n",
    "                    image_predictions = model.predict(source=image_path, conf=0.25)[0]\n",
    "                    pred_list = []\n",
    "                    for box in image_predictions.boxes:\n",
    "                        pred_list.append({\n",
    "                            'class': box.cls.item(),\n",
    "                            'confidence': box.conf.item(),\n",
    "                            'bbox': box.xyxy[0].tolist()\n",
    "                        })\n",
    "                    self.draw_predictions(image_path, pred_list, output_path)\n",
    "\n",
    "            # 평가 메트릭스 계산\n",
    "            print(\"\\nCalculating evaluation metrics...\")\n",
    "            true_labels = []\n",
    "            pred_labels = []\n",
    "            \n",
    "            test_label_dir = os.path.join(base_path, \"test\", \"labels\")\n",
    "            pred_label_dir = os.path.join(results_dir, \"inference\", \"labels\")\n",
    "            \n",
    "            for pred_file in os.listdir(pred_label_dir):\n",
    "                true_file = os.path.join(test_label_dir, pred_file)\n",
    "                pred_file_path = os.path.join(pred_label_dir, pred_file)\n",
    "                \n",
    "                if os.path.exists(true_file):\n",
    "                    with open(true_file, 'r') as f:\n",
    "                        for line in f:\n",
    "                            true_labels.append(int(line.split()[0]))\n",
    "                    \n",
    "                    with open(pred_file_path, 'r') as f:\n",
    "                        for line in f:\n",
    "                            pred_labels.append(int(line.split()[0]))\n",
    "\n",
    "            # 메트릭스 계산 및 저장\n",
    "            metrics, conf_matrix = self.calculate_metrics(true_labels, pred_labels)\n",
    "            \n",
    "            # 결과 저장\n",
    "            print(\"\\nSaving evaluation results...\")\n",
    "            results_file = os.path.join(results_dir, 'evaluation_results.txt')\n",
    "            with open(results_file, 'w') as f:\n",
    "                f.write(\"Evaluation Results:\\n\\n\")\n",
    "                for class_name, class_metrics in metrics.items():\n",
    "                    f.write(f\"\\n{class_name}:\\n\")\n",
    "                    for metric_name, value in class_metrics.items():\n",
    "                        f.write(f\"{metric_name}: {value:.4f}\\n\")\n",
    "\n",
    "            # 혼동 행렬 시각화 저장\n",
    "            self.plot_confusion_matrix(conf_matrix, os.path.join(results_dir, 'confusion_matrix.png'))\n",
    "\n",
    "            print(f\"\\nTraining and evaluation completed successfully!\")\n",
    "            print(f\"Results saved in: {results_dir}\")\n",
    "            print(f\"Visualizations saved in: {visualization_dir}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"\\nError during training: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "def main():\n",
    "    # 기본 경로 설정\n",
    "    base_path = r\"C:\\Users\\test\\Desktop\\waste detection.v4-add-pp-bag-8-1-1-.coco\"\n",
    "    \n",
    "    # 트레이너 초기화 및 학습 실행\n",
    "    trainer = WasteDetectionTrainer()\n",
    "    trainer.train(base_path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
