{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 현재 코드는 4개의 대분류 클레스에 대해 평균값이 어떻게 되는지 확인하고자 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: 필요한 라이브러리 임포트\n",
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: 카테고리 매핑 정의\n",
    "category_mapping = {\n",
    "    # 대형폐기물 (Large Waste Items)\n",
    "    'arcade machine': 'Large Waste Items',\n",
    "    'Audio': 'Large Waste Items',\n",
    "    'Computer': 'Large Waste Items',\n",
    "    'fax machine': 'Large Waste Items',\n",
    "    'Main unit': 'Large Waste Items',\n",
    "    'Monitor': 'Large Waste Items',\n",
    "    'Printer': 'Large Waste Items',\n",
    "    'sewing machine': 'Large Waste Items',\n",
    "    'Speaker': 'Large Waste Items',\n",
    "    'typewriter': 'Large Waste Items',\n",
    "    'Vacuum cleaner': 'Large Waste Items',\n",
    "    'Video player': 'Large Waste Items',\n",
    "    'Bathtub': 'Large Waste Items',\n",
    "    'Sink': 'Large Waste Items',\n",
    "    'Kitchen sink': 'Large Waste Items',\n",
    "    'Toilet bowl': 'Large Waste Items',\n",
    "    'Bed': 'Large Waste Items',\n",
    "    'Bookcase': 'Large Waste Items',\n",
    "    'Bookstand': 'Large Waste Items',\n",
    "    'Cabinet': 'Large Waste Items',\n",
    "    'chair': 'Large Waste Items',\n",
    "    'Cupboard': 'Large Waste Items',\n",
    "    'Desk': 'Large Waste Items',\n",
    "    'Dining table': 'Large Waste Items',\n",
    "    'Display cabinet': 'Large Waste Items',\n",
    "    'Display stand': 'Large Waste Items',\n",
    "    'Drawer unit': 'Large Waste Items',\n",
    "    'Shoe rack': 'Large Waste Items',\n",
    "    'Small cabinet': 'Large Waste Items',\n",
    "    'Sofa': 'Large Waste Items',\n",
    "    'Table': 'Large Waste Items',\n",
    "    'TV stand': 'Large Waste Items',\n",
    "    'Vanity table': 'Large Waste Items',\n",
    "    'Wardrobe': 'Large Waste Items',\n",
    "    'Air conditioner': 'Large Waste Items',\n",
    "    'Air purifier': 'Large Waste Items',\n",
    "    'dish dryer': 'Large Waste Items',\n",
    "    'Electric rice cooker': 'Large Waste Items',\n",
    "    'Fan': 'Large Waste Items',\n",
    "    'Gas oven range': 'Large Waste Items',\n",
    "    'Heater': 'Large Waste Items',\n",
    "    'Humidifier': 'Large Waste Items',\n",
    "    'Microwave': 'Large Waste Items',\n",
    "    'refrigerator': 'Large Waste Items',\n",
    "    'Spin dryer': 'Large Waste Items',\n",
    "    'TV': 'Large Waste Items',\n",
    "    'Washing machine': 'Large Waste Items',\n",
    "    'Aquarium': 'Large Waste Items',\n",
    "    'Bamboo mat': 'Large Waste Items',\n",
    "    'Bedding items': 'Large Waste Items',\n",
    "    'bicycle': 'Large Waste Items',\n",
    "    'Carpet': 'Large Waste Items',\n",
    "    'Clothes drying rack': 'Large Waste Items',\n",
    "    'Coat rack': 'Large Waste Items',\n",
    "    'Door panel': 'Large Waste Items',\n",
    "    'Earthenware jar': 'Large Waste Items',\n",
    "    'Floor covering': 'Large Waste Items',\n",
    "    'Frame': 'Large Waste Items',\n",
    "    'lumber': 'Large Waste Items',\n",
    "    'Mannequin': 'Large Waste Items',\n",
    "    'Mat': 'Large Waste Items',\n",
    "    'Piano': 'Large Waste Items',\n",
    "    'Rice storage container': 'Large Waste Items',\n",
    "    'Signboard': 'Large Waste Items',\n",
    "    'Stroller': 'Large Waste Items',\n",
    "    'Wall clock': 'Large Waste Items',\n",
    "    'Water tank': 'Large Waste Items',\n",
    "    'audio cabinet': 'Large Waste Items',\n",
    "    'suitcase': 'Large Waste Items',\n",
    "    \n",
    "    # 기타 카테고리\n",
    "    'PP bag': 'PP bag',\n",
    "    'General waste bag': 'General Waste',\n",
    "    'waste pile': 'General Waste',\n",
    "    'CleanNet': 'CleanNet'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: 폴더 내 모든 이미지 객체 감지용 함수\n",
    "def detect_objects(model_path, image_dir):\n",
    "    # 모델 로드\n",
    "    model = YOLO(model_path)\n",
    "    \n",
    "    # 출력 디렉토리 생성\n",
    "    output_dir = './runs/detect/predict/'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # 모든 이미지 파일 가져오기 (jpg, jpeg, png)\n",
    "    image_paths = []\n",
    "    for ext in ['*.jpg', '*.jpeg', '*.png']:\n",
    "        image_paths.extend(list(Path(image_dir).glob(ext)))\n",
    "    \n",
    "    print(f\"Found {len(image_paths)} images in the directory.\")\n",
    "    \n",
    "    results = []\n",
    "    for idx, image_path in enumerate(image_paths, 1):\n",
    "        print(f\"\\nProcessing image {idx}/{len(image_paths)}: {image_path.name}\")\n",
    "        result = model.predict(source=str(image_path), save=True, device='cpu')\n",
    "        results.append(result[0])\n",
    "        \n",
    "        # 각 이미지의 감지 결과 출력\n",
    "        detected_classes = []\n",
    "        confidences = []\n",
    "        for box, cls in zip(result[0].boxes.conf, result[0].boxes.cls):\n",
    "            class_name = result[0].names[int(cls)]\n",
    "            confidence = float(box)\n",
    "            detected_classes.append(class_name)\n",
    "            confidences.append(confidence)\n",
    "            print(f\"- {class_name}: {confidence:.3f}\")\n",
    "        \n",
    "        # 처리 속도 정보 출력\n",
    "        preprocess_time = result[0].speed['preprocess']\n",
    "        inference_time = result[0].speed['inference']\n",
    "        postprocess_time = result[0].speed['postprocess']\n",
    "        print(f\"Processing times:\")\n",
    "        print(f\"- Preprocess: {preprocess_time}ms\")\n",
    "        print(f\"- Inference: {inference_time}ms\")\n",
    "        print(f\"- Postprocess: {postprocess_time}ms\")\n",
    "        \n",
    "    print(f\"\\nAll results saved to {output_dir}\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: JSON 라벨 변환 함수\n",
    "def convert_labels_to_major_category(json_path, category_mapping):\n",
    "    # JSON 파일 로드\n",
    "    with open(json_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    converted_data = {}\n",
    "    for image_name, annotations in data.items():\n",
    "        converted_annotations = []\n",
    "        for ann in annotations:\n",
    "            class_name = ann[\"class_name\"]\n",
    "            if class_name in category_mapping:\n",
    "                major_category = category_mapping[class_name]\n",
    "                converted_annotations.append({\n",
    "                    \"class_name\": major_category,\n",
    "                    \"bbox\": ann[\"bbox\"]\n",
    "                })\n",
    "        converted_data[image_name] = converted_annotations\n",
    "    \n",
    "    return converted_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: 이진 분류 함수\n",
    "def classify_images_by_major_category(converted_labels, target_class):\n",
    "    binary_classification = {}\n",
    "    \n",
    "    for image_name, annotations in converted_labels.items():\n",
    "        if any(ann[\"class_name\"] == target_class for ann in annotations):\n",
    "            binary_classification[image_name] = \"Positive\"\n",
    "        else:\n",
    "            binary_classification[image_name] = \"Negative\"\n",
    "            \n",
    "        # 상세 정보 출력\n",
    "        detected_classes = [ann[\"class_name\"] for ann in annotations]\n",
    "        print(f\"\\nImage: {image_name}\")\n",
    "        print(f\"Detected classes: {detected_classes}\")\n",
    "        print(f\"Classification: {binary_classification[image_name]}\")\n",
    "    \n",
    "    return binary_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: 결과 저장 함수\n",
    "def save_results(binary_classification, output_path):\n",
    "    results_summary = {\n",
    "        \"total_images\": len(binary_classification),\n",
    "        \"positive_count\": sum(1 for v in binary_classification.values() if v == \"Positive\"),\n",
    "        \"negative_count\": sum(1 for v in binary_classification.values() if v == \"Negative\"),\n",
    "        \"classifications\": binary_classification\n",
    "    }\n",
    "    \n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(results_summary, f, indent=4, ensure_ascii=False)\n",
    "    \n",
    "    return results_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Confidence Score 분석 함수\n",
    "def calculate_confidence_scores(results):\n",
    "    # 각 클래스별 confidence scores를 저장할 딕셔너리\n",
    "    class_confidences = {\n",
    "        'Large Waste Items': [],\n",
    "        'PP bag': [],\n",
    "        'General Waste': [],\n",
    "        'CleanNet': []\n",
    "    }\n",
    "    \n",
    "    # 모든 결과에 대해 순회\n",
    "    for result in results:\n",
    "        boxes = result.boxes\n",
    "        for box, cls in zip(boxes.conf, boxes.cls):\n",
    "            class_name = result.names[int(cls)]\n",
    "            # 소분류를 대분류로 변환\n",
    "            major_class = category_mapping.get(class_name, class_name)\n",
    "            # confidence score 추가\n",
    "            if major_class in class_confidences:\n",
    "                class_confidences[major_class].append(float(box))\n",
    "    \n",
    "    # 각 클래스별 통계 계산\n",
    "    statistics = {}\n",
    "    for class_name, scores in class_confidences.items():\n",
    "        if scores:  # 해당 클래스가 감지된 경우에만\n",
    "            statistics[class_name] = {\n",
    "                'mean': sum(scores) / len(scores),\n",
    "                'max': max(scores),\n",
    "                'min': min(scores),\n",
    "                'count': len(scores),\n",
    "                'all_scores': scores  # 모든 점수를 저장\n",
    "            }\n",
    "        else:\n",
    "            statistics[class_name] = {\n",
    "                'mean': 0,\n",
    "                'max': 0,\n",
    "                'min': 0,\n",
    "                'count': 0,\n",
    "                'all_scores': []\n",
    "            }\n",
    "    \n",
    "    # 결과 출력\n",
    "    print(\"\\nConfidence Score Statistics by Class:\")\n",
    "    for class_name, stats in statistics.items():\n",
    "        print(f\"\\n{class_name}:\")\n",
    "        print(f\"- Number of detections: {stats['count']}\")\n",
    "        if stats['count'] > 0:\n",
    "            print(f\"- Average confidence: {stats['mean']:.3f}\")\n",
    "            print(f\"- Maximum confidence: {stats['max']:.3f}\")\n",
    "            print(f\"- Minimum confidence: {stats['min']:.3f}\")\n",
    "            # 모든 점수 출력\n",
    "            print(f\"- All confidence scores: {[f'{score:.3f}' for score in stats['all_scores']]}\")\n",
    "    \n",
    "    return statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: 메인 실행 코드\n",
    "def main():\n",
    "    # 경로 설정\n",
    "    model_path = r'E:\\livingLab\\YOLOv8기반 객체인식\\results_20241124_155336\\waste_detection_model\\weights\\best.pt'\n",
    "    image_dir = r'E:\\livingLab\\YOLOv8기반 객체인식\\test\\images'\n",
    "    json_path = r'E:\\livingLab\\YOLOv8기반 객체인식\\test\\test_annotations_yolo.json'\n",
    "    target_class = 'Large Waste Items'\n",
    "    \n",
    "    # 1. 객체 감지 실행\n",
    "    print(\"Starting object detection...\")\n",
    "    results = detect_objects(model_path, image_dir)\n",
    "    \n",
    "    # 2. 라벨 변환\n",
    "    print(\"\\nConverting labels to major categories...\")\n",
    "    converted_labels = convert_labels_to_major_category(json_path, category_mapping)\n",
    "    \n",
    "    # 3. 이진 분류 실행\n",
    "    print(\"\\nPerforming binary classification...\")\n",
    "    binary_classification = classify_images_by_major_category(converted_labels, target_class)\n",
    "    \n",
    "    # 4. 결과 저장\n",
    "    output_path = './test/classification_results.json'\n",
    "    results_summary = save_results(binary_classification, output_path)\n",
    "    \n",
    "    # 5. Confidence Score 분석\n",
    "    print(\"\\nAnalyzing confidence scores...\")\n",
    "    confidence_statistics = calculate_confidence_scores(results)\n",
    "    \n",
    "    # 6. Confidence Score 결과 저장\n",
    "    confidence_output_path = './test/confidence_statistics.json'\n",
    "    with open(confidence_output_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(confidence_statistics, f, indent=4, ensure_ascii=False)\n",
    "    \n",
    "    # 7. 최종 결과 출력\n",
    "    print(\"\\nClassification Summary:\")\n",
    "    print(f\"Total images processed: {results_summary['total_images']}\")\n",
    "    print(f\"Positive classifications: {results_summary['positive_count']}\")\n",
    "    print(f\"Negative classifications: {results_summary['negative_count']}\")\n",
    "    print(f\"\\nDetailed results saved to: {output_path}\")\n",
    "    print(f\"Confidence statistics saved to: {confidence_output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting object detection...\n",
      "Found 79 images in the directory.\n",
      "\n",
      "Processing image 1/79: -7-_jpg.rf.3e4e45f01df8da1ac999a94efb60473e.jpg\n",
      "\n",
      "image 1/1 E:\\livingLab\\YOLOv8 \\test\\images\\-7-_jpg.rf.3e4e45f01df8da1ac999a94efb60473e.jpg: 640x640 1 PP bag, 50.3ms\n",
      "Speed: 2.1ms preprocess, 50.3ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\predict10\u001b[0m\n",
      "- PP bag: 0.798\n",
      "Processing times:\n",
      "- Preprocess: 2.061128616333008ms\n",
      "- Inference: 50.33278465270996ms\n",
      "- Postprocess: 0.0ms\n",
      "\n",
      "Processing image 2/79: 202411061313-36-307488-127-352668_jpg.rf.2debbe814df92503a0079be859650cc0.jpg\n",
      "\n",
      "image 1/1 E:\\livingLab\\YOLOv8 \\test\\images\\202411061313-36-307488-127-352668_jpg.rf.2debbe814df92503a0079be859650cc0.jpg: 640x640 1 General Waste, 2 CleanNets, 51.3ms\n",
      "Speed: 0.0ms preprocess, 51.3ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\predict10\u001b[0m\n",
      "- CleanNet: 0.891\n",
      "- CleanNet: 0.878\n",
      "- General Waste: 0.690\n",
      "Processing times:\n",
      "- Preprocess: 0.0ms\n",
      "- Inference: 51.331520080566406ms\n",
      "- Postprocess: 0.9677410125732422ms\n",
      "\n",
      "Processing image 3/79: 202411061319-36-307974-127-355197_jpg.rf.40e66b47ea454f90ee055c740410cc1c.jpg\n",
      "\n",
      "image 1/1 E:\\livingLab\\YOLOv8 \\test\\images\\202411061319-36-307974-127-355197_jpg.rf.40e66b47ea454f90ee055c740410cc1c.jpg: 640x640 3 General Wastes, 2 CleanNets, 35.1ms\n",
      "Speed: 0.0ms preprocess, 35.1ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\predict10\u001b[0m\n",
      "- CleanNet: 0.890\n",
      "- CleanNet: 0.850\n",
      "- General Waste: 0.409\n",
      "- General Waste: 0.318\n",
      "- General Waste: 0.308\n",
      "Processing times:\n",
      "- Preprocess: 0.0ms\n",
      "- Inference: 35.079240798950195ms\n",
      "- Postprocess: 0.4963874816894531ms\n",
      "\n",
      "Processing image 4/79: 202411061322-36-307550-127-354681_jpg.rf.e09f59bc4cc8915a9da9c47c91f306f0.jpg\n",
      "\n",
      "image 1/1 E:\\livingLab\\YOLOv8 \\test\\images\\202411061322-36-307550-127-354681_jpg.rf.e09f59bc4cc8915a9da9c47c91f306f0.jpg: 640x640 2 General Wastes, 2 CleanNets, 28.1ms\n",
      "Speed: 0.0ms preprocess, 28.1ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\predict10\u001b[0m\n",
      "- CleanNet: 0.933\n",
      "- CleanNet: 0.856\n",
      "- General Waste: 0.525\n",
      "- General Waste: 0.374\n",
      "Processing times:\n",
      "- Preprocess: 0.0ms\n",
      "- Inference: 28.137922286987305ms\n",
      "- Postprocess: 0.0ms\n",
      "\n",
      "Processing image 5/79: 202411061322-36-320218-127-343319_jpg.rf.86814c1d60da676d5d9596a2a3da0efb.jpg\n",
      "\n",
      "image 1/1 E:\\livingLab\\YOLOv8 \\test\\images\\202411061322-36-320218-127-343319_jpg.rf.86814c1d60da676d5d9596a2a3da0efb.jpg: 640x640 1 Large Waste Items, 4 General Wastes, 1 CleanNet, 31.2ms\n",
      "Speed: 0.0ms preprocess, 31.2ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\predict10\u001b[0m\n",
      "- CleanNet: 0.882\n",
      "- General Waste: 0.592\n",
      "- General Waste: 0.482\n",
      "- Large Waste Items: 0.367\n",
      "- General Waste: 0.311\n",
      "- General Waste: 0.253\n",
      "Processing times:\n",
      "- Preprocess: 0.0ms\n",
      "- Inference: 31.246185302734375ms\n",
      "- Postprocess: 0.0ms\n",
      "\n",
      "Processing image 6/79: 202411061323-36-319426-127-343581-2_jpg.rf.5c85eeb865231f3fb9ac4fd355319416.jpg\n",
      "\n",
      "image 1/1 E:\\livingLab\\YOLOv8 \\test\\images\\202411061323-36-319426-127-343581-2_jpg.rf.5c85eeb865231f3fb9ac4fd355319416.jpg: 640x640 1 Large Waste Items, 1 General Waste, 1 CleanNet, 15.0ms\n",
      "Speed: 0.9ms preprocess, 15.0ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\predict10\u001b[0m\n",
      "- Large Waste Items: 0.681\n",
      "- CleanNet: 0.646\n",
      "- General Waste: 0.251\n",
      "Processing times:\n",
      "- Preprocess: 0.9281635284423828ms\n",
      "- Inference: 15.00844955444336ms\n",
      "- Postprocess: 0.0ms\n",
      "\n",
      "Processing image 7/79: 202411061325-36-308399-127-354248_jpg.rf.fe8e74b6e0015e468fad270eb87ca499.jpg\n",
      "\n",
      "image 1/1 E:\\livingLab\\YOLOv8 \\test\\images\\202411061325-36-308399-127-354248_jpg.rf.fe8e74b6e0015e468fad270eb87ca499.jpg: 640x640 3 General Wastes, 2 CleanNets, 31.3ms\n",
      "Speed: 0.0ms preprocess, 31.3ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\predict10\u001b[0m\n",
      "- CleanNet: 0.904\n",
      "- CleanNet: 0.879\n",
      "- General Waste: 0.617\n",
      "- General Waste: 0.555\n",
      "- General Waste: 0.412\n",
      "Processing times:\n",
      "- Preprocess: 0.0ms\n",
      "- Inference: 31.347990036010742ms\n",
      "- Postprocess: 0.0ms\n",
      "\n",
      "Processing image 8/79: 202411061326-36-318897-127-343185_jpg.rf.d6707abf64c488ae79ddec26206a2f89.jpg\n",
      "\n",
      "image 1/1 E:\\livingLab\\YOLOv8 \\test\\images\\202411061326-36-318897-127-343185_jpg.rf.d6707abf64c488ae79ddec26206a2f89.jpg: 640x640 2 Large Waste Itemss, 2 General Wastes, 1 CleanNet, 33.5ms\n",
      "Speed: 0.0ms preprocess, 33.5ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\predict10\u001b[0m\n",
      "- CleanNet: 0.856\n",
      "- General Waste: 0.604\n",
      "- General Waste: 0.381\n",
      "- Large Waste Items: 0.270\n",
      "- Large Waste Items: 0.267\n",
      "Processing times:\n",
      "- Preprocess: 0.0ms\n",
      "- Inference: 33.548593521118164ms\n",
      "- Postprocess: 0.0ms\n",
      "\n",
      "Processing image 9/79: 202411061328-36-318047-127-344154_jpg.rf.7aefec6f116650e0e611553b9811a6d8.jpg\n",
      "\n",
      "image 1/1 E:\\livingLab\\YOLOv8 \\test\\images\\202411061328-36-318047-127-344154_jpg.rf.7aefec6f116650e0e611553b9811a6d8.jpg: 640x640 2 General Wastes, 2 CleanNets, 26.4ms\n",
      "Speed: 1.8ms preprocess, 26.4ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\predict10\u001b[0m\n",
      "- CleanNet: 0.930\n",
      "- CleanNet: 0.882\n",
      "- General Waste: 0.565\n",
      "- General Waste: 0.404\n",
      "Processing times:\n",
      "- Preprocess: 1.829385757446289ms\n",
      "- Inference: 26.384353637695312ms\n",
      "- Postprocess: 0.0ms\n",
      "\n",
      "Processing image 10/79: 202411061328-36-318467-127-342773_jpg.rf.8e7d885664cc4a093d1ef0afcc79b2e8.jpg\n",
      "\n",
      "image 1/1 E:\\livingLab\\YOLOv8 \\test\\images\\202411061328-36-318467-127-342773_jpg.rf.8e7d885664cc4a093d1ef0afcc79b2e8.jpg: 640x640 4 General Wastes, 2 CleanNets, 27.6ms\n",
      "Speed: 15.6ms preprocess, 27.6ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\predict10\u001b[0m\n",
      "- CleanNet: 0.915\n",
      "- CleanNet: 0.905\n",
      "- General Waste: 0.494\n",
      "- General Waste: 0.464\n",
      "- General Waste: 0.387\n",
      "- General Waste: 0.250\n",
      "Processing times:\n",
      "- Preprocess: 15.623807907104492ms\n",
      "- Inference: 27.556180953979492ms\n",
      "- Postprocess: 0.0ms\n",
      "\n",
      "Processing image 11/79: 202411061329-36-309133-127-353018_jpg.rf.66723f2b0a1f776c240789b2605d7c6e.jpg\n",
      "\n",
      "image 1/1 E:\\livingLab\\YOLOv8 \\test\\images\\202411061329-36-309133-127-353018_jpg.rf.66723f2b0a1f776c240789b2605d7c6e.jpg: 640x640 2 General Wastes, 2 CleanNets, 31.2ms\n",
      "Speed: 0.0ms preprocess, 31.2ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\predict10\u001b[0m\n",
      "- CleanNet: 0.912\n",
      "- CleanNet: 0.840\n",
      "- General Waste: 0.428\n",
      "- General Waste: 0.263\n",
      "Processing times:\n",
      "- Preprocess: 0.0ms\n",
      "- Inference: 31.25ms\n",
      "- Postprocess: 0.0ms\n",
      "\n",
      "Processing image 12/79: 202411061330-36-318792-127-344275_jpg.rf.3f74fb24878d9a28014f79a8ad00e4c4.jpg\n",
      "\n",
      "image 1/1 E:\\livingLab\\YOLOv8 \\test\\images\\202411061330-36-318792-127-344275_jpg.rf.3f74fb24878d9a28014f79a8ad00e4c4.jpg: 640x640 1 General Waste, 31.2ms\n",
      "Speed: 0.0ms preprocess, 31.2ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\predict10\u001b[0m\n",
      "- General Waste: 0.487\n",
      "Processing times:\n",
      "- Preprocess: 0.0ms\n",
      "- Inference: 31.24833106994629ms\n",
      "- Postprocess: 0.0ms\n",
      "\n",
      "Processing image 13/79: 202411061337-36-309239-127-353916_jpg.rf.4c7a323affb98f997405ec20ca6d216e.jpg\n",
      "\n",
      "image 1/1 E:\\livingLab\\YOLOv8 \\test\\images\\202411061337-36-309239-127-353916_jpg.rf.4c7a323affb98f997405ec20ca6d216e.jpg: 640x640 1 Large Waste Items, 4 General Wastes, 2 CleanNets, 45.3ms\n",
      "Speed: 0.0ms preprocess, 45.3ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\predict10\u001b[0m\n",
      "- CleanNet: 0.868\n",
      "- CleanNet: 0.824\n",
      "- Large Waste Items: 0.575\n",
      "- General Waste: 0.553\n",
      "- General Waste: 0.438\n",
      "- General Waste: 0.385\n",
      "- General Waste: 0.289\n",
      "Processing times:\n",
      "- Preprocess: 0.0ms\n",
      "- Inference: 45.252323150634766ms\n",
      "- Postprocess: 0.0ms\n",
      "\n",
      "Processing image 14/79: 202411061339-36-310790-127-352166_jpg.rf.78fd2988c35f72ab7c8c932764bb04c6.jpg\n",
      "\n",
      "image 1/1 E:\\livingLab\\YOLOv8 \\test\\images\\202411061339-36-310790-127-352166_jpg.rf.78fd2988c35f72ab7c8c932764bb04c6.jpg: 640x640 2 General Wastes, 2 CleanNets, 33.9ms\n",
      "Speed: 15.6ms preprocess, 33.9ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\predict10\u001b[0m\n",
      "- CleanNet: 0.770\n",
      "- CleanNet: 0.749\n",
      "- General Waste: 0.454\n",
      "- General Waste: 0.273\n",
      "Processing times:\n",
      "- Preprocess: 15.623331069946289ms\n",
      "- Inference: 33.9353084564209ms\n",
      "- Postprocess: 0.0ms\n",
      "\n",
      "Processing image 15/79: 202411061342-36-309176-127-351941_jpg.rf.05afb3c8e078242b96c4a730357296bb.jpg\n",
      "\n",
      "image 1/1 E:\\livingLab\\YOLOv8 \\test\\images\\202411061342-36-309176-127-351941_jpg.rf.05afb3c8e078242b96c4a730357296bb.jpg: 640x640 3 General Wastes, 2 CleanNets, 35.5ms\n",
      "Speed: 0.0ms preprocess, 35.5ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\predict10\u001b[0m\n",
      "- CleanNet: 0.772\n",
      "- CleanNet: 0.744\n",
      "- General Waste: 0.338\n",
      "- General Waste: 0.318\n",
      "- General Waste: 0.280\n",
      "Processing times:\n",
      "- Preprocess: 0.0ms\n",
      "- Inference: 35.463809967041016ms\n",
      "- Postprocess: 0.0ms\n",
      "\n",
      "Processing image 16/79: 202411061342-36-315640-127-349174_jpg.rf.5444f5989d8b4ed01d08d05553d63571.jpg\n",
      "\n",
      "image 1/1 E:\\livingLab\\YOLOv8 \\test\\images\\202411061342-36-315640-127-349174_jpg.rf.5444f5989d8b4ed01d08d05553d63571.jpg: 640x640 3 General Wastes, 2 CleanNets, 39.5ms\n",
      "Speed: 0.0ms preprocess, 39.5ms inference, 5.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\predict10\u001b[0m\n",
      "- CleanNet: 0.870\n",
      "- CleanNet: 0.847\n",
      "- General Waste: 0.549\n",
      "- General Waste: 0.284\n",
      "- General Waste: 0.277\n",
      "Processing times:\n",
      "- Preprocess: 0.0ms\n",
      "- Inference: 39.48473930358887ms\n",
      "- Postprocess: 5.846977233886719ms\n",
      "\n",
      "Processing image 17/79: 202411061343-36-315732-127-349908_jpg.rf.f32a738af7e8cb0569568da90e660036.jpg\n",
      "\n",
      "image 1/1 E:\\livingLab\\YOLOv8 \\test\\images\\202411061343-36-315732-127-349908_jpg.rf.f32a738af7e8cb0569568da90e660036.jpg: 640x640 4 General Wastes, 1 CleanNet, 36.9ms\n",
      "Speed: 6.9ms preprocess, 36.9ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\predict10\u001b[0m\n",
      "- CleanNet: 0.657\n",
      "- General Waste: 0.332\n",
      "- General Waste: 0.322\n",
      "- General Waste: 0.317\n",
      "- General Waste: 0.284\n",
      "Processing times:\n",
      "- Preprocess: 6.924629211425781ms\n",
      "- Inference: 36.86809539794922ms\n",
      "- Postprocess: 0.0ms\n",
      "\n",
      "Processing image 18/79: 202411061345-36-314850-127-350158_jpg.rf.a5c076f0199e673c36da4ac790ee940e.jpg\n",
      "\n",
      "image 1/1 E:\\livingLab\\YOLOv8 \\test\\images\\202411061345-36-314850-127-350158_jpg.rf.a5c076f0199e673c36da4ac790ee940e.jpg: 640x640 1 General Waste, 2 CleanNets, 44.1ms\n",
      "Speed: 0.0ms preprocess, 44.1ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\predict10\u001b[0m\n",
      "- CleanNet: 0.913\n",
      "- CleanNet: 0.845\n",
      "- General Waste: 0.452\n",
      "Processing times:\n",
      "- Preprocess: 0.0ms\n",
      "- Inference: 44.07548904418945ms\n",
      "- Postprocess: 0.0ms\n",
      "\n",
      "Processing image 19/79: 202411061346-36-308081-127-353130_jpg.rf.83770fab01c2a2e62838a2a2729231a7.jpg\n",
      "\n",
      "image 1/1 E:\\livingLab\\YOLOv8 \\test\\images\\202411061346-36-308081-127-353130_jpg.rf.83770fab01c2a2e62838a2a2729231a7.jpg: 640x640 4 General Wastes, 2 CleanNets, 35.6ms\n",
      "Speed: 0.0ms preprocess, 35.6ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\predict10\u001b[0m\n",
      "- CleanNet: 0.846\n",
      "- CleanNet: 0.824\n",
      "- General Waste: 0.571\n",
      "- General Waste: 0.301\n",
      "- General Waste: 0.301\n",
      "- General Waste: 0.300\n",
      "Processing times:\n",
      "- Preprocess: 0.0ms\n",
      "- Inference: 35.5525016784668ms\n",
      "- Postprocess: 0.0ms\n",
      "\n",
      "Processing image 20/79: 202411061346-36-311745-127-353092_jpg.rf.3e408eccde358b63c05fd5a29f956aab.jpg\n",
      "\n",
      "image 1/1 E:\\livingLab\\YOLOv8 \\test\\images\\202411061346-36-311745-127-353092_jpg.rf.3e408eccde358b63c05fd5a29f956aab.jpg: 640x640 3 General Wastes, 3 CleanNets, 40.0ms\n",
      "Speed: 0.0ms preprocess, 40.0ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\predict10\u001b[0m\n",
      "- CleanNet: 0.857\n",
      "- CleanNet: 0.790\n",
      "- CleanNet: 0.432\n",
      "- General Waste: 0.387\n",
      "- General Waste: 0.382\n",
      "- General Waste: 0.261\n",
      "Processing times:\n",
      "- Preprocess: 0.0ms\n",
      "- Inference: 40.00544548034668ms\n",
      "- Postprocess: 0.0ms\n",
      "\n",
      "Processing image 21/79: 202411061346-36-314198-127-350451_jpg.rf.c77a12c283995fb4057bd0ca00b83161.jpg\n",
      "\n",
      "image 1/1 E:\\livingLab\\YOLOv8 \\test\\images\\202411061346-36-314198-127-350451_jpg.rf.c77a12c283995fb4057bd0ca00b83161.jpg: 640x640 3 Large Waste Itemss, 1 General Waste, 1 CleanNet, 49.9ms\n",
      "Speed: 0.0ms preprocess, 49.9ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\predict10\u001b[0m\n",
      "- Large Waste Items: 0.448\n",
      "- CleanNet: 0.389\n",
      "- General Waste: 0.364\n",
      "- Large Waste Items: 0.269\n",
      "- Large Waste Items: 0.252\n",
      "Processing times:\n",
      "- Preprocess: 0.0ms\n",
      "- Inference: 49.91745948791504ms\n",
      "- Postprocess: 0.0ms\n",
      "\n",
      "Processing image 22/79: 202411061348-36-310510-127-353380_jpg.rf.1b445c1bbde0cedac1df9b0f9c220dd4.jpg\n",
      "\n",
      "image 1/1 E:\\livingLab\\YOLOv8 \\test\\images\\202411061348-36-310510-127-353380_jpg.rf.1b445c1bbde0cedac1df9b0f9c220dd4.jpg: 640x640 3 General Wastes, 2 CleanNets, 40.5ms\n",
      "Speed: 2.0ms preprocess, 40.5ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\predict10\u001b[0m\n",
      "- CleanNet: 0.955\n",
      "- CleanNet: 0.760\n",
      "- General Waste: 0.499\n",
      "- General Waste: 0.305\n",
      "- General Waste: 0.294\n",
      "Processing times:\n",
      "- Preprocess: 1.9974708557128906ms\n",
      "- Inference: 40.54880142211914ms\n",
      "- Postprocess: 0.9970664978027344ms\n",
      "\n",
      "Processing image 23/79: 202411061348-36-313711-127-349988_jpg.rf.3581e8d79a3a050686554a20476cdb34.jpg\n",
      "\n",
      "image 1/1 E:\\livingLab\\YOLOv8 \\test\\images\\202411061348-36-313711-127-349988_jpg.rf.3581e8d79a3a050686554a20476cdb34.jpg: 640x640 2 Large Waste Itemss, 2 General Wastes, 2 CleanNets, 16.1ms\n",
      "Speed: 2.0ms preprocess, 16.1ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\predict10\u001b[0m\n",
      "- CleanNet: 0.896\n",
      "- CleanNet: 0.702\n",
      "- Large Waste Items: 0.633\n",
      "- General Waste: 0.631\n",
      "- General Waste: 0.607\n",
      "- Large Waste Items: 0.317\n",
      "Processing times:\n",
      "- Preprocess: 1.992940902709961ms\n",
      "- Inference: 16.05081558227539ms\n",
      "- Postprocess: 0.0ms\n",
      "\n",
      "Processing image 24/79: 202411061349-36-311745-127-353092_jpg.rf.fb5f4cbdb3036192e55d681050fe72da.jpg\n",
      "\n",
      "image 1/1 E:\\livingLab\\YOLOv8 \\test\\images\\202411061349-36-311745-127-353092_jpg.rf.fb5f4cbdb3036192e55d681050fe72da.jpg: 640x640 2 General Wastes, 2 CleanNets, 30.6ms\n",
      "Speed: 0.0ms preprocess, 30.6ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\predict10\u001b[0m\n",
      "- CleanNet: 0.921\n",
      "- CleanNet: 0.907\n",
      "- General Waste: 0.587\n",
      "- General Waste: 0.520\n",
      "Processing times:\n",
      "- Preprocess: 0.0ms\n",
      "- Inference: 30.628681182861328ms\n",
      "- Postprocess: 0.0ms\n",
      "\n",
      "Processing image 25/79: 202411061349-36-313040-127-350293_jpg.rf.52fab8e4c82156dcff635e98319420f9.jpg\n",
      "\n",
      "image 1/1 E:\\livingLab\\YOLOv8 \\test\\images\\202411061349-36-313040-127-350293_jpg.rf.52fab8e4c82156dcff635e98319420f9.jpg: 640x640 3 General Wastes, 2 CleanNets, 28.8ms\n",
      "Speed: 0.0ms preprocess, 28.8ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\predict10\u001b[0m\n",
      "- CleanNet: 0.888\n",
      "- General Waste: 0.741\n",
      "- CleanNet: 0.651\n",
      "- General Waste: 0.449\n",
      "- General Waste: 0.312\n",
      "Processing times:\n",
      "- Preprocess: 0.0ms\n",
      "- Inference: 28.833389282226562ms\n",
      "- Postprocess: 1.146554946899414ms\n",
      "\n",
      "Processing image 26/79: 202411061350-36-314129-127-351297_jpg.rf.7b7238965a8cae0683e4d2bac4f5a06a.jpg\n",
      "\n",
      "image 1/1 E:\\livingLab\\YOLOv8 \\test\\images\\202411061350-36-314129-127-351297_jpg.rf.7b7238965a8cae0683e4d2bac4f5a06a.jpg: 640x640 4 General Wastes, 3 CleanNets, 31.2ms\n",
      "Speed: 0.0ms preprocess, 31.2ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\predict10\u001b[0m\n",
      "- CleanNet: 0.882\n",
      "- CleanNet: 0.881\n",
      "- CleanNet: 0.815\n",
      "- General Waste: 0.649\n",
      "- General Waste: 0.485\n",
      "- General Waste: 0.389\n",
      "- General Waste: 0.352\n",
      "Processing times:\n",
      "- Preprocess: 0.0ms\n",
      "- Inference: 31.247615814208984ms\n",
      "- Postprocess: 0.0ms\n",
      "\n",
      "Processing image 27/79: 202411061352-36-313215-127-351277-2_jpg.rf.f6aa801aed2a6e2e10462b781d4358bd.jpg\n",
      "\n",
      "image 1/1 E:\\livingLab\\YOLOv8 \\test\\images\\202411061352-36-313215-127-351277-2_jpg.rf.f6aa801aed2a6e2e10462b781d4358bd.jpg: 640x640 1 Large Waste Items, 1 PP bag, 29.4ms\n",
      "Speed: 0.0ms preprocess, 29.4ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\predict10\u001b[0m\n",
      "- Large Waste Items: 0.373\n",
      "- PP bag: 0.361\n",
      "Processing times:\n",
      "- Preprocess: 0.0ms\n",
      "- Inference: 29.447317123413086ms\n",
      "- Postprocess: 0.0ms\n",
      "\n",
      "Processing image 28/79: 202411061352-36-313215-127-351277-3_jpg.rf.46cfc13747bebd1a1755da3b8a3cd8dd.jpg\n",
      "\n",
      "image 1/1 E:\\livingLab\\YOLOv8 \\test\\images\\202411061352-36-313215-127-351277-3_jpg.rf.46cfc13747bebd1a1755da3b8a3cd8dd.jpg: 640x640 1 Large Waste Items, 5 PP bags, 31.2ms\n",
      "Speed: 0.0ms preprocess, 31.2ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\predict10\u001b[0m\n",
      "- PP bag: 0.555\n",
      "- PP bag: 0.503\n",
      "- Large Waste Items: 0.437\n",
      "- PP bag: 0.367\n",
      "- PP bag: 0.362\n",
      "- PP bag: 0.297\n",
      "Processing times:\n",
      "- Preprocess: 0.0ms\n",
      "- Inference: 31.24856948852539ms\n",
      "- Postprocess: 0.0ms\n",
      "\n",
      "Processing image 29/79: 202411061358-36-308465-127-354548_jpg.rf.965f2f2b47de25bdc9d795cb71576543.jpg\n",
      "\n",
      "image 1/1 E:\\livingLab\\YOLOv8 \\test\\images\\202411061358-36-308465-127-354548_jpg.rf.965f2f2b47de25bdc9d795cb71576543.jpg: 640x640 4 General Wastes, 2 CleanNets, 31.6ms\n",
      "Speed: 15.6ms preprocess, 31.6ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\predict10\u001b[0m\n",
      "- CleanNet: 0.890\n",
      "- CleanNet: 0.846\n",
      "- General Waste: 0.532\n",
      "- General Waste: 0.437\n",
      "- General Waste: 0.321\n",
      "- General Waste: 0.317\n",
      "Processing times:\n",
      "- Preprocess: 15.621662139892578ms\n",
      "- Inference: 31.630277633666992ms\n",
      "- Postprocess: 0.0ms\n",
      "\n",
      "Processing image 30/79: 202411061359-36-310957-127-353130_jpg.rf.906b474010a7ba8fcea77eeef01433cd.jpg\n",
      "\n",
      "image 1/1 E:\\livingLab\\YOLOv8 \\test\\images\\202411061359-36-310957-127-353130_jpg.rf.906b474010a7ba8fcea77eeef01433cd.jpg: 640x640 3 General Wastes, 2 CleanNets, 15.6ms\n",
      "Speed: 15.6ms preprocess, 15.6ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\predict10\u001b[0m\n",
      "- CleanNet: 0.873\n",
      "- CleanNet: 0.834\n",
      "- General Waste: 0.476\n",
      "- General Waste: 0.378\n",
      "- General Waste: 0.306\n",
      "Processing times:\n",
      "- Preprocess: 15.626192092895508ms\n",
      "- Inference: 15.623331069946289ms\n",
      "- Postprocess: 0.0ms\n",
      "\n",
      "Processing image 31/79: 202411061415-36-316636-127-347495_jpg.rf.79ef2fb18c5b765ead9711ec6516fcea.jpg\n",
      "\n",
      "image 1/1 E:\\livingLab\\YOLOv8 \\test\\images\\202411061415-36-316636-127-347495_jpg.rf.79ef2fb18c5b765ead9711ec6516fcea.jpg: 640x640 3 Large Waste Itemss, 31.2ms\n",
      "Speed: 0.0ms preprocess, 31.2ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\predict10\u001b[0m\n",
      "- Large Waste Items: 0.396\n",
      "- Large Waste Items: 0.275\n",
      "- Large Waste Items: 0.257\n",
      "Processing times:\n",
      "- Preprocess: 0.0ms\n",
      "- Inference: 31.248092651367188ms\n",
      "- Postprocess: 0.0ms\n",
      "\n",
      "Processing image 32/79: 202411061418-36-315697-127-346125_jpg.rf.e9aab1e2970ae7ad8efb2ab0e3dd7b81.jpg\n",
      "\n",
      "image 1/1 E:\\livingLab\\YOLOv8 \\test\\images\\202411061418-36-315697-127-346125_jpg.rf.e9aab1e2970ae7ad8efb2ab0e3dd7b81.jpg: 640x640 5 Large Waste Itemss, 31.2ms\n",
      "Speed: 0.0ms preprocess, 31.2ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\predict10\u001b[0m\n",
      "- Large Waste Items: 0.624\n",
      "- Large Waste Items: 0.543\n",
      "- Large Waste Items: 0.542\n",
      "- Large Waste Items: 0.493\n",
      "- Large Waste Items: 0.261\n",
      "Processing times:\n",
      "- Preprocess: 0.0ms\n",
      "- Inference: 31.2497615814209ms\n",
      "- Postprocess: 0.0ms\n",
      "\n",
      "Processing image 33/79: 202411061420-36-315055-127-346190_jpg.rf.f735fd248bc6690f4c75068de0897d1f.jpg\n",
      "\n",
      "image 1/1 E:\\livingLab\\YOLOv8 \\test\\images\\202411061420-36-315055-127-346190_jpg.rf.f735fd248bc6690f4c75068de0897d1f.jpg: 640x640 4 Large Waste Itemss, 15.6ms\n",
      "Speed: 0.0ms preprocess, 15.6ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\predict10\u001b[0m\n",
      "- Large Waste Items: 0.715\n",
      "- Large Waste Items: 0.644\n",
      "- Large Waste Items: 0.470\n",
      "- Large Waste Items: 0.269\n",
      "Processing times:\n",
      "- Preprocess: 0.0ms\n",
      "- Inference: 15.622854232788086ms\n",
      "- Postprocess: 0.0ms\n",
      "\n",
      "Processing image 34/79: 202411061423-36-313839-127-348579_jpg.rf.4c6e29fd453c998c1710c6dd014daacf.jpg\n",
      "\n",
      "image 1/1 E:\\livingLab\\YOLOv8 \\test\\images\\202411061423-36-313839-127-348579_jpg.rf.4c6e29fd453c998c1710c6dd014daacf.jpg: 640x640 2 Large Waste Itemss, 5 General Wastes, 39.1ms\n",
      "Speed: 0.0ms preprocess, 39.1ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\predict10\u001b[0m\n",
      "- General Waste: 0.650\n",
      "- Large Waste Items: 0.455\n",
      "- General Waste: 0.366\n",
      "- General Waste: 0.340\n",
      "- Large Waste Items: 0.334\n",
      "- General Waste: 0.330\n",
      "- General Waste: 0.263\n",
      "Processing times:\n",
      "- Preprocess: 0.0ms\n",
      "- Inference: 39.1385555267334ms\n",
      "- Postprocess: 0.0ms\n",
      "\n",
      "Processing image 35/79: 202411061423-36-314851-127-347123_jpg.rf.6b1796d5ce76d550ec8c684a8c23fafd.jpg\n",
      "\n",
      "image 1/1 E:\\livingLab\\YOLOv8 \\test\\images\\202411061423-36-314851-127-347123_jpg.rf.6b1796d5ce76d550ec8c684a8c23fafd.jpg: 640x640 4 Large Waste Itemss, 4 PP bags, 1 General Waste, 43.9ms\n",
      "Speed: 15.7ms preprocess, 43.9ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\predict10\u001b[0m\n",
      "- PP bag: 0.543\n",
      "- Large Waste Items: 0.483\n",
      "- PP bag: 0.446\n",
      "- Large Waste Items: 0.407\n",
      "- General Waste: 0.388\n",
      "- Large Waste Items: 0.338\n",
      "- PP bag: 0.307\n",
      "- PP bag: 0.291\n",
      "- Large Waste Items: 0.274\n",
      "Processing times:\n",
      "- Preprocess: 15.654325485229492ms\n",
      "- Inference: 43.90668869018555ms\n",
      "- Postprocess: 0.9965896606445312ms\n",
      "\n",
      "Processing image 36/79: 202411061423-36-315046-127-346080_jpg.rf.bdc3f041759c2e28c0f15e1397a3584d.jpg\n",
      "\n",
      "image 1/1 E:\\livingLab\\YOLOv8 \\test\\images\\202411061423-36-315046-127-346080_jpg.rf.bdc3f041759c2e28c0f15e1397a3584d.jpg: 640x640 2 Large Waste Itemss, 33.7ms\n",
      "Speed: 1.5ms preprocess, 33.7ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\predict10\u001b[0m\n",
      "- Large Waste Items: 0.287\n",
      "- Large Waste Items: 0.278\n",
      "Processing times:\n",
      "- Preprocess: 1.482248306274414ms\n",
      "- Inference: 33.669233322143555ms\n",
      "- Postprocess: 1.0454654693603516ms\n",
      "\n",
      "Processing image 37/79: 202411061431-36-309908-127-348185_jpg.rf.23fe7b1ddec31535d885e6cc9288d2da.jpg\n",
      "\n",
      "image 1/1 E:\\livingLab\\YOLOv8 \\test\\images\\202411061431-36-309908-127-348185_jpg.rf.23fe7b1ddec31535d885e6cc9288d2da.jpg: 640x640 6 Large Waste Itemss, 22.8ms\n",
      "Speed: 0.0ms preprocess, 22.8ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\predict10\u001b[0m\n",
      "- Large Waste Items: 0.912\n",
      "- Large Waste Items: 0.750\n",
      "- Large Waste Items: 0.642\n",
      "- Large Waste Items: 0.526\n",
      "- Large Waste Items: 0.313\n",
      "- Large Waste Items: 0.288\n",
      "Processing times:\n",
      "- Preprocess: 0.0ms\n",
      "- Inference: 22.826671600341797ms\n",
      "- Postprocess: 0.0ms\n",
      "\n",
      "Processing image 38/79: 202411061431-36-311574-127-349342_jpg.rf.5182513e42f5000cf9af2f91816f2114.jpg\n",
      "\n",
      "image 1/1 E:\\livingLab\\YOLOv8 \\test\\images\\202411061431-36-311574-127-349342_jpg.rf.5182513e42f5000cf9af2f91816f2114.jpg: 640x640 2 Large Waste Itemss, 2 General Wastes, 31.6ms\n",
      "Speed: 0.0ms preprocess, 31.6ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\predict10\u001b[0m\n",
      "- General Waste: 0.532\n",
      "- General Waste: 0.396\n",
      "- Large Waste Items: 0.375\n",
      "- Large Waste Items: 0.264\n",
      "Processing times:\n",
      "- Preprocess: 0.0ms\n",
      "- Inference: 31.629562377929688ms\n",
      "- Postprocess: 0.0ms\n",
      "\n",
      "Processing image 39/79: 202411061433-36-309089-127-349308_jpg.rf.680978375d34a5ce6bd988b65213fe68.jpg\n",
      "\n",
      "image 1/1 E:\\livingLab\\YOLOv8 \\test\\images\\202411061433-36-309089-127-349308_jpg.rf.680978375d34a5ce6bd988b65213fe68.jpg: 640x640 10 Large Waste Itemss, 1 General Waste, 26.9ms\n",
      "Speed: 0.0ms preprocess, 26.9ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\predict10\u001b[0m\n",
      "- Large Waste Items: 0.555\n",
      "- Large Waste Items: 0.504\n",
      "- Large Waste Items: 0.427\n",
      "- Large Waste Items: 0.425\n",
      "- Large Waste Items: 0.416\n",
      "- Large Waste Items: 0.414\n",
      "- Large Waste Items: 0.364\n",
      "- Large Waste Items: 0.349\n",
      "- General Waste: 0.325\n",
      "- Large Waste Items: 0.291\n",
      "- Large Waste Items: 0.255\n",
      "Processing times:\n",
      "- Preprocess: 0.0ms\n",
      "- Inference: 26.868104934692383ms\n",
      "- Postprocess: 0.0ms\n",
      "\n",
      "Processing image 40/79: 202411061434-36-309502-127-348344_jpg.rf.18196b3699941fd051eb67f4246945bb.jpg\n",
      "\n",
      "image 1/1 E:\\livingLab\\YOLOv8 \\test\\images\\202411061434-36-309502-127-348344_jpg.rf.18196b3699941fd051eb67f4246945bb.jpg: 640x640 2 General Wastes, 36.4ms\n",
      "Speed: 0.0ms preprocess, 36.4ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\predict10\u001b[0m\n",
      "- General Waste: 0.522\n",
      "- General Waste: 0.336\n",
      "Processing times:\n",
      "- Preprocess: 0.0ms\n",
      "- Inference: 36.4069938659668ms\n",
      "- Postprocess: 0.0ms\n",
      "\n",
      "Processing image 41/79: 202411061438-36-309254-127-350078_jpg.rf.63980ab462c1a491c72131a321aa9bc4.jpg\n",
      "\n",
      "image 1/1 E:\\livingLab\\YOLOv8 \\test\\images\\202411061438-36-309254-127-350078_jpg.rf.63980ab462c1a491c72131a321aa9bc4.jpg: 640x640 1 Large Waste Items, 1 CleanNet, 31.2ms\n",
      "Speed: 0.0ms preprocess, 31.2ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\predict10\u001b[0m\n",
      "- CleanNet: 0.708\n",
      "- Large Waste Items: 0.438\n",
      "Processing times:\n",
      "- Preprocess: 0.0ms\n",
      "- Inference: 31.249046325683594ms\n",
      "- Postprocess: 0.0ms\n",
      "\n",
      "Processing image 42/79: 202411081236-36-423913-127-407165-1_jpg.rf.0dfb1f31bd1bafebca3f8424e98cdc61.jpg\n",
      "\n",
      "image 1/1 E:\\livingLab\\YOLOv8 \\test\\images\\202411081236-36-423913-127-407165-1_jpg.rf.0dfb1f31bd1bafebca3f8424e98cdc61.jpg: 640x640 3 Large Waste Itemss, 2 CleanNets, 33.7ms\n",
      "Speed: 0.0ms preprocess, 33.7ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\predict10\u001b[0m\n",
      "- Large Waste Items: 0.456\n",
      "- Large Waste Items: 0.386\n",
      "- CleanNet: 0.302\n",
      "- CleanNet: 0.293\n",
      "- Large Waste Items: 0.275\n",
      "Processing times:\n",
      "- Preprocess: 0.0ms\n",
      "- Inference: 33.69474411010742ms\n",
      "- Postprocess: 0.0ms\n",
      "\n",
      "Processing image 43/79: 202411081236-36-423913-127-407165-2_jpg.rf.f6e2a098f84601afac7d2350a475d4cc.jpg\n",
      "\n",
      "image 1/1 E:\\livingLab\\YOLOv8 \\test\\images\\202411081236-36-423913-127-407165-2_jpg.rf.f6e2a098f84601afac7d2350a475d4cc.jpg: 640x640 22 PP bags, 15.6ms\n",
      "Speed: 0.0ms preprocess, 15.6ms inference, 15.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\predict10\u001b[0m\n",
      "- PP bag: 0.922\n",
      "- PP bag: 0.912\n",
      "- PP bag: 0.909\n",
      "- PP bag: 0.906\n",
      "- PP bag: 0.895\n",
      "- PP bag: 0.863\n",
      "- PP bag: 0.776\n",
      "- PP bag: 0.749\n",
      "- PP bag: 0.748\n",
      "- PP bag: 0.744\n",
      "- PP bag: 0.729\n",
      "- PP bag: 0.670\n",
      "- PP bag: 0.664\n",
      "- PP bag: 0.541\n",
      "- PP bag: 0.521\n",
      "- PP bag: 0.512\n",
      "- PP bag: 0.451\n",
      "- PP bag: 0.417\n",
      "- PP bag: 0.390\n",
      "- PP bag: 0.306\n",
      "- PP bag: 0.306\n",
      "- PP bag: 0.285\n",
      "Processing times:\n",
      "- Preprocess: 0.0ms\n",
      "- Inference: 15.625953674316406ms\n",
      "- Postprocess: 15.623092651367188ms\n",
      "\n",
      "Processing image 44/79: 202411111136-36-331616-127-335799_jpg.rf.896aa812873491b43e52dfbd3261d149.jpg\n",
      "\n",
      "image 1/1 E:\\livingLab\\YOLOv8 \\test\\images\\202411111136-36-331616-127-335799_jpg.rf.896aa812873491b43e52dfbd3261d149.jpg: 640x640 1 PP bag, 2 General Wastes, 1 CleanNet, 15.6ms\n",
      "Speed: 15.6ms preprocess, 15.6ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\predict10\u001b[0m\n",
      "- CleanNet: 0.672\n",
      "- General Waste: 0.534\n",
      "- PP bag: 0.300\n",
      "- General Waste: 0.292\n",
      "Processing times:\n",
      "- Preprocess: 15.623331069946289ms\n",
      "- Inference: 15.622854232788086ms\n",
      "- Postprocess: 0.0ms\n",
      "\n",
      "Processing image 45/79: 202411111137-36-331383-127-335375_jpg.rf.a50332a13cae1728d2c5dc6162c5b05d.jpg\n",
      "\n",
      "image 1/1 E:\\livingLab\\YOLOv8 \\test\\images\\202411111137-36-331383-127-335375_jpg.rf.a50332a13cae1728d2c5dc6162c5b05d.jpg: 640x640 1 PP bag, 1 CleanNet, 31.6ms\n",
      "Speed: 0.0ms preprocess, 31.6ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\predict10\u001b[0m\n",
      "- CleanNet: 0.447\n",
      "- PP bag: 0.270\n",
      "Processing times:\n",
      "- Preprocess: 0.0ms\n",
      "- Inference: 31.63003921508789ms\n",
      "- Postprocess: 0.0ms\n",
      "\n",
      "Processing image 46/79: 202411111139-36-331111-127-337848_jpg.rf.cbb982f1d1e160b11ae586add407ded9.jpg\n",
      "\n",
      "image 1/1 E:\\livingLab\\YOLOv8 \\test\\images\\202411111139-36-331111-127-337848_jpg.rf.cbb982f1d1e160b11ae586add407ded9.jpg: 640x640 2 Large Waste Itemss, 2 General Wastes, 2 CleanNets, 34.7ms\n",
      "Speed: 0.0ms preprocess, 34.7ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\predict10\u001b[0m\n",
      "- CleanNet: 0.718\n",
      "- Large Waste Items: 0.607\n",
      "- Large Waste Items: 0.402\n",
      "- General Waste: 0.398\n",
      "- CleanNet: 0.367\n",
      "- General Waste: 0.325\n",
      "Processing times:\n",
      "- Preprocess: 0.0ms\n",
      "- Inference: 34.66391563415527ms\n",
      "- Postprocess: 0.0ms\n",
      "\n",
      "Processing image 47/79: 202411111148-36-330942-127-338586_jpg.rf.09088434bf25f50d833296c7059228a4.jpg\n",
      "\n",
      "image 1/1 E:\\livingLab\\YOLOv8 \\test\\images\\202411111148-36-330942-127-338586_jpg.rf.09088434bf25f50d833296c7059228a4.jpg: 640x640 3 CleanNets, 30.3ms\n",
      "Speed: 0.0ms preprocess, 30.3ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\predict10\u001b[0m\n",
      "- CleanNet: 0.874\n",
      "- CleanNet: 0.802\n",
      "- CleanNet: 0.799\n",
      "Processing times:\n",
      "- Preprocess: 0.0ms\n",
      "- Inference: 30.308961868286133ms\n",
      "- Postprocess: 0.0ms\n",
      "\n",
      "Processing image 48/79: 202411111150-36-331699-127-339169-1_jpg.rf.2dfbaba491bab23e08d12300bdec0a40.jpg\n",
      "\n",
      "image 1/1 E:\\livingLab\\YOLOv8 \\test\\images\\202411111150-36-331699-127-339169-1_jpg.rf.2dfbaba491bab23e08d12300bdec0a40.jpg: 640x640 2 Large Waste Itemss, 3 General Wastes, 3 CleanNets, 17.6ms\n",
      "Speed: 0.0ms preprocess, 17.6ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\predict10\u001b[0m\n",
      "- CleanNet: 0.819\n",
      "- CleanNet: 0.753\n",
      "- CleanNet: 0.608\n",
      "- General Waste: 0.503\n",
      "- General Waste: 0.430\n",
      "- General Waste: 0.422\n",
      "- Large Waste Items: 0.376\n",
      "- Large Waste Items: 0.271\n",
      "Processing times:\n",
      "- Preprocess: 0.0ms\n",
      "- Inference: 17.621278762817383ms\n",
      "- Postprocess: 0.0ms\n",
      "\n",
      "Processing image 49/79: 202411111150-36-331699-127-339169-2_jpg.rf.30de6ea0a36f7a5dff4c2d5633c75ff7.jpg\n",
      "\n",
      "image 1/1 E:\\livingLab\\YOLOv8 \\test\\images\\202411111150-36-331699-127-339169-2_jpg.rf.30de6ea0a36f7a5dff4c2d5633c75ff7.jpg: 640x640 3 Large Waste Itemss, 39.3ms\n",
      "Speed: 0.0ms preprocess, 39.3ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\predict10\u001b[0m\n",
      "- Large Waste Items: 0.745\n",
      "- Large Waste Items: 0.405\n",
      "- Large Waste Items: 0.332\n",
      "Processing times:\n",
      "- Preprocess: 0.0ms\n",
      "- Inference: 39.29305076599121ms\n",
      "- Postprocess: 1.0223388671875ms\n",
      "\n",
      "Processing image 50/79: 202411111153-36-36-331686-127-340556_jpg.rf.f610735c1c46cf5328fb96c544f43dd2.jpg\n",
      "\n",
      "image 1/1 E:\\livingLab\\YOLOv8 \\test\\images\\202411111153-36-36-331686-127-340556_jpg.rf.f610735c1c46cf5328fb96c544f43dd2.jpg: 640x640 2 CleanNets, 25.9ms\n",
      "Speed: 1.7ms preprocess, 25.9ms inference, 6.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\predict10\u001b[0m\n",
      "- CleanNet: 0.820\n",
      "- CleanNet: 0.792\n",
      "Processing times:\n",
      "- Preprocess: 1.7142295837402344ms\n",
      "- Inference: 25.886058807373047ms\n",
      "- Postprocess: 6.49261474609375ms\n",
      "\n",
      "Processing image 51/79: 202411111156-36-331079-127-341792_jpg.rf.fe0c796d57c7083f244e29ed95c6585c.jpg\n",
      "\n",
      "image 1/1 E:\\livingLab\\YOLOv8 \\test\\images\\202411111156-36-331079-127-341792_jpg.rf.fe0c796d57c7083f244e29ed95c6585c.jpg: 640x640 6 General Wastes, 2 CleanNets, 30.0ms\n",
      "Speed: 4.3ms preprocess, 30.0ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\predict10\u001b[0m\n",
      "- CleanNet: 0.783\n",
      "- General Waste: 0.720\n",
      "- General Waste: 0.526\n",
      "- CleanNet: 0.430\n",
      "- General Waste: 0.382\n",
      "- General Waste: 0.315\n",
      "- General Waste: 0.313\n",
      "- General Waste: 0.254\n",
      "Processing times:\n",
      "- Preprocess: 4.333972930908203ms\n",
      "- Inference: 29.987573623657227ms\n",
      "- Postprocess: 0.0ms\n",
      "\n",
      "Processing image 52/79: 202411111206-36-311745-127-353092_jpg.rf.175848f0b77ba404e4e6f11f44f8c590.jpg\n",
      "\n",
      "image 1/1 E:\\livingLab\\YOLOv8 \\test\\images\\202411111206-36-311745-127-353092_jpg.rf.175848f0b77ba404e4e6f11f44f8c590.jpg: 640x640 1 General Waste, 2 CleanNets, 15.6ms\n",
      "Speed: 2.5ms preprocess, 15.6ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\predict10\u001b[0m\n",
      "- CleanNet: 0.879\n",
      "- CleanNet: 0.851\n",
      "- General Waste: 0.330\n",
      "Processing times:\n",
      "- Preprocess: 2.5148391723632812ms\n",
      "- Inference: 15.624523162841797ms\n",
      "- Postprocess: 0.0ms\n",
      "\n",
      "Processing image 53/79: 202411111207-36-310510-127-353380_jpg.rf.816d4f9ca63a539abb4e7bd0bfafc0e0.jpg\n",
      "\n",
      "image 1/1 E:\\livingLab\\YOLOv8 \\test\\images\\202411111207-36-310510-127-353380_jpg.rf.816d4f9ca63a539abb4e7bd0bfafc0e0.jpg: 640x640 3 General Wastes, 2 CleanNets, 24.3ms\n",
      "Speed: 0.0ms preprocess, 24.3ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\predict10\u001b[0m\n",
      "- CleanNet: 0.900\n",
      "- CleanNet: 0.793\n",
      "- General Waste: 0.408\n",
      "- General Waste: 0.396\n",
      "- General Waste: 0.335\n",
      "Processing times:\n",
      "- Preprocess: 0.0ms\n",
      "- Inference: 24.339914321899414ms\n",
      "- Postprocess: 0.0ms\n",
      "\n",
      "Processing image 54/79: 202411111210-36-310957-127-353130_jpg.rf.693876733d993c585b16039273a00299.jpg\n",
      "\n",
      "image 1/1 E:\\livingLab\\YOLOv8 \\test\\images\\202411111210-36-310957-127-353130_jpg.rf.693876733d993c585b16039273a00299.jpg: 640x640 2 General Wastes, 2 CleanNets, 27.2ms\n",
      "Speed: 0.0ms preprocess, 27.2ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\predict10\u001b[0m\n",
      "- CleanNet: 0.649\n",
      "- CleanNet: 0.607\n",
      "- General Waste: 0.465\n",
      "- General Waste: 0.290\n",
      "Processing times:\n",
      "- Preprocess: 0.0ms\n",
      "- Inference: 27.172565460205078ms\n",
      "- Postprocess: 0.0ms\n",
      "\n",
      "Processing image 55/79: 202411111212-36-310790-127-352166_jpg.rf.d4bb484b862d3105c60cdbe0712b5651.jpg\n",
      "\n",
      "image 1/1 E:\\livingLab\\YOLOv8 \\test\\images\\202411111212-36-310790-127-352166_jpg.rf.d4bb484b862d3105c60cdbe0712b5651.jpg: 640x640 7 General Wastes, 2 CleanNets, 35.3ms\n",
      "Speed: 0.0ms preprocess, 35.3ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\predict10\u001b[0m\n",
      "- CleanNet: 0.728\n",
      "- CleanNet: 0.711\n",
      "- General Waste: 0.642\n",
      "- General Waste: 0.618\n",
      "- General Waste: 0.497\n",
      "- General Waste: 0.436\n",
      "- General Waste: 0.362\n",
      "- General Waste: 0.284\n",
      "- General Waste: 0.278\n",
      "Processing times:\n",
      "- Preprocess: 0.0ms\n",
      "- Inference: 35.2632999420166ms\n",
      "- Postprocess: 0.0ms\n",
      "\n",
      "Processing image 56/79: 202411111213-36-309176-127-351941_jpg.rf.ddc9885bd847ea5b84dc27ccf6fd9fd9.jpg\n",
      "\n",
      "image 1/1 E:\\livingLab\\YOLOv8 \\test\\images\\202411111213-36-309176-127-351941_jpg.rf.ddc9885bd847ea5b84dc27ccf6fd9fd9.jpg: 640x640 2 CleanNets, 32.0ms\n",
      "Speed: 15.5ms preprocess, 32.0ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\predict10\u001b[0m\n",
      "- CleanNet: 0.845\n",
      "- CleanNet: 0.656\n",
      "Processing times:\n",
      "- Preprocess: 15.502452850341797ms\n",
      "- Inference: 32.03940391540527ms\n",
      "- Postprocess: 0.0ms\n",
      "\n",
      "Processing image 57/79: 202411111215-36-309133-127-353018_jpg.rf.d74fb20ba076ca562ab4b9922841c5e4.jpg\n",
      "\n",
      "image 1/1 E:\\livingLab\\YOLOv8 \\test\\images\\202411111215-36-309133-127-353018_jpg.rf.d74fb20ba076ca562ab4b9922841c5e4.jpg: 640x640 1 General Waste, 4 CleanNets, 43.5ms\n",
      "Speed: 0.0ms preprocess, 43.5ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\predict10\u001b[0m\n",
      "- CleanNet: 0.806\n",
      "- CleanNet: 0.668\n",
      "- General Waste: 0.480\n",
      "- CleanNet: 0.477\n",
      "- CleanNet: 0.366\n",
      "Processing times:\n",
      "- Preprocess: 0.0ms\n",
      "- Inference: 43.49970817565918ms\n",
      "- Postprocess: 0.0ms\n",
      "\n",
      "Processing image 58/79: 202411111216-36-309841-127-353640_jpg.rf.0cfe34d4e561604f27e4d6d51709d093.jpg\n",
      "\n",
      "image 1/1 E:\\livingLab\\YOLOv8 \\test\\images\\202411111216-36-309841-127-353640_jpg.rf.0cfe34d4e561604f27e4d6d51709d093.jpg: 640x640 2 General Wastes, 3 CleanNets, 46.0ms\n",
      "Speed: 0.0ms preprocess, 46.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\predict10\u001b[0m\n",
      "- CleanNet: 0.936\n",
      "- CleanNet: 0.864\n",
      "- General Waste: 0.304\n",
      "- CleanNet: 0.265\n",
      "- General Waste: 0.255\n",
      "Processing times:\n",
      "- Preprocess: 0.0ms\n",
      "- Inference: 46.039581298828125ms\n",
      "- Postprocess: 1.0001659393310547ms\n",
      "\n",
      "Processing image 59/79: 202411111217-36-309239-127-353916_jpg.rf.45de2cd861119d382d4752616a1c7e31.jpg\n",
      "\n",
      "image 1/1 E:\\livingLab\\YOLOv8 \\test\\images\\202411111217-36-309239-127-353916_jpg.rf.45de2cd861119d382d4752616a1c7e31.jpg: 640x640 4 General Wastes, 2 CleanNets, 34.0ms\n",
      "Speed: 1.1ms preprocess, 34.0ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\predict10\u001b[0m\n",
      "- CleanNet: 0.882\n",
      "- CleanNet: 0.820\n",
      "- General Waste: 0.608\n",
      "- General Waste: 0.603\n",
      "- General Waste: 0.388\n",
      "- General Waste: 0.285\n",
      "Processing times:\n",
      "- Preprocess: 1.0807514190673828ms\n",
      "- Inference: 33.9818000793457ms\n",
      "- Postprocess: 0.0ms\n",
      "\n",
      "Processing image 60/79: 202411111220-36-308081-127-353130_jpg.rf.d482c4c6079ceee93fea151e3af3092a.jpg\n",
      "\n",
      "image 1/1 E:\\livingLab\\YOLOv8 \\test\\images\\202411111220-36-308081-127-353130_jpg.rf.d482c4c6079ceee93fea151e3af3092a.jpg: 640x640 2 General Wastes, 2 CleanNets, 34.7ms\n",
      "Speed: 0.0ms preprocess, 34.7ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\predict10\u001b[0m\n",
      "- CleanNet: 0.804\n",
      "- CleanNet: 0.684\n",
      "- General Waste: 0.464\n",
      "- General Waste: 0.397\n",
      "Processing times:\n",
      "- Preprocess: 0.0ms\n",
      "- Inference: 34.70563888549805ms\n",
      "- Postprocess: 0.0ms\n",
      "\n",
      "Processing image 61/79: 202411111221-36-307340-127-353540_jpg.rf.44607ac9cf9e836b2131ef722b16478e.jpg\n",
      "\n",
      "image 1/1 E:\\livingLab\\YOLOv8 \\test\\images\\202411111221-36-307340-127-353540_jpg.rf.44607ac9cf9e836b2131ef722b16478e.jpg: 640x640 (no detections), 27.0ms\n",
      "Speed: 0.0ms preprocess, 27.0ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\predict10\u001b[0m\n",
      "Processing times:\n",
      "- Preprocess: 0.0ms\n",
      "- Inference: 26.96394920349121ms\n",
      "- Postprocess: 0.0ms\n",
      "\n",
      "Processing image 62/79: 202411111223-36-307550-127-354681_jpg.rf.23480e44a8092c9c3311ca80c0ed1d96.jpg\n",
      "\n",
      "image 1/1 E:\\livingLab\\YOLOv8 \\test\\images\\202411111223-36-307550-127-354681_jpg.rf.23480e44a8092c9c3311ca80c0ed1d96.jpg: 640x640 2 CleanNets, 32.4ms\n",
      "Speed: 0.0ms preprocess, 32.4ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\predict10\u001b[0m\n",
      "- CleanNet: 0.713\n",
      "- CleanNet: 0.446\n",
      "Processing times:\n",
      "- Preprocess: 0.0ms\n",
      "- Inference: 32.399654388427734ms\n",
      "- Postprocess: 0.0ms\n",
      "\n",
      "Processing image 63/79: 202411111224-36-307974-127-355197_jpg.rf.8824813e9e1263d3c32b197514edcf0b.jpg\n",
      "\n",
      "image 1/1 E:\\livingLab\\YOLOv8 \\test\\images\\202411111224-36-307974-127-355197_jpg.rf.8824813e9e1263d3c32b197514edcf0b.jpg: 640x640 8 General Wastes, 3 CleanNets, 40.8ms\n",
      "Speed: 0.0ms preprocess, 40.8ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\predict10\u001b[0m\n",
      "- CleanNet: 0.797\n",
      "- General Waste: 0.689\n",
      "- General Waste: 0.587\n",
      "- General Waste: 0.577\n",
      "- General Waste: 0.558\n",
      "- CleanNet: 0.454\n",
      "- CleanNet: 0.358\n",
      "- General Waste: 0.343\n",
      "- General Waste: 0.339\n",
      "- General Waste: 0.303\n",
      "- General Waste: 0.288\n",
      "Processing times:\n",
      "- Preprocess: 0.0ms\n",
      "- Inference: 40.7717227935791ms\n",
      "- Postprocess: 0.40078163146972656ms\n",
      "\n",
      "Processing image 64/79: 202411111226-36-306652-127-353674_jpg.rf.bf1c0077ff4820a3e86121a92f4c3b79.jpg\n",
      "\n",
      "image 1/1 E:\\livingLab\\YOLOv8 \\test\\images\\202411111226-36-306652-127-353674_jpg.rf.bf1c0077ff4820a3e86121a92f4c3b79.jpg: 640x640 4 General Wastes, 2 CleanNets, 34.4ms\n",
      "Speed: 0.0ms preprocess, 34.4ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\predict10\u001b[0m\n",
      "- CleanNet: 0.877\n",
      "- CleanNet: 0.761\n",
      "- General Waste: 0.561\n",
      "- General Waste: 0.512\n",
      "- General Waste: 0.436\n",
      "- General Waste: 0.389\n",
      "Processing times:\n",
      "- Preprocess: 0.0ms\n",
      "- Inference: 34.36565399169922ms\n",
      "- Postprocess: 1.0004043579101562ms\n",
      "\n",
      "Processing image 65/79: 202411111226-36-306967-127-355016-1_jpg.rf.7b71d120bd3741d88af836c1e341b460.jpg\n",
      "\n",
      "image 1/1 E:\\livingLab\\YOLOv8 \\test\\images\\202411111226-36-306967-127-355016-1_jpg.rf.7b71d120bd3741d88af836c1e341b460.jpg: 640x640 1 CleanNet, 34.9ms\n",
      "Speed: 1.0ms preprocess, 34.9ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\predict10\u001b[0m\n",
      "- CleanNet: 0.527\n",
      "Processing times:\n",
      "- Preprocess: 0.9968280792236328ms\n",
      "- Inference: 34.87730026245117ms\n",
      "- Postprocess: 0.0ms\n",
      "\n",
      "Processing image 66/79: 202411111228-36-307488-127-352668_jpg.rf.c021bf13ef90ee8f5a21b75c386ba96e.jpg\n",
      "\n",
      "image 1/1 E:\\livingLab\\YOLOv8 \\test\\images\\202411111228-36-307488-127-352668_jpg.rf.c021bf13ef90ee8f5a21b75c386ba96e.jpg: 640x640 1 Large Waste Items, 2 General Wastes, 2 CleanNets, 26.6ms\n",
      "Speed: 1.0ms preprocess, 26.6ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\predict10\u001b[0m\n",
      "- CleanNet: 0.875\n",
      "- CleanNet: 0.724\n",
      "- General Waste: 0.673\n",
      "- General Waste: 0.294\n",
      "- Large Waste Items: 0.276\n",
      "Processing times:\n",
      "- Preprocess: 1.009225845336914ms\n",
      "- Inference: 26.632308959960938ms\n",
      "- Postprocess: 0.0ms\n",
      "\n",
      "Processing image 67/79: 202411111229-36-308507-127-351318_jpg.rf.d6a1f843c6a43aa44bb88941b03a0142.jpg\n",
      "\n",
      "image 1/1 E:\\livingLab\\YOLOv8 \\test\\images\\202411111229-36-308507-127-351318_jpg.rf.d6a1f843c6a43aa44bb88941b03a0142.jpg: 640x640 1 General Waste, 2 CleanNets, 27.5ms\n",
      "Speed: 0.0ms preprocess, 27.5ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\predict10\u001b[0m\n",
      "- CleanNet: 0.853\n",
      "- CleanNet: 0.741\n",
      "- General Waste: 0.272\n",
      "Processing times:\n",
      "- Preprocess: 0.0ms\n",
      "- Inference: 27.526140213012695ms\n",
      "- Postprocess: 0.0ms\n",
      "\n",
      "Processing image 68/79: 202411111231-36-307637-127-351064_jpg.rf.089c4d450857dd4dca741be2cf4f1cbb.jpg\n",
      "\n",
      "image 1/1 E:\\livingLab\\YOLOv8 \\test\\images\\202411111231-36-307637-127-351064_jpg.rf.089c4d450857dd4dca741be2cf4f1cbb.jpg: 640x640 2 Large Waste Itemss, 1 PP bag, 1 General Waste, 2 CleanNets, 33.3ms\n",
      "Speed: 1.4ms preprocess, 33.3ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\predict10\u001b[0m\n",
      "- CleanNet: 0.810\n",
      "- Large Waste Items: 0.761\n",
      "- General Waste: 0.667\n",
      "- CleanNet: 0.459\n",
      "- PP bag: 0.376\n",
      "- Large Waste Items: 0.318\n",
      "Processing times:\n",
      "- Preprocess: 1.4312267303466797ms\n",
      "- Inference: 33.30874443054199ms\n",
      "- Postprocess: 0.0ms\n",
      "\n",
      "Processing image 69/79: 202411111234-36-306079-127-351546_jpg.rf.c9c863d9067d40dd0035692f3d78d305.jpg\n",
      "\n",
      "image 1/1 E:\\livingLab\\YOLOv8 \\test\\images\\202411111234-36-306079-127-351546_jpg.rf.c9c863d9067d40dd0035692f3d78d305.jpg: 640x640 8 General Wastes, 2 CleanNets, 35.3ms\n",
      "Speed: 0.0ms preprocess, 35.3ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\predict10\u001b[0m\n",
      "- CleanNet: 0.935\n",
      "- CleanNet: 0.795\n",
      "- General Waste: 0.444\n",
      "- General Waste: 0.431\n",
      "- General Waste: 0.411\n",
      "- General Waste: 0.360\n",
      "- General Waste: 0.312\n",
      "- General Waste: 0.295\n",
      "- General Waste: 0.272\n",
      "- General Waste: 0.251\n",
      "Processing times:\n",
      "- Preprocess: 0.0ms\n",
      "- Inference: 35.25829315185547ms\n",
      "- Postprocess: 0.0ms\n",
      "\n",
      "Processing image 70/79: 202411111237-36-306997-127-351164_jpg.rf.45266fd2de02a14c1c674892c8a0ae33.jpg\n",
      "\n",
      "image 1/1 E:\\livingLab\\YOLOv8 \\test\\images\\202411111237-36-306997-127-351164_jpg.rf.45266fd2de02a14c1c674892c8a0ae33.jpg: 640x640 4 General Wastes, 2 CleanNets, 34.8ms\n",
      "Speed: 0.0ms preprocess, 34.8ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\predict10\u001b[0m\n",
      "- CleanNet: 0.838\n",
      "- CleanNet: 0.766\n",
      "- General Waste: 0.704\n",
      "- General Waste: 0.648\n",
      "- General Waste: 0.646\n",
      "- General Waste: 0.412\n",
      "Processing times:\n",
      "- Preprocess: 0.0ms\n",
      "- Inference: 34.837961196899414ms\n",
      "- Postprocess: 0.0ms\n",
      "\n",
      "Processing image 71/79: 202411111238-36-306863-127-350438_jpg.rf.6129b71701b75171bd7c11bce00321de.jpg\n",
      "\n",
      "image 1/1 E:\\livingLab\\YOLOv8 \\test\\images\\202411111238-36-306863-127-350438_jpg.rf.6129b71701b75171bd7c11bce00321de.jpg: 640x640 4 General Wastes, 2 CleanNets, 40.3ms\n",
      "Speed: 0.0ms preprocess, 40.3ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\predict10\u001b[0m\n",
      "- CleanNet: 0.813\n",
      "- CleanNet: 0.721\n",
      "- General Waste: 0.516\n",
      "- General Waste: 0.405\n",
      "- General Waste: 0.401\n",
      "- General Waste: 0.377\n",
      "Processing times:\n",
      "- Preprocess: 0.0ms\n",
      "- Inference: 40.30585289001465ms\n",
      "- Postprocess: 0.0ms\n",
      "\n",
      "Processing image 72/79: 202411111240-36-305737-127-350237_jpg.rf.07a211fd644530b35bcda797c76089f0.jpg\n",
      "\n",
      "image 1/1 E:\\livingLab\\YOLOv8 \\test\\images\\202411111240-36-305737-127-350237_jpg.rf.07a211fd644530b35bcda797c76089f0.jpg: 640x640 1 General Waste, 5 CleanNets, 34.8ms\n",
      "Speed: 0.0ms preprocess, 34.8ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\predict10\u001b[0m\n",
      "- CleanNet: 0.644\n",
      "- CleanNet: 0.575\n",
      "- General Waste: 0.541\n",
      "- CleanNet: 0.388\n",
      "- CleanNet: 0.353\n",
      "- CleanNet: 0.352\n",
      "Processing times:\n",
      "- Preprocess: 0.0ms\n",
      "- Inference: 34.82198715209961ms\n",
      "- Postprocess: 0.0ms\n",
      "\n",
      "Processing image 73/79: 202411111241-36-305407-127-348838_jpg.rf.c9a72083f4584c5401821f536b962985.jpg\n",
      "\n",
      "image 1/1 E:\\livingLab\\YOLOv8 \\test\\images\\202411111241-36-305407-127-348838_jpg.rf.c9a72083f4584c5401821f536b962985.jpg: 640x640 1 General Waste, 2 CleanNets, 34.8ms\n",
      "Speed: 0.0ms preprocess, 34.8ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\predict10\u001b[0m\n",
      "- CleanNet: 0.715\n",
      "- CleanNet: 0.672\n",
      "- General Waste: 0.464\n",
      "Processing times:\n",
      "- Preprocess: 0.0ms\n",
      "- Inference: 34.757375717163086ms\n",
      "- Postprocess: 0.0ms\n",
      "\n",
      "Processing image 74/79: 202411111246-36-324233-127-344985_jpg.rf.d4fac2031eba1010c932d2ba451f0b9e.jpg\n",
      "\n",
      "image 1/1 E:\\livingLab\\YOLOv8 \\test\\images\\202411111246-36-324233-127-344985_jpg.rf.d4fac2031eba1010c932d2ba451f0b9e.jpg: 640x640 1 Large Waste Items, 5 General Wastes, 2 CleanNets, 34.8ms\n",
      "Speed: 0.0ms preprocess, 34.8ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\predict10\u001b[0m\n",
      "- CleanNet: 0.842\n",
      "- General Waste: 0.587\n",
      "- General Waste: 0.538\n",
      "- General Waste: 0.537\n",
      "- General Waste: 0.499\n",
      "- CleanNet: 0.459\n",
      "- Large Waste Items: 0.429\n",
      "- General Waste: 0.398\n",
      "Processing times:\n",
      "- Preprocess: 0.0ms\n",
      "- Inference: 34.7898006439209ms\n",
      "- Postprocess: 0.0ms\n",
      "\n",
      "Processing image 75/79: 202411111251-36-322309-127-342752_jpg.rf.cee98eb6f397c8e712d2cc1e38b025ba.jpg\n",
      "\n",
      "image 1/1 E:\\livingLab\\YOLOv8 \\test\\images\\202411111251-36-322309-127-342752_jpg.rf.cee98eb6f397c8e712d2cc1e38b025ba.jpg: 640x640 4 General Wastes, 2 CleanNets, 41.4ms\n",
      "Speed: 0.0ms preprocess, 41.4ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\predict10\u001b[0m\n",
      "- CleanNet: 0.959\n",
      "- CleanNet: 0.891\n",
      "- General Waste: 0.630\n",
      "- General Waste: 0.452\n",
      "- General Waste: 0.398\n",
      "- General Waste: 0.321\n",
      "Processing times:\n",
      "- Preprocess: 0.0ms\n",
      "- Inference: 41.43166542053223ms\n",
      "- Postprocess: 0.0ms\n",
      "\n",
      "Processing image 76/79: 202411111252-36-321934-127-343575_jpg.rf.f74b0372f7638eef7e00d1d9f0c16f70.jpg\n",
      "\n",
      "image 1/1 E:\\livingLab\\YOLOv8 \\test\\images\\202411111252-36-321934-127-343575_jpg.rf.f74b0372f7638eef7e00d1d9f0c16f70.jpg: 640x640 1 General Waste, 1 CleanNet, 34.4ms\n",
      "Speed: 6.1ms preprocess, 34.4ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\predict10\u001b[0m\n",
      "- CleanNet: 0.762\n",
      "- General Waste: 0.660\n",
      "Processing times:\n",
      "- Preprocess: 6.14476203918457ms\n",
      "- Inference: 34.44647789001465ms\n",
      "- Postprocess: 0.0ms\n",
      "\n",
      "Processing image 77/79: 23907_68481_2024_jpg.rf.b54912c39030808ac7c4302f5f2a9541.jpg\n",
      "\n",
      "image 1/1 E:\\livingLab\\YOLOv8 \\test\\images\\23907_68481_2024_jpg.rf.b54912c39030808ac7c4302f5f2a9541.jpg: 640x640 2 PP bags, 2 General Wastes, 34.9ms\n",
      "Speed: 0.0ms preprocess, 34.9ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\predict10\u001b[0m\n",
      "- PP bag: 0.480\n",
      "- PP bag: 0.408\n",
      "- General Waste: 0.321\n",
      "- General Waste: 0.269\n",
      "Processing times:\n",
      "- Preprocess: 0.0ms\n",
      "- Inference: 34.87753868103027ms\n",
      "- Postprocess: 0.0ms\n",
      "\n",
      "Processing image 78/79: 280-1-_jpg.rf.da456a3686d9264b2091920e32945993.jpg\n",
      "\n",
      "image 1/1 E:\\livingLab\\YOLOv8 \\test\\images\\280-1-_jpg.rf.da456a3686d9264b2091920e32945993.jpg: 640x640 3 PP bags, 34.8ms\n",
      "Speed: 0.0ms preprocess, 34.8ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\predict10\u001b[0m\n",
      "- PP bag: 0.943\n",
      "- PP bag: 0.710\n",
      "- PP bag: 0.373\n",
      "Processing times:\n",
      "- Preprocess: 0.0ms\n",
      "- Inference: 34.81721878051758ms\n",
      "- Postprocess: 0.0ms\n",
      "\n",
      "Processing image 79/79: 40e6c194a7_jpg.rf.d53564843314e6adb88ef92998cdaf82.jpg\n",
      "\n",
      "image 1/1 E:\\livingLab\\YOLOv8 \\test\\images\\40e6c194a7_jpg.rf.d53564843314e6adb88ef92998cdaf82.jpg: 640x640 3 PP bags, 34.9ms\n",
      "Speed: 0.0ms preprocess, 34.9ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\predict10\u001b[0m\n",
      "- PP bag: 0.882\n",
      "- PP bag: 0.385\n",
      "- PP bag: 0.328\n",
      "Processing times:\n",
      "- Preprocess: 0.0ms\n",
      "- Inference: 34.88612174987793ms\n",
      "- Postprocess: 0.0ms\n",
      "\n",
      "All results saved to ./runs/detect/predict/\n",
      "\n",
      "Converting labels to major categories...\n",
      "\n",
      "Performing binary classification...\n",
      "\n",
      "Image: 202411061348-36-313711-127-349988_jpg.rf.3581e8d79a3a050686554a20476cdb34.jpg\n",
      "Detected classes: ['CleanNet', 'CleanNet', 'General Waste']\n",
      "Classification: Negative\n",
      "\n",
      "Image: 202411061346-36-311745-127-353092_jpg.rf.3e408eccde358b63c05fd5a29f956aab.jpg\n",
      "Detected classes: ['General Waste', 'CleanNet', 'CleanNet']\n",
      "Classification: Negative\n",
      "\n",
      "Image: 202411111240-36-305737-127-350237_jpg.rf.07a211fd644530b35bcda797c76089f0.jpg\n",
      "Detected classes: ['CleanNet', 'CleanNet', 'General Waste', 'Large Waste Items']\n",
      "Classification: Positive\n",
      "\n",
      "Image: 202411111231-36-307637-127-351064_jpg.rf.089c4d450857dd4dca741be2cf4f1cbb.jpg\n",
      "Detected classes: ['CleanNet', 'CleanNet', 'Large Waste Items']\n",
      "Classification: Positive\n",
      "\n",
      "Image: 202411061319-36-307974-127-355197_jpg.rf.40e66b47ea454f90ee055c740410cc1c.jpg\n",
      "Detected classes: ['CleanNet', 'CleanNet', 'General Waste']\n",
      "Classification: Negative\n",
      "\n",
      "Image: 202411061313-36-307488-127-352668_jpg.rf.2debbe814df92503a0079be859650cc0.jpg\n",
      "Detected classes: ['Large Waste Items', 'CleanNet', 'CleanNet', 'General Waste']\n",
      "Classification: Positive\n",
      "\n",
      "Image: 202411111223-36-307550-127-354681_jpg.rf.23480e44a8092c9c3311ca80c0ed1d96.jpg\n",
      "Detected classes: ['CleanNet', 'CleanNet']\n",
      "Classification: Negative\n",
      "\n",
      "Image: 202411111216-36-309841-127-353640_jpg.rf.0cfe34d4e561604f27e4d6d51709d093.jpg\n",
      "Detected classes: ['CleanNet', 'CleanNet', 'General Waste']\n",
      "Classification: Negative\n",
      "\n",
      "Image: 202411111148-36-330942-127-338586_jpg.rf.09088434bf25f50d833296c7059228a4.jpg\n",
      "Detected classes: ['CleanNet', 'CleanNet', 'CleanNet']\n",
      "Classification: Negative\n",
      "\n",
      "Image: 202411061431-36-309908-127-348185_jpg.rf.23fe7b1ddec31535d885e6cc9288d2da.jpg\n",
      "Detected classes: ['Large Waste Items', 'Large Waste Items', 'Large Waste Items', 'Large Waste Items', 'Large Waste Items', 'Large Waste Items']\n",
      "Classification: Positive\n",
      "\n",
      "Image: 202411061348-36-310510-127-353380_jpg.rf.1b445c1bbde0cedac1df9b0f9c220dd4.jpg\n",
      "Detected classes: ['CleanNet', 'CleanNet', 'General Waste']\n",
      "Classification: Negative\n",
      "\n",
      "Image: 202411081236-36-423913-127-407165-1_jpg.rf.0dfb1f31bd1bafebca3f8424e98cdc61.jpg\n",
      "Detected classes: ['General Waste', 'Large Waste Items', 'Large Waste Items', 'Large Waste Items', 'Large Waste Items', 'Large Waste Items', 'Large Waste Items']\n",
      "Classification: Positive\n",
      "\n",
      "Image: -7-_jpg.rf.3e4e45f01df8da1ac999a94efb60473e.jpg\n",
      "Detected classes: ['PP bag']\n",
      "Classification: Negative\n",
      "\n",
      "Image: 202411111237-36-306997-127-351164_jpg.rf.45266fd2de02a14c1c674892c8a0ae33.jpg\n",
      "Detected classes: ['CleanNet', 'CleanNet', 'General Waste', 'General Waste', 'General Waste', 'General Waste']\n",
      "Classification: Negative\n",
      "\n",
      "Image: 202411061352-36-313215-127-351277-3_jpg.rf.46cfc13747bebd1a1755da3b8a3cd8dd.jpg\n",
      "Detected classes: ['General Waste', 'Large Waste Items', 'Large Waste Items', 'PP bag', 'PP bag', 'PP bag', 'PP bag', 'PP bag', 'PP bag', 'PP bag', 'PP bag', 'PP bag']\n",
      "Classification: Positive\n",
      "\n",
      "Image: 202411111206-36-311745-127-353092_jpg.rf.175848f0b77ba404e4e6f11f44f8c590.jpg\n",
      "Detected classes: ['CleanNet', 'CleanNet', 'General Waste']\n",
      "Classification: Negative\n",
      "\n",
      "Image: 202411111221-36-307340-127-353540_jpg.rf.44607ac9cf9e836b2131ef722b16478e.jpg\n",
      "Detected classes: ['CleanNet', 'CleanNet']\n",
      "Classification: Negative\n",
      "\n",
      "Image: 202411111150-36-331699-127-339169-2_jpg.rf.30de6ea0a36f7a5dff4c2d5633c75ff7.jpg\n",
      "Detected classes: ['Large Waste Items', 'Large Waste Items']\n",
      "Classification: Positive\n",
      "\n",
      "Image: 202411061342-36-309176-127-351941_jpg.rf.05afb3c8e078242b96c4a730357296bb.jpg\n",
      "Detected classes: ['CleanNet', 'General Waste', 'CleanNet']\n",
      "Classification: Negative\n",
      "\n",
      "Image: 202411061423-36-313839-127-348579_jpg.rf.4c6e29fd453c998c1710c6dd014daacf.jpg\n",
      "Detected classes: ['General Waste', 'Large Waste Items', 'Large Waste Items', 'Large Waste Items', 'Large Waste Items', 'Large Waste Items', 'Large Waste Items', 'Large Waste Items', 'Large Waste Items', 'Large Waste Items', 'General Waste']\n",
      "Classification: Positive\n",
      "\n",
      "Image: 202411061337-36-309239-127-353916_jpg.rf.4c7a323affb98f997405ec20ca6d216e.jpg\n",
      "Detected classes: ['CleanNet', 'CleanNet', 'General Waste', 'General Waste']\n",
      "Classification: Negative\n",
      "\n",
      "Image: 202411061330-36-318792-127-344275_jpg.rf.3f74fb24878d9a28014f79a8ad00e4c4.jpg\n",
      "Detected classes: ['General Waste']\n",
      "Classification: Negative\n",
      "\n",
      "Image: 202411111217-36-309239-127-353916_jpg.rf.45de2cd861119d382d4752616a1c7e31.jpg\n",
      "Detected classes: ['General Waste', 'General Waste', 'General Waste', 'General Waste', 'CleanNet', 'CleanNet']\n",
      "Classification: Negative\n",
      "\n",
      "Image: 202411111150-36-331699-127-339169-1_jpg.rf.2dfbaba491bab23e08d12300bdec0a40.jpg\n",
      "Detected classes: ['CleanNet', 'CleanNet', 'CleanNet', 'General Waste', 'General Waste', 'General Waste', 'General Waste']\n",
      "Classification: Negative\n",
      "\n",
      "Image: 202411061434-36-309502-127-348344_jpg.rf.18196b3699941fd051eb67f4246945bb.jpg\n",
      "Detected classes: ['Large Waste Items', 'Large Waste Items', 'Large Waste Items', 'General Waste', 'Large Waste Items']\n",
      "Classification: Positive\n",
      "\n",
      "Image: 202411061349-36-313040-127-350293_jpg.rf.52fab8e4c82156dcff635e98319420f9.jpg\n",
      "Detected classes: ['General Waste', 'CleanNet', 'CleanNet', 'General Waste']\n",
      "Classification: Negative\n",
      "\n",
      "Image: 202411061323-36-319426-127-343581-2_jpg.rf.5c85eeb865231f3fb9ac4fd355319416.jpg\n",
      "Detected classes: ['General Waste', 'General Waste', 'General Waste', 'General Waste', 'General Waste', 'General Waste', 'General Waste', 'General Waste', 'Large Waste Items']\n",
      "Classification: Positive\n",
      "\n",
      "Image: 202411061415-36-316636-127-347495_jpg.rf.79ef2fb18c5b765ead9711ec6516fcea.jpg\n",
      "Detected classes: ['Large Waste Items', 'Large Waste Items', 'Large Waste Items', 'Large Waste Items', 'Large Waste Items', 'Large Waste Items']\n",
      "Classification: Positive\n",
      "\n",
      "Image: 202411111226-36-306967-127-355016-1_jpg.rf.7b71d120bd3741d88af836c1e341b460.jpg\n",
      "Detected classes: ['CleanNet', 'CleanNet']\n",
      "Classification: Negative\n",
      "\n",
      "Image: 202411111210-36-310957-127-353130_jpg.rf.693876733d993c585b16039273a00299.jpg\n",
      "Detected classes: ['CleanNet', 'CleanNet', 'General Waste', 'General Waste', 'General Waste']\n",
      "Classification: Negative\n",
      "\n",
      "Image: 202411111238-36-306863-127-350438_jpg.rf.6129b71701b75171bd7c11bce00321de.jpg\n",
      "Detected classes: ['CleanNet', 'CleanNet', 'Large Waste Items', 'Large Waste Items', 'General Waste', 'General Waste']\n",
      "Classification: Positive\n",
      "\n",
      "Image: 202411061339-36-310790-127-352166_jpg.rf.78fd2988c35f72ab7c8c932764bb04c6.jpg\n",
      "Detected classes: ['CleanNet', 'CleanNet', 'General Waste']\n",
      "Classification: Negative\n",
      "\n",
      "Image: 202411061342-36-315640-127-349174_jpg.rf.5444f5989d8b4ed01d08d05553d63571.jpg\n",
      "Detected classes: ['CleanNet', 'CleanNet', 'General Waste']\n",
      "Classification: Negative\n",
      "\n",
      "Image: 202411061322-36-320218-127-343319_jpg.rf.86814c1d60da676d5d9596a2a3da0efb.jpg\n",
      "Detected classes: ['CleanNet', 'General Waste', 'General Waste', 'General Waste', 'General Waste', 'Large Waste Items', 'Large Waste Items']\n",
      "Classification: Positive\n",
      "\n",
      "Image: 202411061329-36-309133-127-353018_jpg.rf.66723f2b0a1f776c240789b2605d7c6e.jpg\n",
      "Detected classes: ['CleanNet', 'CleanNet', 'General Waste']\n",
      "Classification: Negative\n",
      "\n",
      "Image: 202411061423-36-314851-127-347123_jpg.rf.6b1796d5ce76d550ec8c684a8c23fafd.jpg\n",
      "Detected classes: ['General Waste', 'Large Waste Items', 'Large Waste Items', 'Large Waste Items', 'Large Waste Items', 'Large Waste Items', 'Large Waste Items', 'Large Waste Items', 'PP bag', 'Large Waste Items', 'PP bag', 'PP bag', 'PP bag', 'PP bag', 'PP bag', 'PP bag', 'PP bag', 'PP bag', 'PP bag', 'PP bag', 'PP bag', 'PP bag', 'PP bag', 'PP bag', 'PP bag', 'PP bag', 'PP bag', 'PP bag', 'PP bag', 'PP bag', 'PP bag', 'PP bag', 'PP bag', 'PP bag', 'PP bag', 'PP bag', 'PP bag', 'PP bag']\n",
      "Classification: Positive\n",
      "\n",
      "Image: 202411061431-36-311574-127-349342_jpg.rf.5182513e42f5000cf9af2f91816f2114.jpg\n",
      "Detected classes: ['Large Waste Items', 'Large Waste Items', 'Large Waste Items', 'Large Waste Items', 'General Waste']\n",
      "Classification: Positive\n",
      "\n",
      "Image: 23907_68481_2024_jpg.rf.b54912c39030808ac7c4302f5f2a9541.jpg\n",
      "Detected classes: ['PP bag', 'PP bag', 'PP bag', 'PP bag', 'PP bag', 'PP bag', 'PP bag', 'PP bag']\n",
      "Classification: Negative\n",
      "\n",
      "Image: 202411061438-36-309254-127-350078_jpg.rf.63980ab462c1a491c72131a321aa9bc4.jpg\n",
      "Detected classes: ['Large Waste Items', 'Large Waste Items', 'Large Waste Items', 'Large Waste Items']\n",
      "Classification: Positive\n",
      "\n",
      "Image: 202411061346-36-308081-127-353130_jpg.rf.83770fab01c2a2e62838a2a2729231a7.jpg\n",
      "Detected classes: ['CleanNet', 'CleanNet', 'General Waste']\n",
      "Classification: Negative\n",
      "\n",
      "Image: 202411061358-36-308465-127-354548_jpg.rf.965f2f2b47de25bdc9d795cb71576543.jpg\n",
      "Detected classes: ['CleanNet', 'CleanNet', 'General Waste', 'General Waste', 'General Waste']\n",
      "Classification: Negative\n",
      "\n",
      "Image: 202411061433-36-309089-127-349308_jpg.rf.680978375d34a5ce6bd988b65213fe68.jpg\n",
      "Detected classes: ['General Waste', 'General Waste', 'General Waste', 'Large Waste Items', 'Large Waste Items', 'Large Waste Items', 'Large Waste Items', 'Large Waste Items', 'Large Waste Items']\n",
      "Classification: Positive\n",
      "\n",
      "Image: 202411111207-36-310510-127-353380_jpg.rf.816d4f9ca63a539abb4e7bd0bfafc0e0.jpg\n",
      "Detected classes: ['General Waste', 'CleanNet', 'CleanNet']\n",
      "Classification: Negative\n",
      "\n",
      "Image: 202411061328-36-318467-127-342773_jpg.rf.8e7d885664cc4a093d1ef0afcc79b2e8.jpg\n",
      "Detected classes: ['General Waste', 'General Waste', 'CleanNet', 'CleanNet']\n",
      "Classification: Negative\n",
      "\n",
      "Image: 202411111224-36-307974-127-355197_jpg.rf.8824813e9e1263d3c32b197514edcf0b.jpg\n",
      "Detected classes: ['CleanNet', 'CleanNet', 'General Waste', 'General Waste', 'General Waste', 'General Waste', 'General Waste']\n",
      "Classification: Negative\n",
      "\n",
      "Image: 202411061345-36-314850-127-350158_jpg.rf.a5c076f0199e673c36da4ac790ee940e.jpg\n",
      "Detected classes: ['CleanNet', 'CleanNet', 'General Waste', 'General Waste']\n",
      "Classification: Negative\n",
      "\n",
      "Image: 202411061359-36-310957-127-353130_jpg.rf.906b474010a7ba8fcea77eeef01433cd.jpg\n",
      "Detected classes: ['CleanNet', 'CleanNet', 'General Waste', 'General Waste', 'General Waste', 'General Waste', 'General Waste']\n",
      "Classification: Negative\n",
      "\n",
      "Image: 202411061328-36-318047-127-344154_jpg.rf.7aefec6f116650e0e611553b9811a6d8.jpg\n",
      "Detected classes: ['CleanNet', 'CleanNet', 'General Waste']\n",
      "Classification: Negative\n",
      "\n",
      "Image: 202411111136-36-331616-127-335799_jpg.rf.896aa812873491b43e52dfbd3261d149.jpg\n",
      "Detected classes: ['PP bag', 'PP bag', 'PP bag', 'PP bag', 'PP bag', 'PP bag', 'PP bag', 'PP bag', 'PP bag', 'PP bag', 'PP bag', 'PP bag', 'PP bag', 'PP bag', 'PP bag', 'PP bag', 'PP bag', 'PP bag']\n",
      "Classification: Negative\n",
      "\n",
      "Image: 202411111228-36-307488-127-352668_jpg.rf.c021bf13ef90ee8f5a21b75c386ba96e.jpg\n",
      "Detected classes: ['CleanNet', 'CleanNet', 'General Waste', 'General Waste', 'General Waste']\n",
      "Classification: Negative\n",
      "\n",
      "Image: 202411061423-36-315046-127-346080_jpg.rf.bdc3f041759c2e28c0f15e1397a3584d.jpg\n",
      "Detected classes: ['Large Waste Items', 'Large Waste Items', 'Large Waste Items', 'Large Waste Items', 'Large Waste Items', 'Large Waste Items', 'Large Waste Items']\n",
      "Classification: Positive\n",
      "\n",
      "Image: 202411111137-36-331383-127-335375_jpg.rf.a50332a13cae1728d2c5dc6162c5b05d.jpg\n",
      "Detected classes: ['PP bag', 'PP bag', 'PP bag', 'PP bag', 'PP bag']\n",
      "Classification: Negative\n",
      "\n",
      "Image: 202411111246-36-324233-127-344985_jpg.rf.d4fac2031eba1010c932d2ba451f0b9e.jpg\n",
      "Detected classes: ['General Waste', 'General Waste', 'General Waste', 'CleanNet', 'CleanNet']\n",
      "Classification: Negative\n",
      "\n",
      "Image: 202411061346-36-314198-127-350451_jpg.rf.c77a12c283995fb4057bd0ca00b83161.jpg\n",
      "Detected classes: ['Large Waste Items', 'Large Waste Items']\n",
      "Classification: Positive\n",
      "\n",
      "Image: 202411111234-36-306079-127-351546_jpg.rf.c9c863d9067d40dd0035692f3d78d305.jpg\n",
      "Detected classes: ['General Waste', 'General Waste', 'General Waste', 'General Waste', 'General Waste', 'General Waste', 'CleanNet', 'CleanNet']\n",
      "Classification: Negative\n",
      "\n",
      "Image: 202411111220-36-308081-127-353130_jpg.rf.d482c4c6079ceee93fea151e3af3092a.jpg\n",
      "Detected classes: ['CleanNet', 'CleanNet', 'General Waste', 'General Waste', 'General Waste', 'General Waste']\n",
      "Classification: Negative\n",
      "\n",
      "Image: 202411061322-36-307550-127-354681_jpg.rf.e09f59bc4cc8915a9da9c47c91f306f0.jpg\n",
      "Detected classes: ['CleanNet', 'CleanNet', 'PP bag', 'General Waste', 'General Waste', 'General Waste']\n",
      "Classification: Negative\n",
      "\n",
      "Image: 202411111251-36-322309-127-342752_jpg.rf.cee98eb6f397c8e712d2cc1e38b025ba.jpg\n",
      "Detected classes: ['CleanNet', 'CleanNet', 'General Waste', 'General Waste', 'General Waste', 'General Waste']\n",
      "Classification: Negative\n",
      "\n",
      "Image: 280-1-_jpg.rf.da456a3686d9264b2091920e32945993.jpg\n",
      "Detected classes: ['PP bag', 'PP bag']\n",
      "Classification: Negative\n",
      "\n",
      "Image: 202411061350-36-314129-127-351297_jpg.rf.7b7238965a8cae0683e4d2bac4f5a06a.jpg\n",
      "Detected classes: ['CleanNet', 'CleanNet', 'General Waste', 'General Waste', 'General Waste', 'General Waste']\n",
      "Classification: Negative\n",
      "\n",
      "Image: 202411111139-36-331111-127-337848_jpg.rf.cbb982f1d1e160b11ae586add407ded9.jpg\n",
      "Detected classes: ['Large Waste Items', 'CleanNet']\n",
      "Classification: Positive\n",
      "\n",
      "Image: 202411061352-36-313215-127-351277-2_jpg.rf.f6aa801aed2a6e2e10462b781d4358bd.jpg\n",
      "Detected classes: ['PP bag', 'PP bag', 'PP bag', 'PP bag', 'PP bag', 'PP bag', 'Large Waste Items']\n",
      "Classification: Positive\n",
      "\n",
      "Image: 202411061418-36-315697-127-346125_jpg.rf.e9aab1e2970ae7ad8efb2ab0e3dd7b81.jpg\n",
      "Detected classes: ['General Waste', 'Large Waste Items', 'Large Waste Items', 'Large Waste Items', 'Large Waste Items', 'Large Waste Items', 'Large Waste Items', 'Large Waste Items', 'Large Waste Items', 'Large Waste Items']\n",
      "Classification: Positive\n",
      "\n",
      "Image: 202411111153-36-36-331686-127-340556_jpg.rf.f610735c1c46cf5328fb96c544f43dd2.jpg\n",
      "Detected classes: ['CleanNet', 'CleanNet']\n",
      "Classification: Negative\n",
      "\n",
      "Image: 40e6c194a7_jpg.rf.d53564843314e6adb88ef92998cdaf82.jpg\n",
      "Detected classes: ['PP bag']\n",
      "Classification: Negative\n",
      "\n",
      "Image: 202411061343-36-315732-127-349908_jpg.rf.f32a738af7e8cb0569568da90e660036.jpg\n",
      "Detected classes: ['General Waste', 'General Waste', 'General Waste', 'General Waste', 'CleanNet']\n",
      "Classification: Negative\n",
      "\n",
      "Image: 202411061420-36-315055-127-346190_jpg.rf.f735fd248bc6690f4c75068de0897d1f.jpg\n",
      "Detected classes: ['Large Waste Items', 'Large Waste Items', 'Large Waste Items']\n",
      "Classification: Positive\n",
      "\n",
      "Image: 202411111212-36-310790-127-352166_jpg.rf.d4bb484b862d3105c60cdbe0712b5651.jpg\n",
      "Detected classes: ['General Waste', 'General Waste', 'General Waste', 'General Waste', 'General Waste', 'General Waste', 'CleanNet', 'CleanNet', 'General Waste']\n",
      "Classification: Negative\n",
      "\n",
      "Image: 202411111213-36-309176-127-351941_jpg.rf.ddc9885bd847ea5b84dc27ccf6fd9fd9.jpg\n",
      "Detected classes: ['CleanNet', 'CleanNet', 'Large Waste Items']\n",
      "Classification: Positive\n",
      "\n",
      "Image: 202411111215-36-309133-127-353018_jpg.rf.d74fb20ba076ca562ab4b9922841c5e4.jpg\n",
      "Detected classes: ['CleanNet', 'CleanNet', 'General Waste']\n",
      "Classification: Negative\n",
      "\n",
      "Image: 202411111229-36-308507-127-351318_jpg.rf.d6a1f843c6a43aa44bb88941b03a0142.jpg\n",
      "Detected classes: ['CleanNet', 'CleanNet', 'General Waste']\n",
      "Classification: Negative\n",
      "\n",
      "Image: 202411061326-36-318897-127-343185_jpg.rf.d6707abf64c488ae79ddec26206a2f89.jpg\n",
      "Detected classes: ['Large Waste Items', 'Large Waste Items', 'CleanNet', 'General Waste', 'General Waste']\n",
      "Classification: Positive\n",
      "\n",
      "Image: 202411111226-36-306652-127-353674_jpg.rf.bf1c0077ff4820a3e86121a92f4c3b79.jpg\n",
      "Detected classes: ['General Waste', 'CleanNet', 'CleanNet']\n",
      "Classification: Negative\n",
      "\n",
      "Image: 202411111241-36-305407-127-348838_jpg.rf.c9a72083f4584c5401821f536b962985.jpg\n",
      "Detected classes: ['General Waste', 'CleanNet', 'CleanNet']\n",
      "Classification: Negative\n",
      "\n",
      "Image: 202411081236-36-423913-127-407165-2_jpg.rf.f6e2a098f84601afac7d2350a475d4cc.jpg\n",
      "Detected classes: ['PP bag', 'PP bag', 'PP bag', 'PP bag', 'PP bag', 'PP bag', 'PP bag', 'PP bag', 'PP bag', 'PP bag', 'PP bag', 'PP bag', 'PP bag', 'PP bag', 'PP bag', 'PP bag', 'PP bag', 'PP bag', 'PP bag', 'PP bag', 'PP bag', 'PP bag']\n",
      "Classification: Negative\n",
      "\n",
      "Image: 202411061349-36-311745-127-353092_jpg.rf.fb5f4cbdb3036192e55d681050fe72da.jpg\n",
      "Detected classes: ['General Waste', 'General Waste', 'General Waste', 'CleanNet', 'CleanNet']\n",
      "Classification: Negative\n",
      "\n",
      "Image: 202411111252-36-321934-127-343575_jpg.rf.f74b0372f7638eef7e00d1d9f0c16f70.jpg\n",
      "Detected classes: ['General Waste', 'CleanNet']\n",
      "Classification: Negative\n",
      "\n",
      "Image: 202411111156-36-331079-127-341792_jpg.rf.fe0c796d57c7083f244e29ed95c6585c.jpg\n",
      "Detected classes: ['General Waste', 'General Waste', 'General Waste', 'CleanNet', 'CleanNet']\n",
      "Classification: Negative\n",
      "\n",
      "Image: 202411061325-36-308399-127-354248_jpg.rf.fe8e74b6e0015e468fad270eb87ca499.jpg\n",
      "Detected classes: ['CleanNet', 'CleanNet', 'General Waste', 'General Waste']\n",
      "Classification: Negative\n",
      "\n",
      "Analyzing confidence scores...\n",
      "\n",
      "Confidence Score Statistics by Class:\n",
      "\n",
      "Large Waste Items:\n",
      "- Number of detections: 65\n",
      "- Average confidence: 0.427\n",
      "- Maximum confidence: 0.912\n",
      "- Minimum confidence: 0.252\n",
      "- All confidence scores: ['0.367', '0.681', '0.270', '0.267', '0.575', '0.448', '0.269', '0.252', '0.633', '0.317', '0.373', '0.437', '0.396', '0.275', '0.257', '0.624', '0.543', '0.542', '0.493', '0.261', '0.715', '0.644', '0.470', '0.269', '0.455', '0.334', '0.483', '0.407', '0.338', '0.274', '0.287', '0.278', '0.912', '0.750', '0.642', '0.526', '0.313', '0.288', '0.375', '0.264', '0.555', '0.504', '0.427', '0.425', '0.416', '0.414', '0.364', '0.349', '0.291', '0.255', '0.438', '0.456', '0.386', '0.275', '0.607', '0.402', '0.376', '0.271', '0.745', '0.405', '0.332', '0.276', '0.761', '0.318', '0.429']\n",
      "\n",
      "PP bag:\n",
      "- Number of detections: 44\n",
      "- Average confidence: 0.557\n",
      "- Maximum confidence: 0.943\n",
      "- Minimum confidence: 0.270\n",
      "- All confidence scores: ['0.798', '0.361', '0.555', '0.503', '0.367', '0.362', '0.297', '0.543', '0.446', '0.307', '0.291', '0.922', '0.912', '0.909', '0.906', '0.895', '0.863', '0.776', '0.749', '0.748', '0.744', '0.729', '0.670', '0.664', '0.541', '0.521', '0.512', '0.451', '0.417', '0.390', '0.306', '0.306', '0.285', '0.300', '0.270', '0.376', '0.480', '0.408', '0.943', '0.710', '0.373', '0.882', '0.385', '0.328']\n",
      "\n",
      "General Waste:\n",
      "- Number of detections: 163\n",
      "- Average confidence: 0.426\n",
      "- Maximum confidence: 0.741\n",
      "- Minimum confidence: 0.250\n",
      "- All confidence scores: ['0.690', '0.409', '0.318', '0.308', '0.525', '0.374', '0.592', '0.482', '0.311', '0.253', '0.251', '0.617', '0.555', '0.412', '0.604', '0.381', '0.565', '0.404', '0.494', '0.464', '0.387', '0.250', '0.428', '0.263', '0.487', '0.553', '0.438', '0.385', '0.289', '0.454', '0.273', '0.338', '0.318', '0.280', '0.549', '0.284', '0.277', '0.332', '0.322', '0.317', '0.284', '0.452', '0.571', '0.301', '0.301', '0.300', '0.387', '0.382', '0.261', '0.364', '0.499', '0.305', '0.294', '0.631', '0.607', '0.587', '0.520', '0.741', '0.449', '0.312', '0.649', '0.485', '0.389', '0.352', '0.532', '0.437', '0.321', '0.317', '0.476', '0.378', '0.306', '0.650', '0.366', '0.340', '0.330', '0.263', '0.388', '0.532', '0.396', '0.325', '0.522', '0.336', '0.534', '0.292', '0.398', '0.325', '0.503', '0.430', '0.422', '0.720', '0.526', '0.382', '0.315', '0.313', '0.254', '0.330', '0.408', '0.396', '0.335', '0.465', '0.290', '0.642', '0.618', '0.497', '0.436', '0.362', '0.284', '0.278', '0.480', '0.304', '0.255', '0.608', '0.603', '0.388', '0.285', '0.464', '0.397', '0.689', '0.587', '0.577', '0.558', '0.343', '0.339', '0.303', '0.288', '0.561', '0.512', '0.436', '0.389', '0.673', '0.294', '0.272', '0.667', '0.444', '0.431', '0.411', '0.360', '0.312', '0.295', '0.272', '0.251', '0.704', '0.648', '0.646', '0.412', '0.516', '0.405', '0.401', '0.377', '0.541', '0.464', '0.587', '0.538', '0.537', '0.499', '0.398', '0.630', '0.452', '0.398', '0.321', '0.660', '0.321', '0.269']\n",
      "\n",
      "CleanNet:\n",
      "- Number of detections: 119\n",
      "- Average confidence: 0.745\n",
      "- Maximum confidence: 0.959\n",
      "- Minimum confidence: 0.265\n",
      "- All confidence scores: ['0.891', '0.878', '0.890', '0.850', '0.933', '0.856', '0.882', '0.646', '0.904', '0.879', '0.856', '0.930', '0.882', '0.915', '0.905', '0.912', '0.840', '0.868', '0.824', '0.770', '0.749', '0.772', '0.744', '0.870', '0.847', '0.657', '0.913', '0.845', '0.846', '0.824', '0.857', '0.790', '0.432', '0.389', '0.955', '0.760', '0.896', '0.702', '0.921', '0.907', '0.888', '0.651', '0.882', '0.881', '0.815', '0.890', '0.846', '0.873', '0.834', '0.708', '0.302', '0.293', '0.672', '0.447', '0.718', '0.367', '0.874', '0.802', '0.799', '0.819', '0.753', '0.608', '0.820', '0.792', '0.783', '0.430', '0.879', '0.851', '0.900', '0.793', '0.649', '0.607', '0.728', '0.711', '0.845', '0.656', '0.806', '0.668', '0.477', '0.366', '0.936', '0.864', '0.265', '0.882', '0.820', '0.804', '0.684', '0.713', '0.446', '0.797', '0.454', '0.358', '0.877', '0.761', '0.527', '0.875', '0.724', '0.853', '0.741', '0.810', '0.459', '0.935', '0.795', '0.838', '0.766', '0.813', '0.721', '0.644', '0.575', '0.388', '0.353', '0.352', '0.715', '0.672', '0.842', '0.459', '0.959', '0.891', '0.762']\n",
      "\n",
      "Classification Summary:\n",
      "Total images processed: 79\n",
      "Positive classifications: 25\n",
      "Negative classifications: 54\n",
      "\n",
      "Detailed results saved to: ./test/classification_results.json\n",
      "Confidence statistics saved to: ./test/confidence_statistics.json\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: 스크립트 실행\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
