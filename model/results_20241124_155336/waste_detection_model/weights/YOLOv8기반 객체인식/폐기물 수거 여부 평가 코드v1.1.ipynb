{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Large Waste Items:\n",
    "- Number of detections: 65\n",
    "- Average confidence: 0.427\n",
    "- Maximum confidence: 0.912\n",
    "- Minimum confidence: 0.252\n",
    "\n",
    "PP bag:\n",
    "- Number of detections: 44\n",
    "- Average confidence: 0.557\n",
    "- Maximum confidence: 0.943\n",
    "- Minimum confidence: 0.270\n",
    "\n",
    "General Waste:\n",
    "- Number of detections: 163\n",
    "- Average confidence: 0.426\n",
    "- Maximum confidence: 0.741\n",
    "- Minimum confidence: 0.250\n",
    "\n",
    "CleanNet:\n",
    "- Number of detections: 119\n",
    "- Average confidence: 0.745\n",
    "- Maximum confidence: 0.959\n",
    "- Minimum confidence: 0.265\n",
    "\n",
    "이걸 기준으로 평균보다 아래는 Red바운딩 박스 이상은 Green 바운딩 박스를 표출한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting object detection...\n",
      "Found 79 images in the directory.\n",
      "\n",
      "Processing image 1/79: -7-_jpg.rf.3e4e45f01df8da1ac999a94efb60473e.jpg\n",
      "\n",
      "image 1/1 E:\\livingLab\\YOLOv8 \\test\\images\\-7-_jpg.rf.3e4e45f01df8da1ac999a94efb60473e.jpg: 640x640 1 PP bag, 41.0ms\n",
      "Speed: 1.8ms preprocess, 41.0ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detected Classes in -7-_jpg.rf.3e4e45f01df8da1ac999a94efb60473e.jpg:\n",
      "- PP bag\n",
      "Processing times:\n",
      "- Preprocess: 1.8334388732910156ms\n",
      "- Inference: 40.97270965576172ms\n",
      "- Postprocess: 0.0ms\n",
      "\n",
      "Processing image 2/79: 202411061313-36-307488-127-352668_jpg.rf.2debbe814df92503a0079be859650cc0.jpg\n",
      "\n",
      "image 1/1 E:\\livingLab\\YOLOv8 \\test\\images\\202411061313-36-307488-127-352668_jpg.rf.2debbe814df92503a0079be859650cc0.jpg: 640x640 1 General Waste, 2 CleanNets, 35.8ms\n",
      "Speed: 4.0ms preprocess, 35.8ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detected Classes in 202411061313-36-307488-127-352668_jpg.rf.2debbe814df92503a0079be859650cc0.jpg:\n",
      "- CleanNet\n",
      "- CleanNet\n",
      "- General Waste\n",
      "Processing times:\n",
      "- Preprocess: 3.9894580841064453ms\n",
      "- Inference: 35.797834396362305ms\n",
      "- Postprocess: 0.9968280792236328ms\n",
      "\n",
      "Processing image 3/79: 202411061319-36-307974-127-355197_jpg.rf.40e66b47ea454f90ee055c740410cc1c.jpg\n",
      "\n",
      "image 1/1 E:\\livingLab\\YOLOv8 \\test\\images\\202411061319-36-307974-127-355197_jpg.rf.40e66b47ea454f90ee055c740410cc1c.jpg: 640x640 3 General Wastes, 2 CleanNets, 39.3ms\n",
      "Speed: 2.0ms preprocess, 39.3ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detected Classes in 202411061319-36-307974-127-355197_jpg.rf.40e66b47ea454f90ee055c740410cc1c.jpg:\n",
      "- CleanNet\n",
      "- CleanNet\n",
      "- General Waste\n",
      "- General Waste\n",
      "- General Waste\n",
      "Processing times:\n",
      "- Preprocess: 1.9979476928710938ms\n",
      "- Inference: 39.318084716796875ms\n",
      "- Postprocess: 0.9961128234863281ms\n",
      "\n",
      "Processing image 4/79: 202411061322-36-307550-127-354681_jpg.rf.e09f59bc4cc8915a9da9c47c91f306f0.jpg\n",
      "\n",
      "image 1/1 E:\\livingLab\\YOLOv8 \\test\\images\\202411061322-36-307550-127-354681_jpg.rf.e09f59bc4cc8915a9da9c47c91f306f0.jpg: 640x640 2 General Wastes, 2 CleanNets, 35.7ms\n",
      "Speed: 3.0ms preprocess, 35.7ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detected Classes in 202411061322-36-307550-127-354681_jpg.rf.e09f59bc4cc8915a9da9c47c91f306f0.jpg:\n",
      "- CleanNet\n",
      "- CleanNet\n",
      "- General Waste\n",
      "- General Waste\n",
      "Processing times:\n",
      "- Preprocess: 3.0488967895507812ms\n",
      "- Inference: 35.71319580078125ms\n",
      "- Postprocess: 0.0ms\n",
      "\n",
      "Processing image 5/79: 202411061322-36-320218-127-343319_jpg.rf.86814c1d60da676d5d9596a2a3da0efb.jpg\n",
      "\n",
      "image 1/1 E:\\livingLab\\YOLOv8 \\test\\images\\202411061322-36-320218-127-343319_jpg.rf.86814c1d60da676d5d9596a2a3da0efb.jpg: 640x640 1 Large Waste Items, 4 General Wastes, 1 CleanNet, 32.9ms\n",
      "Speed: 3.0ms preprocess, 32.9ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detected Classes in 202411061322-36-320218-127-343319_jpg.rf.86814c1d60da676d5d9596a2a3da0efb.jpg:\n",
      "- CleanNet\n",
      "- General Waste\n",
      "- General Waste\n",
      "- Large Waste Items\n",
      "- General Waste\n",
      "- General Waste\n",
      "Processing times:\n",
      "- Preprocess: 2.990245819091797ms\n",
      "- Inference: 32.86933898925781ms\n",
      "- Postprocess: 0.0ms\n",
      "\n",
      "Processing image 6/79: 202411061323-36-319426-127-343581-2_jpg.rf.5c85eeb865231f3fb9ac4fd355319416.jpg\n",
      "\n",
      "image 1/1 E:\\livingLab\\YOLOv8 \\test\\images\\202411061323-36-319426-127-343581-2_jpg.rf.5c85eeb865231f3fb9ac4fd355319416.jpg: 640x640 1 Large Waste Items, 1 General Waste, 1 CleanNet, 39.0ms\n",
      "Speed: 3.9ms preprocess, 39.0ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detected Classes in 202411061323-36-319426-127-343581-2_jpg.rf.5c85eeb865231f3fb9ac4fd355319416.jpg:\n",
      "- Large Waste Items\n",
      "- CleanNet\n",
      "- General Waste\n",
      "Processing times:\n",
      "- Preprocess: 3.9484500885009766ms\n",
      "- Inference: 39.02125358581543ms\n",
      "- Postprocess: 0.0ms\n",
      "\n",
      "Processing image 7/79: 202411061325-36-308399-127-354248_jpg.rf.fe8e74b6e0015e468fad270eb87ca499.jpg\n",
      "\n",
      "image 1/1 E:\\livingLab\\YOLOv8 \\test\\images\\202411061325-36-308399-127-354248_jpg.rf.fe8e74b6e0015e468fad270eb87ca499.jpg: 640x640 3 General Wastes, 2 CleanNets, 38.8ms\n",
      "Speed: 3.0ms preprocess, 38.8ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detected Classes in 202411061325-36-308399-127-354248_jpg.rf.fe8e74b6e0015e468fad270eb87ca499.jpg:\n",
      "- CleanNet\n",
      "- CleanNet\n",
      "- General Waste\n",
      "- General Waste\n",
      "- General Waste\n",
      "Processing times:\n",
      "- Preprocess: 3.0481815338134766ms\n",
      "- Inference: 38.831233978271484ms\n",
      "- Postprocess: 0.0ms\n",
      "\n",
      "Processing image 8/79: 202411061326-36-318897-127-343185_jpg.rf.d6707abf64c488ae79ddec26206a2f89.jpg\n",
      "\n",
      "image 1/1 E:\\livingLab\\YOLOv8 \\test\\images\\202411061326-36-318897-127-343185_jpg.rf.d6707abf64c488ae79ddec26206a2f89.jpg: 640x640 2 Large Waste Itemss, 2 General Wastes, 1 CleanNet, 31.9ms\n",
      "Speed: 2.0ms preprocess, 31.9ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detected Classes in 202411061326-36-318897-127-343185_jpg.rf.d6707abf64c488ae79ddec26206a2f89.jpg:\n",
      "- CleanNet\n",
      "- General Waste\n",
      "- General Waste\n",
      "- Large Waste Items\n",
      "- Large Waste Items\n",
      "Processing times:\n",
      "- Preprocess: 2.017974853515625ms\n",
      "- Inference: 31.86821937561035ms\n",
      "- Postprocess: 0.9965896606445312ms\n",
      "\n",
      "Processing image 9/79: 202411061328-36-318047-127-344154_jpg.rf.7aefec6f116650e0e611553b9811a6d8.jpg\n",
      "\n",
      "image 1/1 E:\\livingLab\\YOLOv8 \\test\\images\\202411061328-36-318047-127-344154_jpg.rf.7aefec6f116650e0e611553b9811a6d8.jpg: 640x640 2 General Wastes, 2 CleanNets, 37.9ms\n",
      "Speed: 3.0ms preprocess, 37.9ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detected Classes in 202411061328-36-318047-127-344154_jpg.rf.7aefec6f116650e0e611553b9811a6d8.jpg:\n",
      "- CleanNet\n",
      "- CleanNet\n",
      "- General Waste\n",
      "- General Waste\n",
      "Processing times:\n",
      "- Preprocess: 2.9892921447753906ms\n",
      "- Inference: 37.91546821594238ms\n",
      "- Postprocess: 1.0197162628173828ms\n",
      "\n",
      "Processing image 10/79: 202411061328-36-318467-127-342773_jpg.rf.8e7d885664cc4a093d1ef0afcc79b2e8.jpg\n",
      "\n",
      "image 1/1 E:\\livingLab\\YOLOv8 \\test\\images\\202411061328-36-318467-127-342773_jpg.rf.8e7d885664cc4a093d1ef0afcc79b2e8.jpg: 640x640 4 General Wastes, 2 CleanNets, 31.6ms\n",
      "Speed: 1.0ms preprocess, 31.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detected Classes in 202411061328-36-318467-127-342773_jpg.rf.8e7d885664cc4a093d1ef0afcc79b2e8.jpg:\n",
      "- CleanNet\n",
      "- CleanNet\n",
      "- General Waste\n",
      "- General Waste\n",
      "- General Waste\n",
      "- General Waste\n",
      "Processing times:\n",
      "- Preprocess: 0.9963512420654297ms\n",
      "- Inference: 31.62097930908203ms\n",
      "- Postprocess: 0.9982585906982422ms\n",
      "\n",
      "Processing image 11/79: 202411061329-36-309133-127-353018_jpg.rf.66723f2b0a1f776c240789b2605d7c6e.jpg\n",
      "\n",
      "image 1/1 E:\\livingLab\\YOLOv8 \\test\\images\\202411061329-36-309133-127-353018_jpg.rf.66723f2b0a1f776c240789b2605d7c6e.jpg: 640x640 2 General Wastes, 2 CleanNets, 30.0ms\n",
      "Speed: 1.0ms preprocess, 30.0ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detected Classes in 202411061329-36-309133-127-353018_jpg.rf.66723f2b0a1f776c240789b2605d7c6e.jpg:\n",
      "- CleanNet\n",
      "- CleanNet\n",
      "- General Waste\n",
      "- General Waste\n",
      "Processing times:\n",
      "- Preprocess: 0.9961128234863281ms\n",
      "- Inference: 29.952049255371094ms\n",
      "- Postprocess: 0.0ms\n",
      "\n",
      "Processing image 12/79: 202411061330-36-318792-127-344275_jpg.rf.3f74fb24878d9a28014f79a8ad00e4c4.jpg\n",
      "\n",
      "image 1/1 E:\\livingLab\\YOLOv8 \\test\\images\\202411061330-36-318792-127-344275_jpg.rf.3f74fb24878d9a28014f79a8ad00e4c4.jpg: 640x640 1 General Waste, 37.0ms\n",
      "Speed: 2.0ms preprocess, 37.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detected Classes in 202411061330-36-318792-127-344275_jpg.rf.3f74fb24878d9a28014f79a8ad00e4c4.jpg:\n",
      "- General Waste\n",
      "Processing times:\n",
      "- Preprocess: 1.9936561584472656ms\n",
      "- Inference: 36.998748779296875ms\n",
      "- Postprocess: 0.9965896606445312ms\n",
      "\n",
      "Processing image 13/79: 202411061337-36-309239-127-353916_jpg.rf.4c7a323affb98f997405ec20ca6d216e.jpg\n",
      "\n",
      "image 1/1 E:\\livingLab\\YOLOv8 \\test\\images\\202411061337-36-309239-127-353916_jpg.rf.4c7a323affb98f997405ec20ca6d216e.jpg: 640x640 1 Large Waste Items, 4 General Wastes, 2 CleanNets, 37.1ms\n",
      "Speed: 2.0ms preprocess, 37.1ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detected Classes in 202411061337-36-309239-127-353916_jpg.rf.4c7a323affb98f997405ec20ca6d216e.jpg:\n",
      "- CleanNet\n",
      "- CleanNet\n",
      "- Large Waste Items\n",
      "- General Waste\n",
      "- General Waste\n",
      "- General Waste\n",
      "- General Waste\n",
      "Processing times:\n",
      "- Preprocess: 1.9931793212890625ms\n",
      "- Inference: 37.10007667541504ms\n",
      "- Postprocess: 0.0ms\n",
      "\n",
      "Processing image 14/79: 202411061339-36-310790-127-352166_jpg.rf.78fd2988c35f72ab7c8c932764bb04c6.jpg\n",
      "\n",
      "image 1/1 E:\\livingLab\\YOLOv8 \\test\\images\\202411061339-36-310790-127-352166_jpg.rf.78fd2988c35f72ab7c8c932764bb04c6.jpg: 640x640 2 General Wastes, 2 CleanNets, 37.0ms\n",
      "Speed: 1.0ms preprocess, 37.0ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detected Classes in 202411061339-36-310790-127-352166_jpg.rf.78fd2988c35f72ab7c8c932764bb04c6.jpg:\n",
      "- CleanNet\n",
      "- CleanNet\n",
      "- General Waste\n",
      "- General Waste\n",
      "Processing times:\n",
      "- Preprocess: 0.9963512420654297ms\n",
      "- Inference: 36.95034980773926ms\n",
      "- Postprocess: 0.0ms\n",
      "\n",
      "Processing image 15/79: 202411061342-36-309176-127-351941_jpg.rf.05afb3c8e078242b96c4a730357296bb.jpg\n",
      "\n",
      "image 1/1 E:\\livingLab\\YOLOv8 \\test\\images\\202411061342-36-309176-127-351941_jpg.rf.05afb3c8e078242b96c4a730357296bb.jpg: 640x640 3 General Wastes, 2 CleanNets, 38.3ms\n",
      "Speed: 1.5ms preprocess, 38.3ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detected Classes in 202411061342-36-309176-127-351941_jpg.rf.05afb3c8e078242b96c4a730357296bb.jpg:\n",
      "- CleanNet\n",
      "- CleanNet\n",
      "- General Waste\n",
      "- General Waste\n",
      "- General Waste\n",
      "Processing times:\n",
      "- Preprocess: 1.5022754669189453ms\n",
      "- Inference: 38.29312324523926ms\n",
      "- Postprocess: 0.0ms\n",
      "\n",
      "Processing image 16/79: 202411061342-36-315640-127-349174_jpg.rf.5444f5989d8b4ed01d08d05553d63571.jpg\n",
      "\n",
      "image 1/1 E:\\livingLab\\YOLOv8 \\test\\images\\202411061342-36-315640-127-349174_jpg.rf.5444f5989d8b4ed01d08d05553d63571.jpg: 640x640 3 General Wastes, 2 CleanNets, 37.8ms\n",
      "Speed: 2.0ms preprocess, 37.8ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detected Classes in 202411061342-36-315640-127-349174_jpg.rf.5444f5989d8b4ed01d08d05553d63571.jpg:\n",
      "- CleanNet\n",
      "- CleanNet\n",
      "- General Waste\n",
      "- General Waste\n",
      "- General Waste\n",
      "Processing times:\n",
      "- Preprocess: 1.9936561584472656ms\n",
      "- Inference: 37.841796875ms\n",
      "- Postprocess: 0.0ms\n",
      "\n",
      "Processing image 17/79: 202411061343-36-315732-127-349908_jpg.rf.f32a738af7e8cb0569568da90e660036.jpg\n",
      "\n",
      "image 1/1 E:\\livingLab\\YOLOv8 \\test\\images\\202411061343-36-315732-127-349908_jpg.rf.f32a738af7e8cb0569568da90e660036.jpg: 640x640 4 General Wastes, 1 CleanNet, 32.6ms\n",
      "Speed: 3.0ms preprocess, 32.6ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detected Classes in 202411061343-36-315732-127-349908_jpg.rf.f32a738af7e8cb0569568da90e660036.jpg:\n",
      "- CleanNet\n",
      "- General Waste\n",
      "- General Waste\n",
      "- General Waste\n",
      "- General Waste\n",
      "Processing times:\n",
      "- Preprocess: 2.989530563354492ms\n",
      "- Inference: 32.59444236755371ms\n",
      "- Postprocess: 0.0ms\n",
      "\n",
      "Processing image 18/79: 202411061345-36-314850-127-350158_jpg.rf.a5c076f0199e673c36da4ac790ee940e.jpg\n",
      "\n",
      "image 1/1 E:\\livingLab\\YOLOv8 \\test\\images\\202411061345-36-314850-127-350158_jpg.rf.a5c076f0199e673c36da4ac790ee940e.jpg: 640x640 1 General Waste, 2 CleanNets, 32.8ms\n",
      "Speed: 3.0ms preprocess, 32.8ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detected Classes in 202411061345-36-314850-127-350158_jpg.rf.a5c076f0199e673c36da4ac790ee940e.jpg:\n",
      "- CleanNet\n",
      "- CleanNet\n",
      "- General Waste\n",
      "Processing times:\n",
      "- Preprocess: 2.990245819091797ms\n",
      "- Inference: 32.80997276306152ms\n",
      "- Postprocess: 0.0ms\n",
      "\n",
      "Processing image 19/79: 202411061346-36-308081-127-353130_jpg.rf.83770fab01c2a2e62838a2a2729231a7.jpg\n",
      "\n",
      "image 1/1 E:\\livingLab\\YOLOv8 \\test\\images\\202411061346-36-308081-127-353130_jpg.rf.83770fab01c2a2e62838a2a2729231a7.jpg: 640x640 4 General Wastes, 2 CleanNets, 32.6ms\n",
      "Speed: 1.0ms preprocess, 32.6ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detected Classes in 202411061346-36-308081-127-353130_jpg.rf.83770fab01c2a2e62838a2a2729231a7.jpg:\n",
      "- CleanNet\n",
      "- CleanNet\n",
      "- General Waste\n",
      "- General Waste\n",
      "- General Waste\n",
      "- General Waste\n",
      "Processing times:\n",
      "- Preprocess: 0.9968280792236328ms\n",
      "- Inference: 32.62972831726074ms\n",
      "- Postprocess: 0.0ms\n",
      "\n",
      "Processing image 20/79: 202411061346-36-311745-127-353092_jpg.rf.3e408eccde358b63c05fd5a29f956aab.jpg\n",
      "\n",
      "image 1/1 E:\\livingLab\\YOLOv8 \\test\\images\\202411061346-36-311745-127-353092_jpg.rf.3e408eccde358b63c05fd5a29f956aab.jpg: 640x640 3 General Wastes, 3 CleanNets, 31.1ms\n",
      "Speed: 2.0ms preprocess, 31.1ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detected Classes in 202411061346-36-311745-127-353092_jpg.rf.3e408eccde358b63c05fd5a29f956aab.jpg:\n",
      "- CleanNet\n",
      "- CleanNet\n",
      "- CleanNet\n",
      "- General Waste\n",
      "- General Waste\n",
      "- General Waste\n",
      "Processing times:\n",
      "- Preprocess: 1.993417739868164ms\n",
      "- Inference: 31.073570251464844ms\n",
      "- Postprocess: 0.0ms\n",
      "\n",
      "Processing image 21/79: 202411061346-36-314198-127-350451_jpg.rf.c77a12c283995fb4057bd0ca00b83161.jpg\n",
      "\n",
      "image 1/1 E:\\livingLab\\YOLOv8 \\test\\images\\202411061346-36-314198-127-350451_jpg.rf.c77a12c283995fb4057bd0ca00b83161.jpg: 640x640 3 Large Waste Itemss, 1 General Waste, 1 CleanNet, 35.9ms\n",
      "Speed: 1.8ms preprocess, 35.9ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detected Classes in 202411061346-36-314198-127-350451_jpg.rf.c77a12c283995fb4057bd0ca00b83161.jpg:\n",
      "- Large Waste Items\n",
      "- CleanNet\n",
      "- General Waste\n",
      "- Large Waste Items\n",
      "- Large Waste Items\n",
      "Processing times:\n",
      "- Preprocess: 1.8229484558105469ms\n",
      "- Inference: 35.88080406188965ms\n",
      "- Postprocess: 0.9970664978027344ms\n",
      "\n",
      "Processing image 22/79: 202411061348-36-310510-127-353380_jpg.rf.1b445c1bbde0cedac1df9b0f9c220dd4.jpg\n",
      "\n",
      "image 1/1 E:\\livingLab\\YOLOv8 \\test\\images\\202411061348-36-310510-127-353380_jpg.rf.1b445c1bbde0cedac1df9b0f9c220dd4.jpg: 640x640 3 General Wastes, 2 CleanNets, 36.8ms\n",
      "Speed: 3.0ms preprocess, 36.8ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detected Classes in 202411061348-36-310510-127-353380_jpg.rf.1b445c1bbde0cedac1df9b0f9c220dd4.jpg:\n",
      "- CleanNet\n",
      "- CleanNet\n",
      "- General Waste\n",
      "- General Waste\n",
      "- General Waste\n",
      "Processing times:\n",
      "- Preprocess: 2.9897689819335938ms\n",
      "- Inference: 36.76247596740723ms\n",
      "- Postprocess: 0.0ms\n",
      "\n",
      "Processing image 23/79: 202411061348-36-313711-127-349988_jpg.rf.3581e8d79a3a050686554a20476cdb34.jpg\n",
      "\n",
      "image 1/1 E:\\livingLab\\YOLOv8 \\test\\images\\202411061348-36-313711-127-349988_jpg.rf.3581e8d79a3a050686554a20476cdb34.jpg: 640x640 2 Large Waste Itemss, 2 General Wastes, 2 CleanNets, 36.9ms\n",
      "Speed: 3.0ms preprocess, 36.9ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detected Classes in 202411061348-36-313711-127-349988_jpg.rf.3581e8d79a3a050686554a20476cdb34.jpg:\n",
      "- CleanNet\n",
      "- CleanNet\n",
      "- Large Waste Items\n",
      "- General Waste\n",
      "- General Waste\n",
      "- Large Waste Items\n",
      "Processing times:\n",
      "- Preprocess: 2.9904842376708984ms\n",
      "- Inference: 36.89765930175781ms\n",
      "- Postprocess: 0.0ms\n",
      "\n",
      "Processing image 24/79: 202411061349-36-311745-127-353092_jpg.rf.fb5f4cbdb3036192e55d681050fe72da.jpg\n",
      "\n",
      "image 1/1 E:\\livingLab\\YOLOv8 \\test\\images\\202411061349-36-311745-127-353092_jpg.rf.fb5f4cbdb3036192e55d681050fe72da.jpg: 640x640 2 General Wastes, 2 CleanNets, 36.7ms\n",
      "Speed: 4.1ms preprocess, 36.7ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detected Classes in 202411061349-36-311745-127-353092_jpg.rf.fb5f4cbdb3036192e55d681050fe72da.jpg:\n",
      "- CleanNet\n",
      "- CleanNet\n",
      "- General Waste\n",
      "- General Waste\n",
      "Processing times:\n",
      "- Preprocess: 4.07719612121582ms\n",
      "- Inference: 36.681175231933594ms\n",
      "- Postprocess: 0.0ms\n",
      "\n",
      "Processing image 25/79: 202411061349-36-313040-127-350293_jpg.rf.52fab8e4c82156dcff635e98319420f9.jpg\n",
      "\n",
      "image 1/1 E:\\livingLab\\YOLOv8 \\test\\images\\202411061349-36-313040-127-350293_jpg.rf.52fab8e4c82156dcff635e98319420f9.jpg: 640x640 3 General Wastes, 2 CleanNets, 35.7ms\n",
      "Speed: 2.0ms preprocess, 35.7ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detected Classes in 202411061349-36-313040-127-350293_jpg.rf.52fab8e4c82156dcff635e98319420f9.jpg:\n",
      "- CleanNet\n",
      "- General Waste\n",
      "- CleanNet\n",
      "- General Waste\n",
      "- General Waste\n",
      "Processing times:\n",
      "- Preprocess: 1.9931793212890625ms\n",
      "- Inference: 35.67957878112793ms\n",
      "- Postprocess: 0.0ms\n",
      "\n",
      "Processing image 26/79: 202411061350-36-314129-127-351297_jpg.rf.7b7238965a8cae0683e4d2bac4f5a06a.jpg\n",
      "\n",
      "image 1/1 E:\\livingLab\\YOLOv8 \\test\\images\\202411061350-36-314129-127-351297_jpg.rf.7b7238965a8cae0683e4d2bac4f5a06a.jpg: 640x640 4 General Wastes, 3 CleanNets, 38.0ms\n",
      "Speed: 2.8ms preprocess, 38.0ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detected Classes in 202411061350-36-314129-127-351297_jpg.rf.7b7238965a8cae0683e4d2bac4f5a06a.jpg:\n",
      "- CleanNet\n",
      "- CleanNet\n",
      "- CleanNet\n",
      "- General Waste\n",
      "- General Waste\n",
      "- General Waste\n",
      "- General Waste\n",
      "Processing times:\n",
      "- Preprocess: 2.756357192993164ms\n",
      "- Inference: 37.973880767822266ms\n",
      "- Postprocess: 0.0ms\n",
      "\n",
      "Processing image 27/79: 202411061352-36-313215-127-351277-2_jpg.rf.f6aa801aed2a6e2e10462b781d4358bd.jpg\n",
      "\n",
      "image 1/1 E:\\livingLab\\YOLOv8 \\test\\images\\202411061352-36-313215-127-351277-2_jpg.rf.f6aa801aed2a6e2e10462b781d4358bd.jpg: 640x640 1 Large Waste Items, 1 PP bag, 35.7ms\n",
      "Speed: 3.1ms preprocess, 35.7ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detected Classes in 202411061352-36-313215-127-351277-2_jpg.rf.f6aa801aed2a6e2e10462b781d4358bd.jpg:\n",
      "- Large Waste Items\n",
      "- PP bag\n",
      "Processing times:\n",
      "- Preprocess: 3.0646324157714844ms\n",
      "- Inference: 35.73727607727051ms\n",
      "- Postprocess: 0.9970664978027344ms\n",
      "\n",
      "Processing image 28/79: 202411061352-36-313215-127-351277-3_jpg.rf.46cfc13747bebd1a1755da3b8a3cd8dd.jpg\n",
      "\n",
      "image 1/1 E:\\livingLab\\YOLOv8 \\test\\images\\202411061352-36-313215-127-351277-3_jpg.rf.46cfc13747bebd1a1755da3b8a3cd8dd.jpg: 640x640 1 Large Waste Items, 5 PP bags, 36.4ms\n",
      "Speed: 4.0ms preprocess, 36.4ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detected Classes in 202411061352-36-313215-127-351277-3_jpg.rf.46cfc13747bebd1a1755da3b8a3cd8dd.jpg:\n",
      "- PP bag\n",
      "- PP bag\n",
      "- Large Waste Items\n",
      "- PP bag\n",
      "- PP bag\n",
      "- PP bag\n",
      "Processing times:\n",
      "- Preprocess: 3.9882659912109375ms\n",
      "- Inference: 36.41462326049805ms\n",
      "- Postprocess: 0.0ms\n",
      "\n",
      "Processing image 29/79: 202411061358-36-308465-127-354548_jpg.rf.965f2f2b47de25bdc9d795cb71576543.jpg\n",
      "\n",
      "image 1/1 E:\\livingLab\\YOLOv8 \\test\\images\\202411061358-36-308465-127-354548_jpg.rf.965f2f2b47de25bdc9d795cb71576543.jpg: 640x640 4 General Wastes, 2 CleanNets, 33.7ms\n",
      "Speed: 2.0ms preprocess, 33.7ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detected Classes in 202411061358-36-308465-127-354548_jpg.rf.965f2f2b47de25bdc9d795cb71576543.jpg:\n",
      "- CleanNet\n",
      "- CleanNet\n",
      "- General Waste\n",
      "- General Waste\n",
      "- General Waste\n",
      "- General Waste\n",
      "Processing times:\n",
      "- Preprocess: 1.9931793212890625ms\n",
      "- Inference: 33.709049224853516ms\n",
      "- Postprocess: 0.9965896606445312ms\n",
      "\n",
      "Processing image 30/79: 202411061359-36-310957-127-353130_jpg.rf.906b474010a7ba8fcea77eeef01433cd.jpg\n",
      "\n",
      "image 1/1 E:\\livingLab\\YOLOv8 \\test\\images\\202411061359-36-310957-127-353130_jpg.rf.906b474010a7ba8fcea77eeef01433cd.jpg: 640x640 3 General Wastes, 2 CleanNets, 31.9ms\n",
      "Speed: 1.0ms preprocess, 31.9ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detected Classes in 202411061359-36-310957-127-353130_jpg.rf.906b474010a7ba8fcea77eeef01433cd.jpg:\n",
      "- CleanNet\n",
      "- CleanNet\n",
      "- General Waste\n",
      "- General Waste\n",
      "- General Waste\n",
      "Processing times:\n",
      "- Preprocess: 0.9963512420654297ms\n",
      "- Inference: 31.88800811767578ms\n",
      "- Postprocess: 0.0ms\n",
      "\n",
      "Processing image 31/79: 202411061415-36-316636-127-347495_jpg.rf.79ef2fb18c5b765ead9711ec6516fcea.jpg\n",
      "\n",
      "image 1/1 E:\\livingLab\\YOLOv8 \\test\\images\\202411061415-36-316636-127-347495_jpg.rf.79ef2fb18c5b765ead9711ec6516fcea.jpg: 640x640 3 Large Waste Itemss, 29.9ms\n",
      "Speed: 2.0ms preprocess, 29.9ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detected Classes in 202411061415-36-316636-127-347495_jpg.rf.79ef2fb18c5b765ead9711ec6516fcea.jpg:\n",
      "- Large Waste Items\n",
      "- Large Waste Items\n",
      "- Large Waste Items\n",
      "Processing times:\n",
      "- Preprocess: 1.992940902709961ms\n",
      "- Inference: 29.949426651000977ms\n",
      "- Postprocess: 1.0027885437011719ms\n",
      "\n",
      "Processing image 32/79: 202411061418-36-315697-127-346125_jpg.rf.e9aab1e2970ae7ad8efb2ab0e3dd7b81.jpg\n",
      "\n",
      "image 1/1 E:\\livingLab\\YOLOv8 \\test\\images\\202411061418-36-315697-127-346125_jpg.rf.e9aab1e2970ae7ad8efb2ab0e3dd7b81.jpg: 640x640 5 Large Waste Itemss, 32.7ms\n",
      "Speed: 3.0ms preprocess, 32.7ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detected Classes in 202411061418-36-315697-127-346125_jpg.rf.e9aab1e2970ae7ad8efb2ab0e3dd7b81.jpg:\n",
      "- Large Waste Items\n",
      "- Large Waste Items\n",
      "- Large Waste Items\n",
      "- Large Waste Items\n",
      "- Large Waste Items\n",
      "Processing times:\n",
      "- Preprocess: 2.9897689819335938ms\n",
      "- Inference: 32.739877700805664ms\n",
      "- Postprocess: 0.0ms\n",
      "\n",
      "Processing image 33/79: 202411061420-36-315055-127-346190_jpg.rf.f735fd248bc6690f4c75068de0897d1f.jpg\n",
      "\n",
      "image 1/1 E:\\livingLab\\YOLOv8 \\test\\images\\202411061420-36-315055-127-346190_jpg.rf.f735fd248bc6690f4c75068de0897d1f.jpg: 640x640 4 Large Waste Itemss, 36.0ms\n",
      "Speed: 3.0ms preprocess, 36.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detected Classes in 202411061420-36-315055-127-346190_jpg.rf.f735fd248bc6690f4c75068de0897d1f.jpg:\n",
      "- Large Waste Items\n",
      "- Large Waste Items\n",
      "- Large Waste Items\n",
      "- Large Waste Items\n",
      "Processing times:\n",
      "- Preprocess: 3.0181407928466797ms\n",
      "- Inference: 35.98904609680176ms\n",
      "- Postprocess: 0.9963512420654297ms\n",
      "\n",
      "Processing image 34/79: 202411061423-36-313839-127-348579_jpg.rf.4c6e29fd453c998c1710c6dd014daacf.jpg\n",
      "\n",
      "image 1/1 E:\\livingLab\\YOLOv8 \\test\\images\\202411061423-36-313839-127-348579_jpg.rf.4c6e29fd453c998c1710c6dd014daacf.jpg: 640x640 2 Large Waste Itemss, 5 General Wastes, 35.8ms\n",
      "Speed: 3.0ms preprocess, 35.8ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detected Classes in 202411061423-36-313839-127-348579_jpg.rf.4c6e29fd453c998c1710c6dd014daacf.jpg:\n",
      "- General Waste\n",
      "- Large Waste Items\n",
      "- General Waste\n",
      "- General Waste\n",
      "- Large Waste Items\n",
      "- General Waste\n",
      "- General Waste\n",
      "Processing times:\n",
      "- Preprocess: 2.989530563354492ms\n",
      "- Inference: 35.834312438964844ms\n",
      "- Postprocess: 0.0ms\n",
      "\n",
      "Processing image 35/79: 202411061423-36-314851-127-347123_jpg.rf.6b1796d5ce76d550ec8c684a8c23fafd.jpg\n",
      "\n",
      "image 1/1 E:\\livingLab\\YOLOv8 \\test\\images\\202411061423-36-314851-127-347123_jpg.rf.6b1796d5ce76d550ec8c684a8c23fafd.jpg: 640x640 4 Large Waste Itemss, 4 PP bags, 1 General Waste, 37.6ms\n",
      "Speed: 3.0ms preprocess, 37.6ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detected Classes in 202411061423-36-314851-127-347123_jpg.rf.6b1796d5ce76d550ec8c684a8c23fafd.jpg:\n",
      "- PP bag\n",
      "- Large Waste Items\n",
      "- PP bag\n",
      "- Large Waste Items\n",
      "- General Waste\n",
      "- Large Waste Items\n",
      "- PP bag\n",
      "- PP bag\n",
      "- Large Waste Items\n",
      "Processing times:\n",
      "- Preprocess: 2.9897689819335938ms\n",
      "- Inference: 37.57214546203613ms\n",
      "- Postprocess: 0.0ms\n",
      "\n",
      "Processing image 36/79: 202411061423-36-315046-127-346080_jpg.rf.bdc3f041759c2e28c0f15e1397a3584d.jpg\n",
      "\n",
      "image 1/1 E:\\livingLab\\YOLOv8 \\test\\images\\202411061423-36-315046-127-346080_jpg.rf.bdc3f041759c2e28c0f15e1397a3584d.jpg: 640x640 2 Large Waste Itemss, 39.0ms\n",
      "Speed: 1.0ms preprocess, 39.0ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detected Classes in 202411061423-36-315046-127-346080_jpg.rf.bdc3f041759c2e28c0f15e1397a3584d.jpg:\n",
      "- Large Waste Items\n",
      "- Large Waste Items\n",
      "Processing times:\n",
      "- Preprocess: 0.9984970092773438ms\n",
      "- Inference: 38.98930549621582ms\n",
      "- Postprocess: 0.0ms\n",
      "\n",
      "Processing image 37/79: 202411061431-36-309908-127-348185_jpg.rf.23fe7b1ddec31535d885e6cc9288d2da.jpg\n",
      "\n",
      "image 1/1 E:\\livingLab\\YOLOv8 \\test\\images\\202411061431-36-309908-127-348185_jpg.rf.23fe7b1ddec31535d885e6cc9288d2da.jpg: 640x640 6 Large Waste Itemss, 36.8ms\n",
      "Speed: 3.0ms preprocess, 36.8ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detected Classes in 202411061431-36-309908-127-348185_jpg.rf.23fe7b1ddec31535d885e6cc9288d2da.jpg:\n",
      "- Large Waste Items\n",
      "- Large Waste Items\n",
      "- Large Waste Items\n",
      "- Large Waste Items\n",
      "- Large Waste Items\n",
      "- Large Waste Items\n",
      "Processing times:\n",
      "- Preprocess: 2.9892921447753906ms\n",
      "- Inference: 36.806344985961914ms\n",
      "- Postprocess: 1.0225772857666016ms\n",
      "\n",
      "Processing image 38/79: 202411061431-36-311574-127-349342_jpg.rf.5182513e42f5000cf9af2f91816f2114.jpg\n",
      "\n",
      "image 1/1 E:\\livingLab\\YOLOv8 \\test\\images\\202411061431-36-311574-127-349342_jpg.rf.5182513e42f5000cf9af2f91816f2114.jpg: 640x640 2 Large Waste Itemss, 2 General Wastes, 36.8ms\n",
      "Speed: 3.0ms preprocess, 36.8ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detected Classes in 202411061431-36-311574-127-349342_jpg.rf.5182513e42f5000cf9af2f91816f2114.jpg:\n",
      "- General Waste\n",
      "- General Waste\n",
      "- Large Waste Items\n",
      "- Large Waste Items\n",
      "Processing times:\n",
      "- Preprocess: 2.9892921447753906ms\n",
      "- Inference: 36.757707595825195ms\n",
      "- Postprocess: 0.0ms\n",
      "\n",
      "Processing image 39/79: 202411061433-36-309089-127-349308_jpg.rf.680978375d34a5ce6bd988b65213fe68.jpg\n",
      "\n",
      "image 1/1 E:\\livingLab\\YOLOv8 \\test\\images\\202411061433-36-309089-127-349308_jpg.rf.680978375d34a5ce6bd988b65213fe68.jpg: 640x640 10 Large Waste Itemss, 1 General Waste, 37.8ms\n",
      "Speed: 3.0ms preprocess, 37.8ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detected Classes in 202411061433-36-309089-127-349308_jpg.rf.680978375d34a5ce6bd988b65213fe68.jpg:\n",
      "- Large Waste Items\n",
      "- Large Waste Items\n",
      "- Large Waste Items\n",
      "- Large Waste Items\n",
      "- Large Waste Items\n",
      "- Large Waste Items\n",
      "- Large Waste Items\n",
      "- Large Waste Items\n",
      "- General Waste\n",
      "- Large Waste Items\n",
      "- Large Waste Items\n",
      "Processing times:\n",
      "- Preprocess: 2.989053726196289ms\n",
      "- Inference: 37.78815269470215ms\n",
      "- Postprocess: 1.0285377502441406ms\n",
      "\n",
      "Processing image 40/79: 202411061434-36-309502-127-348344_jpg.rf.18196b3699941fd051eb67f4246945bb.jpg\n",
      "\n",
      "image 1/1 E:\\livingLab\\YOLOv8 \\test\\images\\202411061434-36-309502-127-348344_jpg.rf.18196b3699941fd051eb67f4246945bb.jpg: 640x640 2 General Wastes, 37.7ms\n",
      "Speed: 3.0ms preprocess, 37.7ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detected Classes in 202411061434-36-309502-127-348344_jpg.rf.18196b3699941fd051eb67f4246945bb.jpg:\n",
      "- General Waste\n",
      "- General Waste\n",
      "Processing times:\n",
      "- Preprocess: 2.9897689819335938ms\n",
      "- Inference: 37.670135498046875ms\n",
      "- Postprocess: 0.0ms\n",
      "\n",
      "Processing image 41/79: 202411061438-36-309254-127-350078_jpg.rf.63980ab462c1a491c72131a321aa9bc4.jpg\n",
      "\n",
      "image 1/1 E:\\livingLab\\YOLOv8 \\test\\images\\202411061438-36-309254-127-350078_jpg.rf.63980ab462c1a491c72131a321aa9bc4.jpg: 640x640 1 Large Waste Items, 1 CleanNet, 29.9ms\n",
      "Speed: 2.0ms preprocess, 29.9ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detected Classes in 202411061438-36-309254-127-350078_jpg.rf.63980ab462c1a491c72131a321aa9bc4.jpg:\n",
      "- CleanNet\n",
      "- Large Waste Items\n",
      "Processing times:\n",
      "- Preprocess: 1.9931793212890625ms\n",
      "- Inference: 29.87217903137207ms\n",
      "- Postprocess: 0.0ms\n",
      "\n",
      "Processing image 42/79: 202411081236-36-423913-127-407165-1_jpg.rf.0dfb1f31bd1bafebca3f8424e98cdc61.jpg\n",
      "\n",
      "image 1/1 E:\\livingLab\\YOLOv8 \\test\\images\\202411081236-36-423913-127-407165-1_jpg.rf.0dfb1f31bd1bafebca3f8424e98cdc61.jpg: 640x640 3 Large Waste Itemss, 2 CleanNets, 28.8ms\n",
      "Speed: 2.0ms preprocess, 28.8ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detected Classes in 202411081236-36-423913-127-407165-1_jpg.rf.0dfb1f31bd1bafebca3f8424e98cdc61.jpg:\n",
      "- Large Waste Items\n",
      "- Large Waste Items\n",
      "- CleanNet\n",
      "- CleanNet\n",
      "- Large Waste Items\n",
      "Processing times:\n",
      "- Preprocess: 2.010345458984375ms\n",
      "- Inference: 28.811931610107422ms\n",
      "- Postprocess: 0.9968280792236328ms\n",
      "\n",
      "Processing image 43/79: 202411081236-36-423913-127-407165-2_jpg.rf.f6e2a098f84601afac7d2350a475d4cc.jpg\n",
      "\n",
      "image 1/1 E:\\livingLab\\YOLOv8 \\test\\images\\202411081236-36-423913-127-407165-2_jpg.rf.f6e2a098f84601afac7d2350a475d4cc.jpg: 640x640 22 PP bags, 30.6ms\n",
      "Speed: 2.8ms preprocess, 30.6ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detected Classes in 202411081236-36-423913-127-407165-2_jpg.rf.f6e2a098f84601afac7d2350a475d4cc.jpg:\n",
      "- PP bag\n",
      "- PP bag\n",
      "- PP bag\n",
      "- PP bag\n",
      "- PP bag\n",
      "- PP bag\n",
      "- PP bag\n",
      "- PP bag\n",
      "- PP bag\n",
      "- PP bag\n",
      "- PP bag\n",
      "- PP bag\n",
      "- PP bag\n",
      "- PP bag\n",
      "- PP bag\n",
      "- PP bag\n",
      "- PP bag\n",
      "- PP bag\n",
      "- PP bag\n",
      "- PP bag\n",
      "- PP bag\n",
      "- PP bag\n",
      "Processing times:\n",
      "- Preprocess: 2.8095245361328125ms\n",
      "- Inference: 30.56502342224121ms\n",
      "- Postprocess: 0.0ms\n",
      "\n",
      "Processing image 44/79: 202411111136-36-331616-127-335799_jpg.rf.896aa812873491b43e52dfbd3261d149.jpg\n",
      "\n",
      "image 1/1 E:\\livingLab\\YOLOv8 \\test\\images\\202411111136-36-331616-127-335799_jpg.rf.896aa812873491b43e52dfbd3261d149.jpg: 640x640 1 PP bag, 2 General Wastes, 1 CleanNet, 35.7ms\n",
      "Speed: 3.8ms preprocess, 35.7ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detected Classes in 202411111136-36-331616-127-335799_jpg.rf.896aa812873491b43e52dfbd3261d149.jpg:\n",
      "- CleanNet\n",
      "- General Waste\n",
      "- PP bag\n",
      "- General Waste\n",
      "Processing times:\n",
      "- Preprocess: 3.8132667541503906ms\n",
      "- Inference: 35.715341567993164ms\n",
      "- Postprocess: 0.9963512420654297ms\n",
      "\n",
      "Processing image 45/79: 202411111137-36-331383-127-335375_jpg.rf.a50332a13cae1728d2c5dc6162c5b05d.jpg\n",
      "\n",
      "image 1/1 E:\\livingLab\\YOLOv8 \\test\\images\\202411111137-36-331383-127-335375_jpg.rf.a50332a13cae1728d2c5dc6162c5b05d.jpg: 640x640 1 PP bag, 1 CleanNet, 29.6ms\n",
      "Speed: 3.0ms preprocess, 29.6ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detected Classes in 202411111137-36-331383-127-335375_jpg.rf.a50332a13cae1728d2c5dc6162c5b05d.jpg:\n",
      "- CleanNet\n",
      "- PP bag\n",
      "Processing times:\n",
      "- Preprocess: 3.0121803283691406ms\n",
      "- Inference: 29.646635055541992ms\n",
      "- Postprocess: 0.0ms\n",
      "\n",
      "Processing image 46/79: 202411111139-36-331111-127-337848_jpg.rf.cbb982f1d1e160b11ae586add407ded9.jpg\n",
      "\n",
      "image 1/1 E:\\livingLab\\YOLOv8 \\test\\images\\202411111139-36-331111-127-337848_jpg.rf.cbb982f1d1e160b11ae586add407ded9.jpg: 640x640 2 Large Waste Itemss, 2 General Wastes, 2 CleanNets, 17.6ms\n",
      "Speed: 0.3ms preprocess, 17.6ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detected Classes in 202411111139-36-331111-127-337848_jpg.rf.cbb982f1d1e160b11ae586add407ded9.jpg:\n",
      "- CleanNet\n",
      "- Large Waste Items\n",
      "- Large Waste Items\n",
      "- General Waste\n",
      "- CleanNet\n",
      "- General Waste\n",
      "Processing times:\n",
      "- Preprocess: 0.2644062042236328ms\n",
      "- Inference: 17.60244369506836ms\n",
      "- Postprocess: 0.0ms\n",
      "\n",
      "Processing image 47/79: 202411111148-36-330942-127-338586_jpg.rf.09088434bf25f50d833296c7059228a4.jpg\n",
      "\n",
      "image 1/1 E:\\livingLab\\YOLOv8 \\test\\images\\202411111148-36-330942-127-338586_jpg.rf.09088434bf25f50d833296c7059228a4.jpg: 640x640 3 CleanNets, 18.6ms\n",
      "Speed: 0.8ms preprocess, 18.6ms inference, 13.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detected Classes in 202411111148-36-330942-127-338586_jpg.rf.09088434bf25f50d833296c7059228a4.jpg:\n",
      "- CleanNet\n",
      "- CleanNet\n",
      "- CleanNet\n",
      "Processing times:\n",
      "- Preprocess: 0.7574558258056641ms\n",
      "- Inference: 18.61739158630371ms\n",
      "- Postprocess: 13.50259780883789ms\n",
      "\n",
      "Processing image 48/79: 202411111150-36-331699-127-339169-1_jpg.rf.2dfbaba491bab23e08d12300bdec0a40.jpg\n",
      "\n",
      "image 1/1 E:\\livingLab\\YOLOv8 \\test\\images\\202411111150-36-331699-127-339169-1_jpg.rf.2dfbaba491bab23e08d12300bdec0a40.jpg: 640x640 2 Large Waste Itemss, 3 General Wastes, 3 CleanNets, 27.7ms\n",
      "Speed: 1.6ms preprocess, 27.7ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detected Classes in 202411111150-36-331699-127-339169-1_jpg.rf.2dfbaba491bab23e08d12300bdec0a40.jpg:\n",
      "- CleanNet\n",
      "- CleanNet\n",
      "- CleanNet\n",
      "- General Waste\n",
      "- General Waste\n",
      "- General Waste\n",
      "- Large Waste Items\n",
      "- Large Waste Items\n",
      "Processing times:\n",
      "- Preprocess: 1.5621185302734375ms\n",
      "- Inference: 27.68731117248535ms\n",
      "- Postprocess: 0.0ms\n",
      "\n",
      "Processing image 49/79: 202411111150-36-331699-127-339169-2_jpg.rf.30de6ea0a36f7a5dff4c2d5633c75ff7.jpg\n",
      "\n",
      "image 1/1 E:\\livingLab\\YOLOv8 \\test\\images\\202411111150-36-331699-127-339169-2_jpg.rf.30de6ea0a36f7a5dff4c2d5633c75ff7.jpg: 640x640 3 Large Waste Itemss, 29.7ms\n",
      "Speed: 1.0ms preprocess, 29.7ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detected Classes in 202411111150-36-331699-127-339169-2_jpg.rf.30de6ea0a36f7a5dff4c2d5633c75ff7.jpg:\n",
      "- Large Waste Items\n",
      "- Large Waste Items\n",
      "- Large Waste Items\n",
      "Processing times:\n",
      "- Preprocess: 0.9961128234863281ms\n",
      "- Inference: 29.742956161499023ms\n",
      "- Postprocess: 0.0ms\n",
      "\n",
      "Processing image 50/79: 202411111153-36-36-331686-127-340556_jpg.rf.f610735c1c46cf5328fb96c544f43dd2.jpg\n",
      "\n",
      "image 1/1 E:\\livingLab\\YOLOv8 \\test\\images\\202411111153-36-36-331686-127-340556_jpg.rf.f610735c1c46cf5328fb96c544f43dd2.jpg: 640x640 2 CleanNets, 31.4ms\n",
      "Speed: 0.2ms preprocess, 31.4ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detected Classes in 202411111153-36-36-331686-127-340556_jpg.rf.f610735c1c46cf5328fb96c544f43dd2.jpg:\n",
      "- CleanNet\n",
      "- CleanNet\n",
      "Processing times:\n",
      "- Preprocess: 0.23508071899414062ms\n",
      "- Inference: 31.40544891357422ms\n",
      "- Postprocess: 0.0ms\n",
      "\n",
      "Processing image 51/79: 202411111156-36-331079-127-341792_jpg.rf.fe0c796d57c7083f244e29ed95c6585c.jpg\n",
      "\n",
      "image 1/1 E:\\livingLab\\YOLOv8 \\test\\images\\202411111156-36-331079-127-341792_jpg.rf.fe0c796d57c7083f244e29ed95c6585c.jpg: 640x640 6 General Wastes, 2 CleanNets, 35.0ms\n",
      "Speed: 0.6ms preprocess, 35.0ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detected Classes in 202411111156-36-331079-127-341792_jpg.rf.fe0c796d57c7083f244e29ed95c6585c.jpg:\n",
      "- CleanNet\n",
      "- General Waste\n",
      "- General Waste\n",
      "- CleanNet\n",
      "- General Waste\n",
      "- General Waste\n",
      "- General Waste\n",
      "- General Waste\n",
      "Processing times:\n",
      "- Preprocess: 0.5826950073242188ms\n",
      "- Inference: 34.967899322509766ms\n",
      "- Postprocess: 0.0ms\n",
      "\n",
      "Processing image 52/79: 202411111206-36-311745-127-353092_jpg.rf.175848f0b77ba404e4e6f11f44f8c590.jpg\n",
      "\n",
      "image 1/1 E:\\livingLab\\YOLOv8 \\test\\images\\202411111206-36-311745-127-353092_jpg.rf.175848f0b77ba404e4e6f11f44f8c590.jpg: 640x640 1 General Waste, 2 CleanNets, 31.9ms\n",
      "Speed: 0.0ms preprocess, 31.9ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detected Classes in 202411111206-36-311745-127-353092_jpg.rf.175848f0b77ba404e4e6f11f44f8c590.jpg:\n",
      "- CleanNet\n",
      "- CleanNet\n",
      "- General Waste\n",
      "Processing times:\n",
      "- Preprocess: 0.0ms\n",
      "- Inference: 31.87251091003418ms\n",
      "- Postprocess: 0.0ms\n",
      "\n",
      "Processing image 53/79: 202411111207-36-310510-127-353380_jpg.rf.816d4f9ca63a539abb4e7bd0bfafc0e0.jpg\n",
      "\n",
      "image 1/1 E:\\livingLab\\YOLOv8 \\test\\images\\202411111207-36-310510-127-353380_jpg.rf.816d4f9ca63a539abb4e7bd0bfafc0e0.jpg: 640x640 3 General Wastes, 2 CleanNets, 34.9ms\n",
      "Speed: 0.3ms preprocess, 34.9ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detected Classes in 202411111207-36-310510-127-353380_jpg.rf.816d4f9ca63a539abb4e7bd0bfafc0e0.jpg:\n",
      "- CleanNet\n",
      "- CleanNet\n",
      "- General Waste\n",
      "- General Waste\n",
      "- General Waste\n",
      "Processing times:\n",
      "- Preprocess: 0.2949237823486328ms\n",
      "- Inference: 34.89089012145996ms\n",
      "- Postprocess: 0.0ms\n",
      "\n",
      "Processing image 54/79: 202411111210-36-310957-127-353130_jpg.rf.693876733d993c585b16039273a00299.jpg\n",
      "\n",
      "image 1/1 E:\\livingLab\\YOLOv8 \\test\\images\\202411111210-36-310957-127-353130_jpg.rf.693876733d993c585b16039273a00299.jpg: 640x640 2 General Wastes, 2 CleanNets, 31.4ms\n",
      "Speed: 0.1ms preprocess, 31.4ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detected Classes in 202411111210-36-310957-127-353130_jpg.rf.693876733d993c585b16039273a00299.jpg:\n",
      "- CleanNet\n",
      "- CleanNet\n",
      "- General Waste\n",
      "- General Waste\n",
      "Processing times:\n",
      "- Preprocess: 0.14448165893554688ms\n",
      "- Inference: 31.441926956176758ms\n",
      "- Postprocess: 0.0ms\n",
      "\n",
      "Processing image 55/79: 202411111212-36-310790-127-352166_jpg.rf.d4bb484b862d3105c60cdbe0712b5651.jpg\n",
      "\n",
      "image 1/1 E:\\livingLab\\YOLOv8 \\test\\images\\202411111212-36-310790-127-352166_jpg.rf.d4bb484b862d3105c60cdbe0712b5651.jpg: 640x640 7 General Wastes, 2 CleanNets, 34.8ms\n",
      "Speed: 0.6ms preprocess, 34.8ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detected Classes in 202411111212-36-310790-127-352166_jpg.rf.d4bb484b862d3105c60cdbe0712b5651.jpg:\n",
      "- CleanNet\n",
      "- CleanNet\n",
      "- General Waste\n",
      "- General Waste\n",
      "- General Waste\n",
      "- General Waste\n",
      "- General Waste\n",
      "- General Waste\n",
      "- General Waste\n",
      "Processing times:\n",
      "- Preprocess: 0.6344318389892578ms\n",
      "- Inference: 34.76452827453613ms\n",
      "- Postprocess: 0.0ms\n",
      "\n",
      "Processing image 56/79: 202411111213-36-309176-127-351941_jpg.rf.ddc9885bd847ea5b84dc27ccf6fd9fd9.jpg\n",
      "\n",
      "image 1/1 E:\\livingLab\\YOLOv8 \\test\\images\\202411111213-36-309176-127-351941_jpg.rf.ddc9885bd847ea5b84dc27ccf6fd9fd9.jpg: 640x640 2 CleanNets, 30.7ms\n",
      "Speed: 3.0ms preprocess, 30.7ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detected Classes in 202411111213-36-309176-127-351941_jpg.rf.ddc9885bd847ea5b84dc27ccf6fd9fd9.jpg:\n",
      "- CleanNet\n",
      "- CleanNet\n",
      "Processing times:\n",
      "- Preprocess: 3.0257701873779297ms\n",
      "- Inference: 30.724763870239258ms\n",
      "- Postprocess: 0.0ms\n",
      "\n",
      "Processing image 57/79: 202411111215-36-309133-127-353018_jpg.rf.d74fb20ba076ca562ab4b9922841c5e4.jpg\n",
      "\n",
      "image 1/1 E:\\livingLab\\YOLOv8 \\test\\images\\202411111215-36-309133-127-353018_jpg.rf.d74fb20ba076ca562ab4b9922841c5e4.jpg: 640x640 1 General Waste, 4 CleanNets, 27.8ms\n",
      "Speed: 2.0ms preprocess, 27.8ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detected Classes in 202411111215-36-309133-127-353018_jpg.rf.d74fb20ba076ca562ab4b9922841c5e4.jpg:\n",
      "- CleanNet\n",
      "- CleanNet\n",
      "- General Waste\n",
      "- CleanNet\n",
      "- CleanNet\n",
      "Processing times:\n",
      "- Preprocess: 2.010822296142578ms\n",
      "- Inference: 27.83799171447754ms\n",
      "- Postprocess: 0.0ms\n",
      "\n",
      "Processing image 58/79: 202411111216-36-309841-127-353640_jpg.rf.0cfe34d4e561604f27e4d6d51709d093.jpg\n",
      "\n",
      "image 1/1 E:\\livingLab\\YOLOv8 \\test\\images\\202411111216-36-309841-127-353640_jpg.rf.0cfe34d4e561604f27e4d6d51709d093.jpg: 640x640 2 General Wastes, 3 CleanNets, 25.8ms\n",
      "Speed: 5.1ms preprocess, 25.8ms inference, 15.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detected Classes in 202411111216-36-309841-127-353640_jpg.rf.0cfe34d4e561604f27e4d6d51709d093.jpg:\n",
      "- CleanNet\n",
      "- CleanNet\n",
      "- General Waste\n",
      "- CleanNet\n",
      "- General Waste\n",
      "Processing times:\n",
      "- Preprocess: 5.106449127197266ms\n",
      "- Inference: 25.81477165222168ms\n",
      "- Postprocess: 15.623331069946289ms\n",
      "\n",
      "Processing image 59/79: 202411111217-36-309239-127-353916_jpg.rf.45de2cd861119d382d4752616a1c7e31.jpg\n",
      "\n",
      "image 1/1 E:\\livingLab\\YOLOv8 \\test\\images\\202411111217-36-309239-127-353916_jpg.rf.45de2cd861119d382d4752616a1c7e31.jpg: 640x640 4 General Wastes, 2 CleanNets, 28.6ms\n",
      "Speed: 1.8ms preprocess, 28.6ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detected Classes in 202411111217-36-309239-127-353916_jpg.rf.45de2cd861119d382d4752616a1c7e31.jpg:\n",
      "- CleanNet\n",
      "- CleanNet\n",
      "- General Waste\n",
      "- General Waste\n",
      "- General Waste\n",
      "- General Waste\n",
      "Processing times:\n",
      "- Preprocess: 1.8100738525390625ms\n",
      "- Inference: 28.6252498626709ms\n",
      "- Postprocess: 0.0ms\n",
      "\n",
      "Processing image 60/79: 202411111220-36-308081-127-353130_jpg.rf.d482c4c6079ceee93fea151e3af3092a.jpg\n",
      "\n",
      "image 1/1 E:\\livingLab\\YOLOv8 \\test\\images\\202411111220-36-308081-127-353130_jpg.rf.d482c4c6079ceee93fea151e3af3092a.jpg: 640x640 2 General Wastes, 2 CleanNets, 38.9ms\n",
      "Speed: 0.1ms preprocess, 38.9ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detected Classes in 202411111220-36-308081-127-353130_jpg.rf.d482c4c6079ceee93fea151e3af3092a.jpg:\n",
      "- CleanNet\n",
      "- CleanNet\n",
      "- General Waste\n",
      "- General Waste\n",
      "Processing times:\n",
      "- Preprocess: 0.1494884490966797ms\n",
      "- Inference: 38.872718811035156ms\n",
      "- Postprocess: 0.0ms\n",
      "\n",
      "Processing image 61/79: 202411111221-36-307340-127-353540_jpg.rf.44607ac9cf9e836b2131ef722b16478e.jpg\n",
      "\n",
      "image 1/1 E:\\livingLab\\YOLOv8 \\test\\images\\202411111221-36-307340-127-353540_jpg.rf.44607ac9cf9e836b2131ef722b16478e.jpg: 640x640 (no detections), 30.9ms\n",
      "Speed: 1.2ms preprocess, 30.9ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detected Classes in 202411111221-36-307340-127-353540_jpg.rf.44607ac9cf9e836b2131ef722b16478e.jpg:\n",
      "Processing times:\n",
      "- Preprocess: 1.22833251953125ms\n",
      "- Inference: 30.88855743408203ms\n",
      "- Postprocess: 0.0ms\n",
      "\n",
      "Processing image 62/79: 202411111223-36-307550-127-354681_jpg.rf.23480e44a8092c9c3311ca80c0ed1d96.jpg\n",
      "\n",
      "image 1/1 E:\\livingLab\\YOLOv8 \\test\\images\\202411111223-36-307550-127-354681_jpg.rf.23480e44a8092c9c3311ca80c0ed1d96.jpg: 640x640 2 CleanNets, 35.3ms\n",
      "Speed: 1.2ms preprocess, 35.3ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detected Classes in 202411111223-36-307550-127-354681_jpg.rf.23480e44a8092c9c3311ca80c0ed1d96.jpg:\n",
      "- CleanNet\n",
      "- CleanNet\n",
      "Processing times:\n",
      "- Preprocess: 1.1670589447021484ms\n",
      "- Inference: 35.33530235290527ms\n",
      "- Postprocess: 0.0ms\n",
      "\n",
      "Processing image 63/79: 202411111224-36-307974-127-355197_jpg.rf.8824813e9e1263d3c32b197514edcf0b.jpg\n",
      "\n",
      "image 1/1 E:\\livingLab\\YOLOv8 \\test\\images\\202411111224-36-307974-127-355197_jpg.rf.8824813e9e1263d3c32b197514edcf0b.jpg: 640x640 8 General Wastes, 3 CleanNets, 33.1ms\n",
      "Speed: 0.5ms preprocess, 33.1ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detected Classes in 202411111224-36-307974-127-355197_jpg.rf.8824813e9e1263d3c32b197514edcf0b.jpg:\n",
      "- CleanNet\n",
      "- General Waste\n",
      "- General Waste\n",
      "- General Waste\n",
      "- General Waste\n",
      "- CleanNet\n",
      "- CleanNet\n",
      "- General Waste\n",
      "- General Waste\n",
      "- General Waste\n",
      "- General Waste\n",
      "Processing times:\n",
      "- Preprocess: 0.4878044128417969ms\n",
      "- Inference: 33.13398361206055ms\n",
      "- Postprocess: 0.0ms\n",
      "\n",
      "Processing image 64/79: 202411111226-36-306652-127-353674_jpg.rf.bf1c0077ff4820a3e86121a92f4c3b79.jpg\n",
      "\n",
      "image 1/1 E:\\livingLab\\YOLOv8 \\test\\images\\202411111226-36-306652-127-353674_jpg.rf.bf1c0077ff4820a3e86121a92f4c3b79.jpg: 640x640 4 General Wastes, 2 CleanNets, 34.4ms\n",
      "Speed: 2.0ms preprocess, 34.4ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detected Classes in 202411111226-36-306652-127-353674_jpg.rf.bf1c0077ff4820a3e86121a92f4c3b79.jpg:\n",
      "- CleanNet\n",
      "- CleanNet\n",
      "- General Waste\n",
      "- General Waste\n",
      "- General Waste\n",
      "- General Waste\n",
      "Processing times:\n",
      "- Preprocess: 2.0155906677246094ms\n",
      "- Inference: 34.433603286743164ms\n",
      "- Postprocess: 0.0ms\n",
      "\n",
      "Processing image 65/79: 202411111226-36-306967-127-355016-1_jpg.rf.7b71d120bd3741d88af836c1e341b460.jpg\n",
      "\n",
      "image 1/1 E:\\livingLab\\YOLOv8 \\test\\images\\202411111226-36-306967-127-355016-1_jpg.rf.7b71d120bd3741d88af836c1e341b460.jpg: 640x640 1 CleanNet, 31.2ms\n",
      "Speed: 0.1ms preprocess, 31.2ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detected Classes in 202411111226-36-306967-127-355016-1_jpg.rf.7b71d120bd3741d88af836c1e341b460.jpg:\n",
      "- CleanNet\n",
      "Processing times:\n",
      "- Preprocess: 0.14209747314453125ms\n",
      "- Inference: 31.15081787109375ms\n",
      "- Postprocess: 0.0ms\n",
      "\n",
      "Processing image 66/79: 202411111228-36-307488-127-352668_jpg.rf.c021bf13ef90ee8f5a21b75c386ba96e.jpg\n",
      "\n",
      "image 1/1 E:\\livingLab\\YOLOv8 \\test\\images\\202411111228-36-307488-127-352668_jpg.rf.c021bf13ef90ee8f5a21b75c386ba96e.jpg: 640x640 1 Large Waste Items, 2 General Wastes, 2 CleanNets, 34.9ms\n",
      "Speed: 0.2ms preprocess, 34.9ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detected Classes in 202411111228-36-307488-127-352668_jpg.rf.c021bf13ef90ee8f5a21b75c386ba96e.jpg:\n",
      "- CleanNet\n",
      "- CleanNet\n",
      "- General Waste\n",
      "- General Waste\n",
      "- Large Waste Items\n",
      "Processing times:\n",
      "- Preprocess: 0.22101402282714844ms\n",
      "- Inference: 34.93857383728027ms\n",
      "- Postprocess: 0.0ms\n",
      "\n",
      "Processing image 67/79: 202411111229-36-308507-127-351318_jpg.rf.d6a1f843c6a43aa44bb88941b03a0142.jpg\n",
      "\n",
      "image 1/1 E:\\livingLab\\YOLOv8 \\test\\images\\202411111229-36-308507-127-351318_jpg.rf.d6a1f843c6a43aa44bb88941b03a0142.jpg: 640x640 1 General Waste, 2 CleanNets, 31.6ms\n",
      "Speed: 0.2ms preprocess, 31.6ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detected Classes in 202411111229-36-308507-127-351318_jpg.rf.d6a1f843c6a43aa44bb88941b03a0142.jpg:\n",
      "- CleanNet\n",
      "- CleanNet\n",
      "- General Waste\n",
      "Processing times:\n",
      "- Preprocess: 0.23102760314941406ms\n",
      "- Inference: 31.55660629272461ms\n",
      "- Postprocess: 0.0ms\n",
      "\n",
      "Processing image 68/79: 202411111231-36-307637-127-351064_jpg.rf.089c4d450857dd4dca741be2cf4f1cbb.jpg\n",
      "\n",
      "image 1/1 E:\\livingLab\\YOLOv8 \\test\\images\\202411111231-36-307637-127-351064_jpg.rf.089c4d450857dd4dca741be2cf4f1cbb.jpg: 640x640 2 Large Waste Itemss, 1 PP bag, 1 General Waste, 2 CleanNets, 36.8ms\n",
      "Speed: 0.2ms preprocess, 36.8ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detected Classes in 202411111231-36-307637-127-351064_jpg.rf.089c4d450857dd4dca741be2cf4f1cbb.jpg:\n",
      "- CleanNet\n",
      "- Large Waste Items\n",
      "- General Waste\n",
      "- CleanNet\n",
      "- PP bag\n",
      "- Large Waste Items\n",
      "Processing times:\n",
      "- Preprocess: 0.22912025451660156ms\n",
      "- Inference: 36.820173263549805ms\n",
      "- Postprocess: 0.0ms\n",
      "\n",
      "Processing image 69/79: 202411111234-36-306079-127-351546_jpg.rf.c9c863d9067d40dd0035692f3d78d305.jpg\n",
      "\n",
      "image 1/1 E:\\livingLab\\YOLOv8 \\test\\images\\202411111234-36-306079-127-351546_jpg.rf.c9c863d9067d40dd0035692f3d78d305.jpg: 640x640 8 General Wastes, 2 CleanNets, 32.1ms\n",
      "Speed: 0.0ms preprocess, 32.1ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detected Classes in 202411111234-36-306079-127-351546_jpg.rf.c9c863d9067d40dd0035692f3d78d305.jpg:\n",
      "- CleanNet\n",
      "- CleanNet\n",
      "- General Waste\n",
      "- General Waste\n",
      "- General Waste\n",
      "- General Waste\n",
      "- General Waste\n",
      "- General Waste\n",
      "- General Waste\n",
      "- General Waste\n",
      "Processing times:\n",
      "- Preprocess: 0.0ms\n",
      "- Inference: 32.05251693725586ms\n",
      "- Postprocess: 0.0ms\n",
      "\n",
      "Processing image 70/79: 202411111237-36-306997-127-351164_jpg.rf.45266fd2de02a14c1c674892c8a0ae33.jpg\n",
      "\n",
      "image 1/1 E:\\livingLab\\YOLOv8 \\test\\images\\202411111237-36-306997-127-351164_jpg.rf.45266fd2de02a14c1c674892c8a0ae33.jpg: 640x640 4 General Wastes, 2 CleanNets, 34.7ms\n",
      "Speed: 0.1ms preprocess, 34.7ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detected Classes in 202411111237-36-306997-127-351164_jpg.rf.45266fd2de02a14c1c674892c8a0ae33.jpg:\n",
      "- CleanNet\n",
      "- CleanNet\n",
      "- General Waste\n",
      "- General Waste\n",
      "- General Waste\n",
      "- General Waste\n",
      "Processing times:\n",
      "- Preprocess: 0.141143798828125ms\n",
      "- Inference: 34.70277786254883ms\n",
      "- Postprocess: 0.0ms\n",
      "\n",
      "Processing image 71/79: 202411111238-36-306863-127-350438_jpg.rf.6129b71701b75171bd7c11bce00321de.jpg\n",
      "\n",
      "image 1/1 E:\\livingLab\\YOLOv8 \\test\\images\\202411111238-36-306863-127-350438_jpg.rf.6129b71701b75171bd7c11bce00321de.jpg: 640x640 4 General Wastes, 2 CleanNets, 27.7ms\n",
      "Speed: 3.2ms preprocess, 27.7ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detected Classes in 202411111238-36-306863-127-350438_jpg.rf.6129b71701b75171bd7c11bce00321de.jpg:\n",
      "- CleanNet\n",
      "- CleanNet\n",
      "- General Waste\n",
      "- General Waste\n",
      "- General Waste\n",
      "- General Waste\n",
      "Processing times:\n",
      "- Preprocess: 3.2100677490234375ms\n",
      "- Inference: 27.667999267578125ms\n",
      "- Postprocess: 0.0ms\n",
      "\n",
      "Processing image 72/79: 202411111240-36-305737-127-350237_jpg.rf.07a211fd644530b35bcda797c76089f0.jpg\n",
      "\n",
      "image 1/1 E:\\livingLab\\YOLOv8 \\test\\images\\202411111240-36-305737-127-350237_jpg.rf.07a211fd644530b35bcda797c76089f0.jpg: 640x640 1 General Waste, 5 CleanNets, 29.7ms\n",
      "Speed: 2.0ms preprocess, 29.7ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detected Classes in 202411111240-36-305737-127-350237_jpg.rf.07a211fd644530b35bcda797c76089f0.jpg:\n",
      "- CleanNet\n",
      "- CleanNet\n",
      "- General Waste\n",
      "- CleanNet\n",
      "- CleanNet\n",
      "- CleanNet\n",
      "Processing times:\n",
      "- Preprocess: 2.032041549682617ms\n",
      "- Inference: 29.6781063079834ms\n",
      "- Postprocess: 0.0ms\n",
      "\n",
      "Processing image 73/79: 202411111241-36-305407-127-348838_jpg.rf.c9a72083f4584c5401821f536b962985.jpg\n",
      "\n",
      "image 1/1 E:\\livingLab\\YOLOv8 \\test\\images\\202411111241-36-305407-127-348838_jpg.rf.c9a72083f4584c5401821f536b962985.jpg: 640x640 1 General Waste, 2 CleanNets, 28.7ms\n",
      "Speed: 3.1ms preprocess, 28.7ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detected Classes in 202411111241-36-305407-127-348838_jpg.rf.c9a72083f4584c5401821f536b962985.jpg:\n",
      "- CleanNet\n",
      "- CleanNet\n",
      "- General Waste\n",
      "Processing times:\n",
      "- Preprocess: 3.147125244140625ms\n",
      "- Inference: 28.676271438598633ms\n",
      "- Postprocess: 0.0ms\n",
      "\n",
      "Processing image 74/79: 202411111246-36-324233-127-344985_jpg.rf.d4fac2031eba1010c932d2ba451f0b9e.jpg\n",
      "\n",
      "image 1/1 E:\\livingLab\\YOLOv8 \\test\\images\\202411111246-36-324233-127-344985_jpg.rf.d4fac2031eba1010c932d2ba451f0b9e.jpg: 640x640 1 Large Waste Items, 5 General Wastes, 2 CleanNets, 28.5ms\n",
      "Speed: 2.5ms preprocess, 28.5ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detected Classes in 202411111246-36-324233-127-344985_jpg.rf.d4fac2031eba1010c932d2ba451f0b9e.jpg:\n",
      "- CleanNet\n",
      "- General Waste\n",
      "- General Waste\n",
      "- General Waste\n",
      "- General Waste\n",
      "- CleanNet\n",
      "- Large Waste Items\n",
      "- General Waste\n",
      "Processing times:\n",
      "- Preprocess: 2.547025680541992ms\n",
      "- Inference: 28.533935546875ms\n",
      "- Postprocess: 1.107931137084961ms\n",
      "\n",
      "Processing image 75/79: 202411111251-36-322309-127-342752_jpg.rf.cee98eb6f397c8e712d2cc1e38b025ba.jpg\n",
      "\n",
      "image 1/1 E:\\livingLab\\YOLOv8 \\test\\images\\202411111251-36-322309-127-342752_jpg.rf.cee98eb6f397c8e712d2cc1e38b025ba.jpg: 640x640 4 General Wastes, 2 CleanNets, 18.0ms\n",
      "Speed: 0.1ms preprocess, 18.0ms inference, 15.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detected Classes in 202411111251-36-322309-127-342752_jpg.rf.cee98eb6f397c8e712d2cc1e38b025ba.jpg:\n",
      "- CleanNet\n",
      "- CleanNet\n",
      "- General Waste\n",
      "- General Waste\n",
      "- General Waste\n",
      "- General Waste\n",
      "Processing times:\n",
      "- Preprocess: 0.14448165893554688ms\n",
      "- Inference: 17.97938346862793ms\n",
      "- Postprocess: 15.624761581420898ms\n",
      "\n",
      "Processing image 76/79: 202411111252-36-321934-127-343575_jpg.rf.f74b0372f7638eef7e00d1d9f0c16f70.jpg\n",
      "\n",
      "image 1/1 E:\\livingLab\\YOLOv8 \\test\\images\\202411111252-36-321934-127-343575_jpg.rf.f74b0372f7638eef7e00d1d9f0c16f70.jpg: 640x640 1 General Waste, 1 CleanNet, 33.3ms\n",
      "Speed: 0.1ms preprocess, 33.3ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detected Classes in 202411111252-36-321934-127-343575_jpg.rf.f74b0372f7638eef7e00d1d9f0c16f70.jpg:\n",
      "- CleanNet\n",
      "- General Waste\n",
      "Processing times:\n",
      "- Preprocess: 0.14781951904296875ms\n",
      "- Inference: 33.32805633544922ms\n",
      "- Postprocess: 0.0ms\n",
      "\n",
      "Processing image 77/79: 23907_68481_2024_jpg.rf.b54912c39030808ac7c4302f5f2a9541.jpg\n",
      "\n",
      "image 1/1 E:\\livingLab\\YOLOv8 \\test\\images\\23907_68481_2024_jpg.rf.b54912c39030808ac7c4302f5f2a9541.jpg: 640x640 2 PP bags, 2 General Wastes, 33.0ms\n",
      "Speed: 0.5ms preprocess, 33.0ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detected Classes in 23907_68481_2024_jpg.rf.b54912c39030808ac7c4302f5f2a9541.jpg:\n",
      "- PP bag\n",
      "- PP bag\n",
      "- General Waste\n",
      "- General Waste\n",
      "Processing times:\n",
      "- Preprocess: 0.4887580871582031ms\n",
      "- Inference: 32.95588493347168ms\n",
      "- Postprocess: 0.0ms\n",
      "\n",
      "Processing image 78/79: 280-1-_jpg.rf.da456a3686d9264b2091920e32945993.jpg\n",
      "\n",
      "image 1/1 E:\\livingLab\\YOLOv8 \\test\\images\\280-1-_jpg.rf.da456a3686d9264b2091920e32945993.jpg: 640x640 3 PP bags, 38.3ms\n",
      "Speed: 0.0ms preprocess, 38.3ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detected Classes in 280-1-_jpg.rf.da456a3686d9264b2091920e32945993.jpg:\n",
      "- PP bag\n",
      "- PP bag\n",
      "- PP bag\n",
      "Processing times:\n",
      "- Preprocess: 0.0ms\n",
      "- Inference: 38.27404975891113ms\n",
      "- Postprocess: 0.0ms\n",
      "\n",
      "Processing image 79/79: 40e6c194a7_jpg.rf.d53564843314e6adb88ef92998cdaf82.jpg\n",
      "\n",
      "image 1/1 E:\\livingLab\\YOLOv8 \\test\\images\\40e6c194a7_jpg.rf.d53564843314e6adb88ef92998cdaf82.jpg: 640x640 3 PP bags, 33.2ms\n",
      "Speed: 0.0ms preprocess, 33.2ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detected Classes in 40e6c194a7_jpg.rf.d53564843314e6adb88ef92998cdaf82.jpg:\n",
      "- PP bag\n",
      "- PP bag\n",
      "- PP bag\n",
      "Processing times:\n",
      "- Preprocess: 0.0ms\n",
      "- Inference: 33.1873893737793ms\n",
      "- Postprocess: 0.0ms\n",
      "\n",
      "All results saved to ./runs/detect/predict/\n",
      "\n",
      "Converting labels to major categories...\n",
      "\n",
      "Performing binary classification...\n",
      "\n",
      "Image: 202411061348-36-313711-127-349988_jpg.rf.3581e8d79a3a050686554a20476cdb34.jpg\n",
      "Detected classes: ['CleanNet', 'CleanNet', 'General Waste']\n",
      "Classification: Negative\n",
      "\n",
      "Image: 202411061346-36-311745-127-353092_jpg.rf.3e408eccde358b63c05fd5a29f956aab.jpg\n",
      "Detected classes: ['General Waste', 'CleanNet', 'CleanNet']\n",
      "Classification: Negative\n",
      "\n",
      "Image: 202411111240-36-305737-127-350237_jpg.rf.07a211fd644530b35bcda797c76089f0.jpg\n",
      "Detected classes: ['CleanNet', 'CleanNet', 'General Waste', 'Large Waste Items']\n",
      "Classification: Positive\n",
      "\n",
      "Image: 202411111231-36-307637-127-351064_jpg.rf.089c4d450857dd4dca741be2cf4f1cbb.jpg\n",
      "Detected classes: ['CleanNet', 'CleanNet', 'Large Waste Items']\n",
      "Classification: Positive\n",
      "\n",
      "Image: 202411061319-36-307974-127-355197_jpg.rf.40e66b47ea454f90ee055c740410cc1c.jpg\n",
      "Detected classes: ['CleanNet', 'CleanNet', 'General Waste']\n",
      "Classification: Negative\n",
      "\n",
      "Image: 202411061313-36-307488-127-352668_jpg.rf.2debbe814df92503a0079be859650cc0.jpg\n",
      "Detected classes: ['Large Waste Items', 'CleanNet', 'CleanNet', 'General Waste']\n",
      "Classification: Positive\n",
      "\n",
      "Image: 202411111223-36-307550-127-354681_jpg.rf.23480e44a8092c9c3311ca80c0ed1d96.jpg\n",
      "Detected classes: ['CleanNet', 'CleanNet']\n",
      "Classification: Negative\n",
      "\n",
      "Image: 202411111216-36-309841-127-353640_jpg.rf.0cfe34d4e561604f27e4d6d51709d093.jpg\n",
      "Detected classes: ['CleanNet', 'CleanNet', 'General Waste']\n",
      "Classification: Negative\n",
      "\n",
      "Image: 202411111148-36-330942-127-338586_jpg.rf.09088434bf25f50d833296c7059228a4.jpg\n",
      "Detected classes: ['CleanNet', 'CleanNet', 'CleanNet']\n",
      "Classification: Negative\n",
      "\n",
      "Image: 202411061431-36-309908-127-348185_jpg.rf.23fe7b1ddec31535d885e6cc9288d2da.jpg\n",
      "Detected classes: ['Large Waste Items', 'Large Waste Items', 'Large Waste Items', 'Large Waste Items', 'Large Waste Items', 'Large Waste Items']\n",
      "Classification: Positive\n",
      "\n",
      "Image: 202411061348-36-310510-127-353380_jpg.rf.1b445c1bbde0cedac1df9b0f9c220dd4.jpg\n",
      "Detected classes: ['CleanNet', 'CleanNet', 'General Waste']\n",
      "Classification: Negative\n",
      "\n",
      "Image: 202411081236-36-423913-127-407165-1_jpg.rf.0dfb1f31bd1bafebca3f8424e98cdc61.jpg\n",
      "Detected classes: ['General Waste', 'Large Waste Items', 'Large Waste Items', 'Large Waste Items', 'Large Waste Items', 'Large Waste Items', 'Large Waste Items']\n",
      "Classification: Positive\n",
      "\n",
      "Image: -7-_jpg.rf.3e4e45f01df8da1ac999a94efb60473e.jpg\n",
      "Detected classes: ['PP bag']\n",
      "Classification: Negative\n",
      "\n",
      "Image: 202411111237-36-306997-127-351164_jpg.rf.45266fd2de02a14c1c674892c8a0ae33.jpg\n",
      "Detected classes: ['CleanNet', 'CleanNet', 'General Waste', 'General Waste', 'General Waste', 'General Waste']\n",
      "Classification: Negative\n",
      "\n",
      "Image: 202411061352-36-313215-127-351277-3_jpg.rf.46cfc13747bebd1a1755da3b8a3cd8dd.jpg\n",
      "Detected classes: ['General Waste', 'Large Waste Items', 'Large Waste Items', 'PP bag', 'PP bag', 'PP bag', 'PP bag', 'PP bag', 'PP bag', 'PP bag', 'PP bag', 'PP bag']\n",
      "Classification: Positive\n",
      "\n",
      "Image: 202411111206-36-311745-127-353092_jpg.rf.175848f0b77ba404e4e6f11f44f8c590.jpg\n",
      "Detected classes: ['CleanNet', 'CleanNet', 'General Waste']\n",
      "Classification: Negative\n",
      "\n",
      "Image: 202411111221-36-307340-127-353540_jpg.rf.44607ac9cf9e836b2131ef722b16478e.jpg\n",
      "Detected classes: ['CleanNet', 'CleanNet']\n",
      "Classification: Negative\n",
      "\n",
      "Image: 202411111150-36-331699-127-339169-2_jpg.rf.30de6ea0a36f7a5dff4c2d5633c75ff7.jpg\n",
      "Detected classes: ['Large Waste Items', 'Large Waste Items']\n",
      "Classification: Positive\n",
      "\n",
      "Image: 202411061342-36-309176-127-351941_jpg.rf.05afb3c8e078242b96c4a730357296bb.jpg\n",
      "Detected classes: ['CleanNet', 'General Waste', 'CleanNet']\n",
      "Classification: Negative\n",
      "\n",
      "Image: 202411061423-36-313839-127-348579_jpg.rf.4c6e29fd453c998c1710c6dd014daacf.jpg\n",
      "Detected classes: ['General Waste', 'Large Waste Items', 'Large Waste Items', 'Large Waste Items', 'Large Waste Items', 'Large Waste Items', 'Large Waste Items', 'Large Waste Items', 'Large Waste Items', 'Large Waste Items', 'General Waste']\n",
      "Classification: Positive\n",
      "\n",
      "Image: 202411061337-36-309239-127-353916_jpg.rf.4c7a323affb98f997405ec20ca6d216e.jpg\n",
      "Detected classes: ['CleanNet', 'CleanNet', 'General Waste', 'General Waste']\n",
      "Classification: Negative\n",
      "\n",
      "Image: 202411061330-36-318792-127-344275_jpg.rf.3f74fb24878d9a28014f79a8ad00e4c4.jpg\n",
      "Detected classes: ['General Waste']\n",
      "Classification: Negative\n",
      "\n",
      "Image: 202411111217-36-309239-127-353916_jpg.rf.45de2cd861119d382d4752616a1c7e31.jpg\n",
      "Detected classes: ['General Waste', 'General Waste', 'General Waste', 'General Waste', 'CleanNet', 'CleanNet']\n",
      "Classification: Negative\n",
      "\n",
      "Image: 202411111150-36-331699-127-339169-1_jpg.rf.2dfbaba491bab23e08d12300bdec0a40.jpg\n",
      "Detected classes: ['CleanNet', 'CleanNet', 'CleanNet', 'General Waste', 'General Waste', 'General Waste', 'General Waste']\n",
      "Classification: Negative\n",
      "\n",
      "Image: 202411061434-36-309502-127-348344_jpg.rf.18196b3699941fd051eb67f4246945bb.jpg\n",
      "Detected classes: ['Large Waste Items', 'Large Waste Items', 'Large Waste Items', 'General Waste', 'Large Waste Items']\n",
      "Classification: Positive\n",
      "\n",
      "Image: 202411061349-36-313040-127-350293_jpg.rf.52fab8e4c82156dcff635e98319420f9.jpg\n",
      "Detected classes: ['General Waste', 'CleanNet', 'CleanNet', 'General Waste']\n",
      "Classification: Negative\n",
      "\n",
      "Image: 202411061323-36-319426-127-343581-2_jpg.rf.5c85eeb865231f3fb9ac4fd355319416.jpg\n",
      "Detected classes: ['General Waste', 'General Waste', 'General Waste', 'General Waste', 'General Waste', 'General Waste', 'General Waste', 'General Waste', 'Large Waste Items']\n",
      "Classification: Positive\n",
      "\n",
      "Image: 202411061415-36-316636-127-347495_jpg.rf.79ef2fb18c5b765ead9711ec6516fcea.jpg\n",
      "Detected classes: ['Large Waste Items', 'Large Waste Items', 'Large Waste Items', 'Large Waste Items', 'Large Waste Items', 'Large Waste Items']\n",
      "Classification: Positive\n",
      "\n",
      "Image: 202411111226-36-306967-127-355016-1_jpg.rf.7b71d120bd3741d88af836c1e341b460.jpg\n",
      "Detected classes: ['CleanNet', 'CleanNet']\n",
      "Classification: Negative\n",
      "\n",
      "Image: 202411111210-36-310957-127-353130_jpg.rf.693876733d993c585b16039273a00299.jpg\n",
      "Detected classes: ['CleanNet', 'CleanNet', 'General Waste', 'General Waste', 'General Waste']\n",
      "Classification: Negative\n",
      "\n",
      "Image: 202411111238-36-306863-127-350438_jpg.rf.6129b71701b75171bd7c11bce00321de.jpg\n",
      "Detected classes: ['CleanNet', 'CleanNet', 'Large Waste Items', 'Large Waste Items', 'General Waste', 'General Waste']\n",
      "Classification: Positive\n",
      "\n",
      "Image: 202411061339-36-310790-127-352166_jpg.rf.78fd2988c35f72ab7c8c932764bb04c6.jpg\n",
      "Detected classes: ['CleanNet', 'CleanNet', 'General Waste']\n",
      "Classification: Negative\n",
      "\n",
      "Image: 202411061342-36-315640-127-349174_jpg.rf.5444f5989d8b4ed01d08d05553d63571.jpg\n",
      "Detected classes: ['CleanNet', 'CleanNet', 'General Waste']\n",
      "Classification: Negative\n",
      "\n",
      "Image: 202411061322-36-320218-127-343319_jpg.rf.86814c1d60da676d5d9596a2a3da0efb.jpg\n",
      "Detected classes: ['CleanNet', 'General Waste', 'General Waste', 'General Waste', 'General Waste', 'Large Waste Items', 'Large Waste Items']\n",
      "Classification: Positive\n",
      "\n",
      "Image: 202411061329-36-309133-127-353018_jpg.rf.66723f2b0a1f776c240789b2605d7c6e.jpg\n",
      "Detected classes: ['CleanNet', 'CleanNet', 'General Waste']\n",
      "Classification: Negative\n",
      "\n",
      "Image: 202411061423-36-314851-127-347123_jpg.rf.6b1796d5ce76d550ec8c684a8c23fafd.jpg\n",
      "Detected classes: ['General Waste', 'Large Waste Items', 'Large Waste Items', 'Large Waste Items', 'Large Waste Items', 'Large Waste Items', 'Large Waste Items', 'Large Waste Items', 'PP bag', 'Large Waste Items', 'PP bag', 'PP bag', 'PP bag', 'PP bag', 'PP bag', 'PP bag', 'PP bag', 'PP bag', 'PP bag', 'PP bag', 'PP bag', 'PP bag', 'PP bag', 'PP bag', 'PP bag', 'PP bag', 'PP bag', 'PP bag', 'PP bag', 'PP bag', 'PP bag', 'PP bag', 'PP bag', 'PP bag', 'PP bag', 'PP bag', 'PP bag', 'PP bag']\n",
      "Classification: Positive\n",
      "\n",
      "Image: 202411061431-36-311574-127-349342_jpg.rf.5182513e42f5000cf9af2f91816f2114.jpg\n",
      "Detected classes: ['Large Waste Items', 'Large Waste Items', 'Large Waste Items', 'Large Waste Items', 'General Waste']\n",
      "Classification: Positive\n",
      "\n",
      "Image: 23907_68481_2024_jpg.rf.b54912c39030808ac7c4302f5f2a9541.jpg\n",
      "Detected classes: ['PP bag', 'PP bag', 'PP bag', 'PP bag', 'PP bag', 'PP bag', 'PP bag', 'PP bag']\n",
      "Classification: Negative\n",
      "\n",
      "Image: 202411061438-36-309254-127-350078_jpg.rf.63980ab462c1a491c72131a321aa9bc4.jpg\n",
      "Detected classes: ['Large Waste Items', 'Large Waste Items', 'Large Waste Items', 'Large Waste Items']\n",
      "Classification: Positive\n",
      "\n",
      "Image: 202411061346-36-308081-127-353130_jpg.rf.83770fab01c2a2e62838a2a2729231a7.jpg\n",
      "Detected classes: ['CleanNet', 'CleanNet', 'General Waste']\n",
      "Classification: Negative\n",
      "\n",
      "Image: 202411061358-36-308465-127-354548_jpg.rf.965f2f2b47de25bdc9d795cb71576543.jpg\n",
      "Detected classes: ['CleanNet', 'CleanNet', 'General Waste', 'General Waste', 'General Waste']\n",
      "Classification: Negative\n",
      "\n",
      "Image: 202411061433-36-309089-127-349308_jpg.rf.680978375d34a5ce6bd988b65213fe68.jpg\n",
      "Detected classes: ['General Waste', 'General Waste', 'General Waste', 'Large Waste Items', 'Large Waste Items', 'Large Waste Items', 'Large Waste Items', 'Large Waste Items', 'Large Waste Items']\n",
      "Classification: Positive\n",
      "\n",
      "Image: 202411111207-36-310510-127-353380_jpg.rf.816d4f9ca63a539abb4e7bd0bfafc0e0.jpg\n",
      "Detected classes: ['General Waste', 'CleanNet', 'CleanNet']\n",
      "Classification: Negative\n",
      "\n",
      "Image: 202411061328-36-318467-127-342773_jpg.rf.8e7d885664cc4a093d1ef0afcc79b2e8.jpg\n",
      "Detected classes: ['General Waste', 'General Waste', 'CleanNet', 'CleanNet']\n",
      "Classification: Negative\n",
      "\n",
      "Image: 202411111224-36-307974-127-355197_jpg.rf.8824813e9e1263d3c32b197514edcf0b.jpg\n",
      "Detected classes: ['CleanNet', 'CleanNet', 'General Waste', 'General Waste', 'General Waste', 'General Waste', 'General Waste']\n",
      "Classification: Negative\n",
      "\n",
      "Image: 202411061345-36-314850-127-350158_jpg.rf.a5c076f0199e673c36da4ac790ee940e.jpg\n",
      "Detected classes: ['CleanNet', 'CleanNet', 'General Waste', 'General Waste']\n",
      "Classification: Negative\n",
      "\n",
      "Image: 202411061359-36-310957-127-353130_jpg.rf.906b474010a7ba8fcea77eeef01433cd.jpg\n",
      "Detected classes: ['CleanNet', 'CleanNet', 'General Waste', 'General Waste', 'General Waste', 'General Waste', 'General Waste']\n",
      "Classification: Negative\n",
      "\n",
      "Image: 202411061328-36-318047-127-344154_jpg.rf.7aefec6f116650e0e611553b9811a6d8.jpg\n",
      "Detected classes: ['CleanNet', 'CleanNet', 'General Waste']\n",
      "Classification: Negative\n",
      "\n",
      "Image: 202411111136-36-331616-127-335799_jpg.rf.896aa812873491b43e52dfbd3261d149.jpg\n",
      "Detected classes: ['PP bag', 'PP bag', 'PP bag', 'PP bag', 'PP bag', 'PP bag', 'PP bag', 'PP bag', 'PP bag', 'PP bag', 'PP bag', 'PP bag', 'PP bag', 'PP bag', 'PP bag', 'PP bag', 'PP bag', 'PP bag']\n",
      "Classification: Negative\n",
      "\n",
      "Image: 202411111228-36-307488-127-352668_jpg.rf.c021bf13ef90ee8f5a21b75c386ba96e.jpg\n",
      "Detected classes: ['CleanNet', 'CleanNet', 'General Waste', 'General Waste', 'General Waste']\n",
      "Classification: Negative\n",
      "\n",
      "Image: 202411061423-36-315046-127-346080_jpg.rf.bdc3f041759c2e28c0f15e1397a3584d.jpg\n",
      "Detected classes: ['Large Waste Items', 'Large Waste Items', 'Large Waste Items', 'Large Waste Items', 'Large Waste Items', 'Large Waste Items', 'Large Waste Items']\n",
      "Classification: Positive\n",
      "\n",
      "Image: 202411111137-36-331383-127-335375_jpg.rf.a50332a13cae1728d2c5dc6162c5b05d.jpg\n",
      "Detected classes: ['PP bag', 'PP bag', 'PP bag', 'PP bag', 'PP bag']\n",
      "Classification: Negative\n",
      "\n",
      "Image: 202411111246-36-324233-127-344985_jpg.rf.d4fac2031eba1010c932d2ba451f0b9e.jpg\n",
      "Detected classes: ['General Waste', 'General Waste', 'General Waste', 'CleanNet', 'CleanNet']\n",
      "Classification: Negative\n",
      "\n",
      "Image: 202411061346-36-314198-127-350451_jpg.rf.c77a12c283995fb4057bd0ca00b83161.jpg\n",
      "Detected classes: ['Large Waste Items', 'Large Waste Items']\n",
      "Classification: Positive\n",
      "\n",
      "Image: 202411111234-36-306079-127-351546_jpg.rf.c9c863d9067d40dd0035692f3d78d305.jpg\n",
      "Detected classes: ['General Waste', 'General Waste', 'General Waste', 'General Waste', 'General Waste', 'General Waste', 'CleanNet', 'CleanNet']\n",
      "Classification: Negative\n",
      "\n",
      "Image: 202411111220-36-308081-127-353130_jpg.rf.d482c4c6079ceee93fea151e3af3092a.jpg\n",
      "Detected classes: ['CleanNet', 'CleanNet', 'General Waste', 'General Waste', 'General Waste', 'General Waste']\n",
      "Classification: Negative\n",
      "\n",
      "Image: 202411061322-36-307550-127-354681_jpg.rf.e09f59bc4cc8915a9da9c47c91f306f0.jpg\n",
      "Detected classes: ['CleanNet', 'CleanNet', 'PP bag', 'General Waste', 'General Waste', 'General Waste']\n",
      "Classification: Negative\n",
      "\n",
      "Image: 202411111251-36-322309-127-342752_jpg.rf.cee98eb6f397c8e712d2cc1e38b025ba.jpg\n",
      "Detected classes: ['CleanNet', 'CleanNet', 'General Waste', 'General Waste', 'General Waste', 'General Waste']\n",
      "Classification: Negative\n",
      "\n",
      "Image: 280-1-_jpg.rf.da456a3686d9264b2091920e32945993.jpg\n",
      "Detected classes: ['PP bag', 'PP bag']\n",
      "Classification: Negative\n",
      "\n",
      "Image: 202411061350-36-314129-127-351297_jpg.rf.7b7238965a8cae0683e4d2bac4f5a06a.jpg\n",
      "Detected classes: ['CleanNet', 'CleanNet', 'General Waste', 'General Waste', 'General Waste', 'General Waste']\n",
      "Classification: Negative\n",
      "\n",
      "Image: 202411111139-36-331111-127-337848_jpg.rf.cbb982f1d1e160b11ae586add407ded9.jpg\n",
      "Detected classes: ['Large Waste Items', 'CleanNet']\n",
      "Classification: Positive\n",
      "\n",
      "Image: 202411061352-36-313215-127-351277-2_jpg.rf.f6aa801aed2a6e2e10462b781d4358bd.jpg\n",
      "Detected classes: ['PP bag', 'PP bag', 'PP bag', 'PP bag', 'PP bag', 'PP bag', 'Large Waste Items']\n",
      "Classification: Positive\n",
      "\n",
      "Image: 202411061418-36-315697-127-346125_jpg.rf.e9aab1e2970ae7ad8efb2ab0e3dd7b81.jpg\n",
      "Detected classes: ['General Waste', 'Large Waste Items', 'Large Waste Items', 'Large Waste Items', 'Large Waste Items', 'Large Waste Items', 'Large Waste Items', 'Large Waste Items', 'Large Waste Items', 'Large Waste Items']\n",
      "Classification: Positive\n",
      "\n",
      "Image: 202411111153-36-36-331686-127-340556_jpg.rf.f610735c1c46cf5328fb96c544f43dd2.jpg\n",
      "Detected classes: ['CleanNet', 'CleanNet']\n",
      "Classification: Negative\n",
      "\n",
      "Image: 40e6c194a7_jpg.rf.d53564843314e6adb88ef92998cdaf82.jpg\n",
      "Detected classes: ['PP bag']\n",
      "Classification: Negative\n",
      "\n",
      "Image: 202411061343-36-315732-127-349908_jpg.rf.f32a738af7e8cb0569568da90e660036.jpg\n",
      "Detected classes: ['General Waste', 'General Waste', 'General Waste', 'General Waste', 'CleanNet']\n",
      "Classification: Negative\n",
      "\n",
      "Image: 202411061420-36-315055-127-346190_jpg.rf.f735fd248bc6690f4c75068de0897d1f.jpg\n",
      "Detected classes: ['Large Waste Items', 'Large Waste Items', 'Large Waste Items']\n",
      "Classification: Positive\n",
      "\n",
      "Image: 202411111212-36-310790-127-352166_jpg.rf.d4bb484b862d3105c60cdbe0712b5651.jpg\n",
      "Detected classes: ['General Waste', 'General Waste', 'General Waste', 'General Waste', 'General Waste', 'General Waste', 'CleanNet', 'CleanNet', 'General Waste']\n",
      "Classification: Negative\n",
      "\n",
      "Image: 202411111213-36-309176-127-351941_jpg.rf.ddc9885bd847ea5b84dc27ccf6fd9fd9.jpg\n",
      "Detected classes: ['CleanNet', 'CleanNet', 'Large Waste Items']\n",
      "Classification: Positive\n",
      "\n",
      "Image: 202411111215-36-309133-127-353018_jpg.rf.d74fb20ba076ca562ab4b9922841c5e4.jpg\n",
      "Detected classes: ['CleanNet', 'CleanNet', 'General Waste']\n",
      "Classification: Negative\n",
      "\n",
      "Image: 202411111229-36-308507-127-351318_jpg.rf.d6a1f843c6a43aa44bb88941b03a0142.jpg\n",
      "Detected classes: ['CleanNet', 'CleanNet', 'General Waste']\n",
      "Classification: Negative\n",
      "\n",
      "Image: 202411061326-36-318897-127-343185_jpg.rf.d6707abf64c488ae79ddec26206a2f89.jpg\n",
      "Detected classes: ['Large Waste Items', 'Large Waste Items', 'CleanNet', 'General Waste', 'General Waste']\n",
      "Classification: Positive\n",
      "\n",
      "Image: 202411111226-36-306652-127-353674_jpg.rf.bf1c0077ff4820a3e86121a92f4c3b79.jpg\n",
      "Detected classes: ['General Waste', 'CleanNet', 'CleanNet']\n",
      "Classification: Negative\n",
      "\n",
      "Image: 202411111241-36-305407-127-348838_jpg.rf.c9a72083f4584c5401821f536b962985.jpg\n",
      "Detected classes: ['General Waste', 'CleanNet', 'CleanNet']\n",
      "Classification: Negative\n",
      "\n",
      "Image: 202411081236-36-423913-127-407165-2_jpg.rf.f6e2a098f84601afac7d2350a475d4cc.jpg\n",
      "Detected classes: ['PP bag', 'PP bag', 'PP bag', 'PP bag', 'PP bag', 'PP bag', 'PP bag', 'PP bag', 'PP bag', 'PP bag', 'PP bag', 'PP bag', 'PP bag', 'PP bag', 'PP bag', 'PP bag', 'PP bag', 'PP bag', 'PP bag', 'PP bag', 'PP bag', 'PP bag']\n",
      "Classification: Negative\n",
      "\n",
      "Image: 202411061349-36-311745-127-353092_jpg.rf.fb5f4cbdb3036192e55d681050fe72da.jpg\n",
      "Detected classes: ['General Waste', 'General Waste', 'General Waste', 'CleanNet', 'CleanNet']\n",
      "Classification: Negative\n",
      "\n",
      "Image: 202411111252-36-321934-127-343575_jpg.rf.f74b0372f7638eef7e00d1d9f0c16f70.jpg\n",
      "Detected classes: ['General Waste', 'CleanNet']\n",
      "Classification: Negative\n",
      "\n",
      "Image: 202411111156-36-331079-127-341792_jpg.rf.fe0c796d57c7083f244e29ed95c6585c.jpg\n",
      "Detected classes: ['General Waste', 'General Waste', 'General Waste', 'CleanNet', 'CleanNet']\n",
      "Classification: Negative\n",
      "\n",
      "Image: 202411061325-36-308399-127-354248_jpg.rf.fe8e74b6e0015e468fad270eb87ca499.jpg\n",
      "Detected classes: ['CleanNet', 'CleanNet', 'General Waste', 'General Waste']\n",
      "Classification: Negative\n",
      "\n",
      "Classification Summary:\n",
      "Total images processed: 79\n",
      "Positive classifications: 25\n",
      "Negative classifications: 54\n",
      "\n",
      "Detailed results saved to: ./test/classification_results.json\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: 필요한 라이브러리 임포트\n",
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Cell 2: 카테고리 매핑 정의\n",
    "category_mapping = {\n",
    "    # 대형폐기물 (Large Waste Items)\n",
    "    'arcade machine': 'Large Waste Items',\n",
    "    'Audio': 'Large Waste Items',\n",
    "    'Computer': 'Large Waste Items',\n",
    "    'fax machine': 'Large Waste Items',\n",
    "    'Main unit': 'Large Waste Items',\n",
    "    'Monitor': 'Large Waste Items',\n",
    "    'Printer': 'Large Waste Items',\n",
    "    'sewing machine': 'Large Waste Items',\n",
    "    'Speaker': 'Large Waste Items',\n",
    "    'typewriter': 'Large Waste Items',\n",
    "    'Vacuum cleaner': 'Large Waste Items',\n",
    "    'Video player': 'Large Waste Items',\n",
    "    'Bathtub': 'Large Waste Items',\n",
    "    'Sink': 'Large Waste Items',\n",
    "    'Kitchen sink': 'Large Waste Items',\n",
    "    'Toilet bowl': 'Large Waste Items',\n",
    "    'Bed': 'Large Waste Items',\n",
    "    'Bookcase': 'Large Waste Items',\n",
    "    'Bookstand': 'Large Waste Items',\n",
    "    'Cabinet': 'Large Waste Items',\n",
    "    'chair': 'Large Waste Items',\n",
    "    'Cupboard': 'Large Waste Items',\n",
    "    'Desk': 'Large Waste Items',\n",
    "    'Dining table': 'Large Waste Items',\n",
    "    'Display cabinet': 'Large Waste Items',\n",
    "    'Display stand': 'Large Waste Items',\n",
    "    'Drawer unit': 'Large Waste Items',\n",
    "    'Shoe rack': 'Large Waste Items',\n",
    "    'Small cabinet': 'Large Waste Items',\n",
    "    'Sofa': 'Large Waste Items',\n",
    "    'Table': 'Large Waste Items',\n",
    "    'TV stand': 'Large Waste Items',\n",
    "    'Vanity table': 'Large Waste Items',\n",
    "    'Wardrobe': 'Large Waste Items',\n",
    "    'Air conditioner': 'Large Waste Items',\n",
    "    'Air purifier': 'Large Waste Items',\n",
    "    'dish dryer': 'Large Waste Items',\n",
    "    'Electric rice cooker': 'Large Waste Items',\n",
    "    'Fan': 'Large Waste Items',\n",
    "    'Gas oven range': 'Large Waste Items',\n",
    "    'Heater': 'Large Waste Items',\n",
    "    'Humidifier': 'Large Waste Items',\n",
    "    'Microwave': 'Large Waste Items',\n",
    "    'refrigerator': 'Large Waste Items',\n",
    "    'Spin dryer': 'Large Waste Items',\n",
    "    'TV': 'Large Waste Items',\n",
    "    'Washing machine': 'Large Waste Items',\n",
    "    'Aquarium': 'Large Waste Items',\n",
    "    'Bamboo mat': 'Large Waste Items',\n",
    "    'Bedding items': 'Large Waste Items',\n",
    "    'bicycle': 'Large Waste Items',\n",
    "    'Carpet': 'Large Waste Items',\n",
    "    'Clothes drying rack': 'Large Waste Items',\n",
    "    'Coat rack': 'Large Waste Items',\n",
    "    'Door panel': 'Large Waste Items',\n",
    "    'Earthenware jar': 'Large Waste Items',\n",
    "    'Floor covering': 'Large Waste Items',\n",
    "    'Frame': 'Large Waste Items',\n",
    "    'lumber': 'Large Waste Items',\n",
    "    'Mannequin': 'Large Waste Items',\n",
    "    'Mat': 'Large Waste Items',\n",
    "    'Piano': 'Large Waste Items',\n",
    "    'Rice storage container': 'Large Waste Items',\n",
    "    'Signboard': 'Large Waste Items',\n",
    "    'Stroller': 'Large Waste Items',\n",
    "    'Wall clock': 'Large Waste Items',\n",
    "    'Water tank': 'Large Waste Items',\n",
    "    'audio cabinet': 'Large Waste Items',\n",
    "    'suitcase': 'Large Waste Items',\n",
    "    \n",
    "    # 기타 카테고리\n",
    "    'PP bag': 'PP bag',\n",
    "    'General waste bag': 'General Waste',\n",
    "    'waste pile': 'General Waste',\n",
    "    'CleanNet': 'CleanNet',\n",
    "    # 원본 클래스명도 추가\n",
    "    'General Waste': 'General Waste'  # 원본 클래스명 매핑 추가\n",
    "}\n",
    "\n",
    "# Cell 3: 바운딩 박스 색상 결정 함수\n",
    "def get_bbox_color(class_name, confidence):\n",
    "    # 먼저 클래스명이 매핑에 있는지 확인하고 매핑된 카테고리 사용\n",
    "    mapped_class = category_mapping.get(class_name, class_name)\n",
    "    \n",
    "    if mapped_class == 'Large Waste Items':\n",
    "        return (0, 255, 0) if confidence > 0.4 else (0, 0, 255)  # 초록색 또는 빨간색\n",
    "    elif mapped_class == 'PP bag':\n",
    "        return (0, 255, 0) if confidence > 0.5 else (0, 0, 255)  # 초록색 또는 빨간색\n",
    "    else:\n",
    "        return (255, 255, 255)  # 흰색 (General Waste와 CleanNet)\n",
    "\n",
    "# Cell 4: 객체 감지 및 시각화 함수\n",
    "def detect_objects(model_path, image_dir):\n",
    "    # 모델 로드\n",
    "    model = YOLO(model_path)\n",
    "    \n",
    "    # 출력 디렉토리 생성\n",
    "    output_dir = './runs/detect/predict/'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # 모든 이미지 파일 가져오기\n",
    "    image_paths = []\n",
    "    for ext in ['*.jpg', '*.jpeg', '*.png']:\n",
    "        image_paths.extend(list(Path(image_dir).glob(ext)))\n",
    "    \n",
    "    print(f\"Found {len(image_paths)} images in the directory.\")\n",
    "    \n",
    "    results = []\n",
    "    for idx, image_path in enumerate(image_paths, 1):\n",
    "        print(f\"\\nProcessing image {idx}/{len(image_paths)}: {image_path.name}\")\n",
    "        \n",
    "        # 이미지 로드 및 객체 감지\n",
    "        image = cv2.imread(str(image_path))\n",
    "        if image is None:\n",
    "            print(f\"Error: Could not load image {image_path}\")\n",
    "            continue\n",
    "            \n",
    "        result = model.predict(source=str(image_path), save=False, device='cpu')[0]\n",
    "        \n",
    "        # 결과 시각화\n",
    "        for box, conf, cls_id in zip(result.boxes.xyxy, result.boxes.conf, result.boxes.cls):\n",
    "            x1, y1, x2, y2 = box.cpu().numpy().astype(int)\n",
    "            confidence = conf.cpu().numpy()\n",
    "            class_name = result.names[int(cls_id)]\n",
    "            \n",
    "            try:\n",
    "                # 바운딩 박스 색상 결정\n",
    "                color = get_bbox_color(class_name, confidence)\n",
    "                \n",
    "                # 바운딩 박스 그리기\n",
    "                cv2.rectangle(image, (x1, y1), (x2, y2), color, 2)\n",
    "                \n",
    "                # 클래스 이름과 신뢰도 표시\n",
    "                label = f'{class_name} {confidence:.2f}'\n",
    "                cv2.putText(image, label, (x1, y1-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing detection for class {class_name}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # 결과 이미지 저장\n",
    "        output_path = os.path.join(output_dir, f'{Path(image_path).stem}_detected{Path(image_path).suffix}')\n",
    "        cv2.imwrite(output_path, image)\n",
    "        \n",
    "        results.append(result)\n",
    "        \n",
    "        # 감지된 클래스 출력\n",
    "        detected_classes = [result.names[int(cls_id)] for cls_id in result.boxes.cls]\n",
    "        print(f\"Detected Classes in {image_path.name}:\")\n",
    "        for cls in detected_classes:\n",
    "            print(f\"- {cls}\")\n",
    "        \n",
    "        # 처리 속도 정보 출력\n",
    "        preprocess_time = result.speed['preprocess']\n",
    "        inference_time = result.speed['inference']\n",
    "        postprocess_time = result.speed['postprocess']\n",
    "        print(f\"Processing times:\")\n",
    "        print(f\"- Preprocess: {preprocess_time}ms\")\n",
    "        print(f\"- Inference: {inference_time}ms\")\n",
    "        print(f\"- Postprocess: {postprocess_time}ms\")\n",
    "    \n",
    "    print(f\"\\nAll results saved to {output_dir}\")\n",
    "    return results\n",
    "\n",
    "# Cell 5: JSON 라벨 변환 함수\n",
    "def convert_labels_to_major_category(json_path, category_mapping):\n",
    "    with open(json_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    converted_data = {}\n",
    "    for image_name, annotations in data.items():\n",
    "        converted_annotations = []\n",
    "        for ann in annotations:\n",
    "            class_name = ann[\"class_name\"]\n",
    "            if class_name in category_mapping:\n",
    "                major_category = category_mapping[class_name]\n",
    "                converted_annotations.append({\n",
    "                    \"class_name\": major_category,\n",
    "                    \"bbox\": ann[\"bbox\"]\n",
    "                })\n",
    "        converted_data[image_name] = converted_annotations\n",
    "    \n",
    "    return converted_data\n",
    "\n",
    "# Cell 6: 이진 분류 함수\n",
    "def classify_images_by_major_category(converted_labels, target_class):\n",
    "    binary_classification = {}\n",
    "    \n",
    "    for image_name, annotations in converted_labels.items():\n",
    "        if any(ann[\"class_name\"] == target_class for ann in annotations):\n",
    "            binary_classification[image_name] = \"Positive\"\n",
    "        else:\n",
    "            binary_classification[image_name] = \"Negative\"\n",
    "            \n",
    "        # 상세 정보 출력\n",
    "        detected_classes = [ann[\"class_name\"] for ann in annotations]\n",
    "        print(f\"\\nImage: {image_name}\")\n",
    "        print(f\"Detected classes: {detected_classes}\")\n",
    "        print(f\"Classification: {binary_classification[image_name]}\")\n",
    "    \n",
    "    return binary_classification\n",
    "\n",
    "# Cell 7: 결과 저장 함수\n",
    "def save_results(binary_classification, output_path):\n",
    "    results_summary = {\n",
    "        \"total_images\": len(binary_classification),\n",
    "        \"positive_count\": sum(1 for v in binary_classification.values() if v == \"Positive\"),\n",
    "        \"negative_count\": sum(1 for v in binary_classification.values() if v == \"Negative\"),\n",
    "        \"classifications\": binary_classification\n",
    "    }\n",
    "    \n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(results_summary, f, indent=4, ensure_ascii=False)\n",
    "    \n",
    "    return results_summary\n",
    "\n",
    "# Cell 8: 메인 실행 코드\n",
    "def main():\n",
    "    # 경로 설정\n",
    "    model_path = r'E:\\livingLab\\YOLOv8기반 객체인식\\results_20241124_155336\\waste_detection_model\\weights\\best.pt'\n",
    "    image_dir = r'E:\\livingLab\\YOLOv8기반 객체인식\\test\\images'\n",
    "    json_path = r'E:\\livingLab\\YOLOv8기반 객체인식\\test\\test_annotations_yolo.json'\n",
    "    target_class = 'Large Waste Items'\n",
    "    \n",
    "    # 1. 객체 감지 실행\n",
    "    print(\"Starting object detection...\")\n",
    "    results = detect_objects(model_path, image_dir)\n",
    "    \n",
    "    # 2. 라벨 변환\n",
    "    print(\"\\nConverting labels to major categories...\")\n",
    "    converted_labels = convert_labels_to_major_category(json_path, category_mapping)\n",
    "    \n",
    "    # 3. 이진 분류 실행\n",
    "    print(\"\\nPerforming binary classification...\")\n",
    "    binary_classification = classify_images_by_major_category(converted_labels, target_class)\n",
    "    \n",
    "    # 4. 결과 저장\n",
    "    output_path = './test/classification_results.json'\n",
    "    results_summary = save_results(binary_classification, output_path)\n",
    "    \n",
    "    # 5. 최종 결과 출력\n",
    "    print(\"\\nClassification Summary:\")\n",
    "    print(f\"Total images processed: {results_summary['total_images']}\")\n",
    "    print(f\"Positive classifications: {results_summary['positive_count']}\")\n",
    "    print(f\"Negative classifications: {results_summary['negative_count']}\")\n",
    "    print(f\"\\nDetailed results saved to: {output_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
